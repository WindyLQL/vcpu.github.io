{"meta":{"title":"i博客","subtitle":null,"description":null,"author":"vcpu","url":"https://vcpu.github.io"},"pages":[{"title":"categories","date":"2017-06-05T04:59:19.000Z","updated":"2017-06-05T05:00:31.000Z","comments":false,"path":"categories/index.html","permalink":"https://vcpu.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2017-06-22T14:58:14.000Z","updated":"2017-06-22T14:58:14.000Z","comments":true,"path":"top/index.html","permalink":"https://vcpu.github.io/top/index.html","excerpt":"","text":"AV.initialize(\"l5u4v8AcoPCp9wlPGVJTG6Ny-gzGzoHsz\", \"RQdYIOd2snvd1muAW65nsn25\"); var time=0 var title=\"\" var url=\"\" var query = new AV.Query('Counter');//表名 query.notEqualTo('id',0); //id不为0的结果 query.descending('time'); //结果按阅读次数降序排序 query.limit(20); //最终只返回10条结果 query.find().then(function (todo) { for (var i=0;i"},{"title":"","date":"2017-06-22T09:22:34.000Z","updated":"2017-06-22T10:09:28.000Z","comments":true,"path":"about/index.html","permalink":"https://vcpu.github.io/about/index.html","excerpt":"","text":"小程一枚，坐标上海浦东 很喜欢写博客，但刚刚行动，多包涵 本博客用于分享个人技术总结和心得 搞过3年的负载均衡产品，同时也接触过审计，FW，DPI等安全设备，也开发过webrtc直播系统等 对于tcp/ip，socket，协议分析，协议栈、webrtc、二三层转发、网络搭建和优化等都有些自己的看法 常年用服务端c语言开发，也会些shell，python，go 最喜欢阅读开源代码，从中体会实现思想、提升自己，有时候会为了好代码而惊起一身鸡皮疙瘩 感到遗憾的是自己还没有成为过开源软件的开发者，现在也在努力中 如果想和我交流，请邮件到1037365462艾特qq.com"},{"title":"标签","date":"2017-06-05T04:50:47.000Z","updated":"2017-06-05T04:51:50.000Z","comments":false,"path":"tags/index.html","permalink":"https://vcpu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"select","slug":"select","date":"2017-06-22T10:39:53.000Z","updated":"2017-06-22T10:39:53.000Z","comments":true,"path":"select/","link":"","permalink":"https://vcpu.github.io/select/","excerpt":"1234567891011#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);void FD_CLR(int fd, fd_set *set);int FD_ISSET(int fd, fd_set *set);void FD_SET(int fd, fd_set *set);void FD_ZERO(fd_set *set); nfds 是最大文件描述符号 +1 怎么可能这么简单，它限制的是最大值而不是个数 readfds 用来记录可读fd集合 writefds 用来记录可写fd集合 exceptfds 用来检查带外数据 timeout 决定select等待I/O时间 1.timeout该值为NULL，会阻塞一定等到监控的文件描述符集合中产生状态变化（可读，可写等）2.timeout值为0分0毫秒，非阻塞，不关注文件描述符是否变化立刻返回3.timeout正常值，timeout这段时间内阻塞，如果监控集合中有信号来临，select将返回，否则超时返回","text":"1234567891011#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);void FD_CLR(int fd, fd_set *set);int FD_ISSET(int fd, fd_set *set);void FD_SET(int fd, fd_set *set);void FD_ZERO(fd_set *set); nfds 是最大文件描述符号 +1 怎么可能这么简单，它限制的是最大值而不是个数 readfds 用来记录可读fd集合 writefds 用来记录可写fd集合 exceptfds 用来检查带外数据 timeout 决定select等待I/O时间 1.timeout该值为NULL，会阻塞一定等到监控的文件描述符集合中产生状态变化（可读，可写等）2.timeout值为0分0毫秒，非阻塞，不关注文件描述符是否变化立刻返回3.timeout正常值，timeout这段时间内阻塞，如果监控集合中有信号来临，select将返回，否则超时返回 返回值： 0 出发信号的fd数目 =0 超时 -1 出错 作用： 用来管理fd集合，实现多fd集合监听操作 select用户态用法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#define MYPORT 1234 // the port users will be connecting to#define BACKLOG 5 // how many pending connections queue will hold#define BUF_SIZE 200int fd_A[BACKLOG]; // accepted connection fdint conn_amount; // current connection amountvoid showclient()&#123; int i; printf(\"client amount: %d\\n\", conn_amount); for (i = 0; i &lt; BACKLOG; i++) &#123; printf(\"[%d]:%d \", i, fd_A[i]); &#125; printf(\"\\n\\n\");&#125;int main(void)&#123; int sock_fd, new_fd; // listen on sock_fd, new connection on new_fd struct sockaddr_in server_addr; // server address information struct sockaddr_in client_addr; // connector's address information socklen_t sin_size; int yes = 1; char buf[BUF_SIZE]; int ret; int i; if ((sock_fd = socket(AF_INET, SOCK_STREAM, 0)) == -1) &#123; perror(\"socket\"); exit(1); &#125; if (setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;yes, sizeof(int)) == -1) &#123; perror(\"setsockopt\"); exit(1); &#125; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(MYPORT); server_addr.sin_addr.s_addr = INADDR_ANY; memset(server_addr.sin_zero, '\\0', sizeof(server_addr.sin_zero)); if (bind(sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)) == -1) &#123; perror(\"bind\"); exit(1); &#125; if (listen(sock_fd, BACKLOG) == -1) &#123; perror(\"listen\"); exit(1); &#125; printf(\"listen port %d\\n\", MYPORT); fd_set fdsr; int maxsock; struct timeval tv; conn_amount = 0; sin_size = sizeof(client_addr); maxsock = sock_fd; while (1) &#123; // initialize file descriptor set FD_ZERO(&amp;fdsr); FD_SET(sock_fd, &amp;fdsr); // timeout setting tv.tv_sec = 30; tv.tv_usec = 0; // add active connection to fd set for (i = 0; i &lt; BACKLOG; i++) &#123; if (fd_A[i] != 0) &#123; FD_SET(fd_A[i], &amp;fdsr); &#125; &#125; ret = select(maxsock + 1, &amp;fdsr, NULL, NULL, &amp;tv); if (ret &lt; 0) &#123; perror(\"select\"); break; &#125; else if (ret == 0) &#123; printf(\"timeout\\n\"); continue; &#125; // check every fd in the set for (i = 0; i &lt; conn_amount; i++) &#123; if (FD_ISSET(fd_A[i], &amp;fdsr)) &#123; ret = recv(fd_A[i], buf, sizeof(buf), 0); if (ret &lt;= 0) &#123; // client close printf(\"client[%d] close\\n\", i); close(fd_A[i]); FD_CLR(fd_A[i], &amp;fdsr); fd_A[i] = 0; &#125; else &#123; // receive data if (ret &lt; BUF_SIZE) memset(&amp;buf[ret], '\\0', 1); printf(\"client[%d] send:%s\\n\", i, buf); &#125; &#125; &#125; // check whether a new connection comes if (FD_ISSET(sock_fd, &amp;fdsr)) &#123; new_fd = accept(sock_fd, (struct sockaddr *)&amp;client_addr, &amp;sin_size); if (new_fd &lt;= 0) &#123; perror(\"accept\"); continue; &#125; // add to fd queue if (conn_amount &lt; BACKLOG) &#123; fd_A[conn_amount++] = new_fd; printf(\"new connection client[%d] %s:%d\\n\", conn_amount, inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port)); if (new_fd &gt; maxsock) maxsock = new_fd; &#125; else &#123; printf(\"max connections arrive, exit\\n\"); send(new_fd, \"bye\", 4, 0); close(new_fd); break; &#125; &#125; showclient(); &#125; // close other connections for (i = 0; i &lt; BACKLOG; i++) &#123; if (fd_A[i] != 0) &#123; close(fd_A[i]); &#125; &#125; exit(0);&#125; 代码实现采用select用法描述： 1.select作为服务端使用2.select监听服务的fd，如果有客户端连接此服务端时候，服务端fd会被触发，然后调用accept完成连接3.select监听服务端和客户端建立好连接的fd，如果客户端发送数据过来，select可监听到读信号，然后recv读出数据。 select实现分析用户态select 系统调用 sys_select 调用栈如下： 0xffffffff81213f80 : sys_select+0x0/0x110 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel] 实现代码位于：fs/select.c SYSCALL_DEFINE5(select,… select功能概述sys_select实现分析分析结论： sys_select1234567891011121314151617181920212223242526SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp, fd_set __user *, exp, struct timeval __user *, tvp)&#123; struct timespec end_time, *to = NULL; struct timeval tv; int ret; //用户态时间处理，将用户态时间拷入内核态并将参数规整为struct timespec以供调用 if (tvp) &#123; if (copy_from_user(&amp;tv, tvp, sizeof(tv))) return -EFAULT; to = &amp;end_time; if (poll_select_set_timeout(to, tv.tv_sec + (tv.tv_usec / USEC_PER_SEC), (tv.tv_usec % USEC_PER_SEC) * NSEC_PER_USEC)) return -EINVAL; &#125; //select的核心实现 ret = core_sys_select(n, inp, outp, exp, to); //该函数会将剩余的时间拷入到用户态的tvp 中 ret = poll_select_copy_remaining(&amp;end_time, tvp, 1, ret); return ret;&#125; 1.将用户态select时间参数拷入内核2.调用core_sys_select3.将select退出后剩余时间结果拷入用户态时间参数中 core_sys_select1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timespec *end_time)&#123; fd_set_bits fds; void *bits; int ret, max_fds; unsigned int size; struct fdtable *fdt; /* Allocate small arguments on the stack to save memory and be faster */ long stack_fds[SELECT_STACK_ALLOC/sizeof(long)]; //用户态给予参数nfds &lt; 0 ,直接返并报告参数非法 -EINVAL ret = -EINVAL; if (n &lt; 0) goto out_nofds; /* max_fds can increase, so grab it once to avoid race */ rcu_read_lock(); fdt = files_fdtable(current-&gt;files); max_fds = fdt-&gt;max_fds; rcu_read_unlock(); if (n &gt; max_fds) n = max_fds; /* * We need 6 bitmaps (in/out/ex for both incoming and outgoing), * since we used fdset we need to allocate memory in units of * long-words. */ //以一个文件描述符占1bit，传递进来的这么多fd共占多数字 size = FDS_BYTES(n); bits = stack_fds; //检查默认静态数据资源是否够用 if (size &gt; sizeof(stack_fds) / 6) &#123; /* Not enough space in on-stack array; must use kmalloc */ ret = -ENOMEM; bits = kmalloc(6 * size, GFP_KERNEL); if (!bits) goto out_nofds; &#125; //fds用来指向具体的存储空间 fds.in = bits; fds.out = bits + size; fds.ex = bits + 2*size; fds.res_in = bits + 3*size; fds.res_out = bits + 4*size; fds.res_ex = bits + 5*size;//将用户空间的inp outp exp 拷入内核空间 if ((ret = get_fd_set(n, inp, fds.in)) || (ret = get_fd_set(n, outp, fds.out)) || (ret = get_fd_set(n, exp, fds.ex))) goto out; //存放返回状态的字段清零，后续可用作返回结果使用 zero_fd_set(n, fds.res_in); zero_fd_set(n, fds.res_out); zero_fd_set(n, fds.res_ex); //select核心逻辑处理函数 ret = do_select(n, &amp;fds, end_time); //存在错误 if (ret &lt; 0) goto out; //超时情况 if (!ret) &#123; ret = -ERESTARTNOHAND; if (signal_pending(current)) goto out; ret = 0; &#125; //把结果集拷入用户空间 if (set_fd_set(n, inp, fds.res_in) || set_fd_set(n, outp, fds.res_out) || set_fd_set(n, exp, fds.res_ex)) ret = -EFAULT;out: //释放辅助内存 if (bits != stack_fds) kfree(bits);out_nofds: return ret;&#125; 1.检验nfds，如果其小于0，参数异常返回；并规整nfds（最大不能超过当前进程的max_fds）2.将用户态fd集合拷入内核态3.运行do_select4.将do_select检测结果拷入用户空间5.释放select运算中辅助内存 do_select123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161int do_select(int n, fd_set_bits *fds, struct timespec *end_time)&#123; ktime_t expire, *to = NULL; struct poll_wqueues table; poll_table *wait; int retval, i, timed_out = 0; unsigned long slack = 0; unsigned int busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0; unsigned long busy_end = 0; rcu_read_lock(); //检查fd对应file状态，且找出最大fd retval = max_select_fd(n, fds); rcu_read_unlock(); if (retval &lt; 0) return retval; n = retval; poll_initwait(&amp;table); wait = &amp;table.pt; //传入的时间为0s 0ms time_out标记为1 这种情况不阻塞直接返回 if (end_time &amp;&amp; !end_time-&gt;tv_sec &amp;&amp; !end_time-&gt;tv_nsec) &#123; wait-&gt;_qproc = NULL; timed_out = 1; &#125; //正常情况处理。 超时时间转换 if (end_time &amp;&amp; !timed_out) slack = select_estimate_accuracy(end_time); retval = 0; for (;;) &#123; unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp; bool can_busy_loop = false; inp = fds-&gt;in; outp = fds-&gt;out; exp = fds-&gt;ex; rinp = fds-&gt;res_in; routp = fds-&gt;res_out; rexp = fds-&gt;res_ex; //所有监听的fd大循环 for (i = 0; i &lt; n; ++rinp, ++routp, ++rexp) &#123; unsigned long in, out, ex, all_bits, bit = 1, mask, j; unsigned long res_in = 0, res_out = 0, res_ex = 0; //32个文件描述符号，没有任何状态被检测，进入下一轮32个 in = *inp++; out = *outp++; ex = *exp++; all_bits = in | out | ex; if (all_bits == 0) &#123; i += BITS_PER_LONG; continue; &#125; // 这一轮32个fd存在需要检测的状态 for (j = 0; j &lt; BITS_PER_LONG; ++j, ++i, bit &lt;&lt;= 1) &#123; struct fd f; //超过最大待检测fd n直接退出循环 if (i &gt;= n) break; //跳过没有状态检测的fd if (!(bit &amp; all_bits)) continue; f = fdget(i); if (f.file) &#123; const struct file_operations *f_op; f_op = f.file-&gt;f_op; //设置fd检测事件掩码，poll相关情况处理 mask = DEFAULT_POLLMASK; if (f_op &amp;&amp; f_op-&gt;poll) &#123; //设置用户需要探查的标记 wait_key_set(wait, in, out, bit, busy_flag); //获取fd当前对应的信号掩码 mask = (*f_op-&gt;poll)(f.file, wait); &#125; fdput(f); //可读 if ((mask &amp; POLLIN_SET) &amp;&amp; (in &amp; bit)) &#123; res_in |= bit; retval++; wait-&gt;_qproc = NULL; &#125; //可写 if ((mask &amp; POLLOUT_SET) &amp;&amp; (out &amp; bit)) &#123; res_out |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLEX_SET) &amp;&amp; (ex &amp; bit)) &#123; res_ex |= bit; retval++; wait-&gt;_qproc = NULL; &#125; /* got something, stop busy polling */ if (retval) &#123; can_busy_loop = false; busy_flag = 0; /* * only remember a returned * POLL_BUSY_LOOP if we asked for it */ &#125; else if (busy_flag &amp; mask) can_busy_loop = true; &#125; &#125; //将检测结果存下来 if (res_in) *rinp = res_in; if (res_out) *routp = res_out; if (res_ex) *rexp = res_ex; //增加抢占点 该抢占点可达到效果是：判断是否有进程需要抢占当前进程，如果是将立即发生调度 //已经检查过的fd如果此时被唤醒，则会在此产生调度 cond_resched(); &#125; wait-&gt;_qproc = NULL; if (retval || timed_out || signal_pending(current)) break; //设备就绪异常超时终止灯信号触发，直接break，可跳出大循环结束程序 if (table.error) &#123; retval = table.error; break; &#125; /* only if found POLL_BUSY_LOOP sockets &amp;&amp; not out of time */ if (can_busy_loop &amp;&amp; !need_resched()) &#123; if (!busy_end) &#123; busy_end = busy_loop_end_time(); continue; &#125; if (!busy_loop_timeout(busy_end)) continue; &#125; busy_flag = 0; /* * If this is the first loop and we have a timeout * given, then we convert to ktime_t and set the to * pointer to the expiry value. */ if (end_time &amp;&amp; !to) &#123; expire = timespec_to_ktime(*end_time); to = &amp;expire; &#125; //当前用户进程从这里进入睡眠，超时后timed_out 置1 直接退出 if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) timed_out = 1; &#125; poll_freewait(&amp;table); return retval; &#125; do_select为select的核心实现，其处理过程如下： 1.调用poll_initwait初始化poll_wqueues对象table，包括其成员poll_table； 2.如果用户传入的timeout不为NULL，但是设定的时间为0，那么设置poll_table指针wait(即 &amp;table.pt）为NULL；当&amp;table.pt为NULL，它并不会被加到等到队列中。 3.将in,out和exception进行或运算，得到all_bits，然后遍历all_bits中bit为1的fd，根据进程的fd_table查找到file指针filp，然后设置wait的key值（POLLEX_SET, POLLIN_SET,POLLIN_SET三者的或运算，取决于用户输入），并调用filp-&gt;poll(filp, wait)，获得返回值mask。 再根据mask值检查该文件是否立即满足条件，如果满足，设置res_in/res_out/res_exception的值，执行retval++, 并设置wait为NULL。 4.在每遍历32（取决于long型整数的位数）个文件后，调用1次cond_resched()，主动寻求调度，可以等待已经遍历过的文件是否有唤醒的； 5.在遍历完所有文件之后，设置wait为NULL，并检查是否有满足条件的文件（retval值是否为0），或者是否超时，或者是否有未决信号，如果有那么直接跳出循环，进入步骤7； 6.否则调用poll_schedule_timeout，使进程进入睡眠，直到超时（如果未设置超时，那么是直接调用的schedule()）。如果是超时后进程继续执行，那么设置pwq-&gt;triggered为0；如果是被文件对应的驱动程序唤醒的，那么pwq-&gt;triggered被设置为1. 7.最终，函数调用poll_freewait，将本进程从所有文件的等待队列中删掉，并删除分配的poll_table_page对象，回收内存，并返回retval值。 8.拷贝res_in, res_out和res_exception到传入的in, out, exception，并返回ret。 select睡眠过程do_select … 步骤1 poll_initwait(&amp;table); wait = &amp;table.pt;… 步骤2 if (f_op &amp;&amp; f_op-&gt;poll) { wait_key_set(wait, in, out, bit, busy_flag); //如果是socket此处调用的是sock_poll mask = (*f_op-&gt;poll)(f.file, wait);} 步骤3… if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) 步骤1:初始化table struct poll_wqueues table; 12345678910void poll_initwait(struct poll_wqueues *pwq)&#123; init_poll_funcptr(&amp;pwq-&gt;pt, __pollwait); pwq-&gt;polling_task = current; pwq-&gt;triggered = 0; pwq-&gt;error = 0; pwq-&gt;table = NULL; pwq-&gt;inline_index = 0;&#125;EXPORT_SYMBOL(poll_initwait); 将当前进程标志current给table让其记录下来 将__pollwait给table-&gt;pt-&gt;_qproc让其记录下来 步骤2:调用sock_poll最终调用tcp_pool12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* No kernel lock held - perfect */static unsigned int sock_poll(struct file *file, poll_table *wait)&#123; unsigned int busy_flag = 0; struct socket *sock; /* * We can't return errors to poll, so it's either yes or no. */ sock = file-&gt;private_data; if (sk_can_busy_loop(sock-&gt;sk)) &#123; /* this socket can poll_ll so tell the system call */ busy_flag = POLL_BUSY_LOOP; /* once, only if requested by syscall */ if (wait &amp;&amp; (wait-&gt;_key &amp; POLL_BUSY_LOOP)) sk_busy_loop(sock-&gt;sk, 1); &#125; //针对于tcpsocket来讲此处调用tcp_pool return busy_flag | sock-&gt;ops-&gt;poll(file, sock, wait);&#125;/* * Wait for a TCP event. * * Note that we don't need to lock the socket, as the upper poll layers * take care of normal races (between the test and the event) and we don't * go look at any of the socket buffers directly. */unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)&#123; unsigned int mask; struct sock *sk = sock-&gt;sk; const struct tcp_sock *tp = tcp_sk(sk); sock_rps_record_flow(sk); sock_poll_wait(file, sk_sleep(sk), wait); if (sk-&gt;sk_state == TCP_LISTEN) return inet_csk_listen_poll(sk); /* Socket is not locked. We are protected from async events * by poll logic and correct handling of state changes * made by other threads is impossible in any case. */ mask = 0; if (sk-&gt;sk_shutdown == SHUTDOWN_MASK || sk-&gt;sk_state == TCP_CLOSE) mask |= POLLHUP; if (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) mask |= POLLIN | POLLRDNORM | POLLRDHUP; /* Connected or passive Fast Open socket? */ if (sk-&gt;sk_state != TCP_SYN_SENT &amp;&amp; (sk-&gt;sk_state != TCP_SYN_RECV || tp-&gt;fastopen_rsk != NULL)) &#123; int target = sock_rcvlowat(sk, 0, INT_MAX); if (tp-&gt;urg_seq == tp-&gt;copied_seq &amp;&amp; !sock_flag(sk, SOCK_URGINLINE) &amp;&amp; tp-&gt;urg_data) target++; /* Potential race condition. If read of tp below will * escape above sk-&gt;sk_state, we can be illegally awaken * in SYN_* states. */ if (tp-&gt;rcv_nxt - tp-&gt;copied_seq &gt;= target) mask |= POLLIN | POLLRDNORM; if (!(sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) &#123; if (sk_stream_is_writeable(sk)) &#123; mask |= POLLOUT | POLLWRNORM; &#125; else &#123; /* send SIGIO later */ set_bit(SOCK_ASYNC_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); set_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /* Race breaker. If space is freed after * wspace test but before the flags are set, * IO signal will be lost. */ if (sk_stream_is_writeable(sk)) mask |= POLLOUT | POLLWRNORM; &#125; &#125; else mask |= POLLOUT | POLLWRNORM; if (tp-&gt;urg_data &amp; TCP_URG_VALID) mask |= POLLPRI; &#125; /* This barrier is coupled with smp_wmb() in tcp_reset() */ smp_rmb(); if (sk-&gt;sk_err) mask |= POLLERR; return mask;&#125; 收集信号状态以mask方式返回 调用sock_poll_wait然后poll_wait最终调用_qproc也就是__pollwait __pollwait 123456789101112131415/* Add a new entry */static void __pollwait(struct file *filp, wait_queue_head_t *wait_address, poll_table *p)&#123; struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt); struct poll_table_entry *entry = poll_get_entry(pwq); if (!entry) return; entry-&gt;filp = get_file(filp); entry-&gt;wait_address = wait_address; entry-&gt;key = p-&gt;_key; init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake); entry-&gt;wait.private = pwq; add_wait_queue(wait_address, &amp;entry-&gt;wait);&#125; 为每个fd对应文件分配 poll_table_entry 将fd对应poll_table_entry加入到等待队列中 步骤3: poll_schedule_timeout，作用是使进程进入睡眠，直到超时或者被唤醒 如果超时后进程继续执行设置pwq-&gt;triggered为0 如果是被文件对应的驱动程序唤醒pwq-&gt;triggered为1 1234567891011121314151617181920212223242526int poll_schedule_timeout(struct poll_wqueues *pwq, int state, ktime_t *expires, unsigned long slack)&#123; int rc = -EINTR; set_current_state(state); if (!pwq-&gt;triggered) rc = freezable_schedule_hrtimeout_range(expires, slack, HRTIMER_MODE_ABS); __set_current_state(TASK_RUNNING); /* * Prepare for the next iteration. * * The following set_mb() serves two purposes. First, it's * the counterpart rmb of the wmb in pollwake() such that data * written before wake up is always visible after wake up. * Second, the full barrier guarantees that triggered clearing * doesn't pass event check of the next iteration. Note that * this problem doesn't exist for the first iteration as * add_wait_queue() has full barrier semantics. */ set_mb(pwq-&gt;triggered, 0); return rc;&#125; select唤醒过程 0xffffffff81213130 : pollwake+0x0/0x90 [kernel] 0xffffffff810ba628 : wake_up_common+0x58/0x90 [kernel] 0xffffffff810bc4a4 : wake_up_sync_key+0x44/0x60 [kernel] 0xffffffff8155825a : sock_def_readable+0x3a/0x70 [kernel] 0xffffffff815c8197 : tcp_data_queue+0x497/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000] pollwake -&gt;__pollwake-&gt;default_wake_function-&gt;try_to_wake_up try_to_wake_up会把进程的状态设置为TASK_RUNNING，并把进程插入CPU运行队列，来唤醒睡眠的进程 linux select 1024限制魔咒__FD_SETSIZE 默认最大为1024，一个int占用4个byte，也就是32个bit，所以使用了一个int数组大小为32位来表示了我们要操作的fd的数值，每个bit代表了一个handle数值 需要注意的问题是，这里的最大为1024，如果handle数值为1025是不能处理的（而且很容易导致破坏堆栈），不是说可以容纳1024个网络客户端句柄，而是最大的handle数值为1024，再算上系统本身使用的stdout,stdin, stderr默认的3个，因此最多也就是1021个，再算上程序打开的文件句柄等等，实际上使用可能要比1024少上好多。 另外，ulimit对每个进程打开的句柄也有限制。 why 1024 ?内核参数适用结构体是fd_set 123SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp, fd_set __user *, exp, struct timeval __user *, tvp)&#123; fd_set是 __kernel_fd_set 1typedef __kernel_fd_set fd_set; __kernel_fd_set 中fds_bits 最大只能1024 12345#define __FD_SETSIZE 1024typedef struct &#123; unsigned long fds_bits[__FD_SETSIZE / (8 * sizeof(long))];&#125; __kernel_fd_set; 我该怎么办才能突破1024限制？修改掉此宏重新编译吧，当然还有其他办法，但是没必要这么复杂，直接用pool或者epool解决吧当然你也可以多进程或者多线程，每个进程／线程 分别select select缺点总结 select效率低下，用户空间和内核空间来回拷贝，select内部吧存进程上下文切换，大型项目不适用可同时监听的文件数量有限，linux平台1024个每次调用select都要遍历完成所有的fd，每隔32fd需要调度一次多个fd情况下，如果小的fs一直可读，会导致大的fd信号不会被收集到需要在用户态和内核态来回拷贝fd_set，睡眠唤醒机制需要为fd分配poll_table_entry","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"select","slug":"select","permalink":"https://vcpu.github.io/tags/select/"},{"name":"kernel3.10.0-415.16.1","slug":"kernel3-10-0-415-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-415-16-1/"}]},{"title":"Linux系统调用","slug":"Linux系统调用","date":"2017-06-22T10:10:14.000Z","updated":"2017-06-22T10:10:14.000Z","comments":true,"path":"Linux系统调用/","link":"","permalink":"https://vcpu.github.io/Linux系统调用/","excerpt":"","text":"什么事系统调用 linux虚拟地址空间分为用户空间和内核空间 用户空间不可直接访问内核空间，帝王班的内核空间可直接访问用户空间 用户空间只能通过系统调用访问内核空间 系统调用时内核提供的一组函数接口，使得用户空间上进程可以和内核空间交互 系统调用过程 执行用户程序 根据glibc中实现，取得系统调用号，将其存入EAX并执行int $0x80（128号中断） 用户态可以传递变量、参数值给内核，内核态运行时候会保存用户进程的一些寄存器值等（上下文环境） 触发中断后内核根据系统调用号执行对应的中断处理函数 系统调用结束将访问址存入EAX，返回中断处理函数 中断处理函数根据存储用户态进程上下文环境恢复用户态，同时用户态就获取了内核态函数执行的返回值 系统调用汇编123456// pid = fork();asm volatile( &quot;mov $0x2, %%eax\\n\\t&quot; // 将fork的系统调用号2存到eax寄存器 &quot;int $0x80\\n\\t&quot; // 产生int 0x80中断 &quot;mov %%eax,%0\\n\\t&quot; // 将结果存入pid中 : &quot;=m&quot; (pid) 系统调用实现分析待续 添加系统调用待续","categories":[{"name":"linux","slug":"linux","permalink":"https://vcpu.github.io/categories/linux/"}],"tags":[{"name":"系统调用","slug":"系统调用","permalink":"https://vcpu.github.io/tags/系统调用/"}]},{"title":"tcp socket发送缓冲区","slug":"tcp_sndbuf","date":"2017-06-20T10:31:48.000Z","updated":"2017-06-20T10:31:48.000Z","comments":true,"path":"tcp_sndbuf/","link":"","permalink":"https://vcpu.github.io/tcp_sndbuf/","excerpt":"tcp socket发送缓冲区探究结论 1: 未设置SO_SNDBUF时，sk-&gt;sk_sndbuf值由tcp_finish_connect-&gt;tcp_init_buffer_space-&gt;tcp_sndbuf_expand决定，TCP协议栈会自己计算一个值出来46080，sk_sndbuf是46080和net.ipv4.tcp_wmem[2]（4194304）的较小值 2: 设置SO_SNDBUF后，tcp_sndbuf_expand将不会再被调用，其值情况完全由sock_setsockopt决定 2-1: 设置值较小 value &lt; 2304 { SOCK_MIN_SNDBUF（4608）/2 } sk_sndbuf = 4608 2-2: 设置值适中 { SOCK_MIN_SNDBUF（4608）/2 } &lt; value &lt; net.core.wmem_max sk_sndbuf = value*2 2-3: 设置值较大 value &gt; net.core.wmem_max sk_sndbuf = net.core.wmem_max* 2","text":"tcp socket发送缓冲区探究结论 1: 未设置SO_SNDBUF时，sk-&gt;sk_sndbuf值由tcp_finish_connect-&gt;tcp_init_buffer_space-&gt;tcp_sndbuf_expand决定，TCP协议栈会自己计算一个值出来46080，sk_sndbuf是46080和net.ipv4.tcp_wmem[2]（4194304）的较小值 2: 设置SO_SNDBUF后，tcp_sndbuf_expand将不会再被调用，其值情况完全由sock_setsockopt决定 2-1: 设置值较小 value &lt; 2304 { SOCK_MIN_SNDBUF（4608）/2 } sk_sndbuf = 4608 2-2: 设置值适中 { SOCK_MIN_SNDBUF（4608）/2 } &lt; value &lt; net.core.wmem_max sk_sndbuf = value*2 2-3: 设置值较大 value &gt; net.core.wmem_max sk_sndbuf = net.core.wmem_max* 2 默认情况下（未设置SO_SNDBUF）net.core.wmem_default = 212992net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 TCPsocket未connect之前 sendbuf:16384 sk-&gt;sk_sndbuf是sysctl_tcp_wmem[1]的值 connect之后，sendbuf:46080 通过调试机制可知，sendbuf默认大小为sysctl_tcp_wmem[1] 为16384connect连接连接到服务端后，sendbuf变为46080，该值不是尚书配置中任何一个值 原因探究阶段1:tcp_init_sock初始化，sk-&gt;sk_sndbuf = sysctl_tcp_wmem[1] 阶段2:主动连接进入ES状态时候，状态切换时候调用tcp_sndbuf_expand调整sk_sndbuf stp脚本探测结果如下：123456789101112131415161718tcp_v4_connect[2017/6/20,10:57:56]local=0.0.0.0:3000,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:16384tcp_v4_connect return [2017/6/20,10:57:56]local=192.168.55.178:3000,remote=180.97.33.108:80 state:SYN_SENT,sndbubf 1280:16384tcp_input:302 return [2017/6/20,10:57:56]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:16384 sndmem : 46080 permss 2304 0xffffffff815c3527 : tcp_sndbuf_expand+0x67/0x90 [kernel] 0xffffffff815c7ba8 : tcp_init_buffer_space+0x178/0x190 [kernel] 0xffffffff815cbbae : tcp_finish_connect+0x6e/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000] 12345678910111213141516171819202122232425262728static void tcp_sndbuf_expand(struct sock *sk)&#123; const struct tcp_sock *tp = tcp_sk(sk); int sndmem, per_mss; u32 nr_segs; /* Worst case is non GSO/TSO : each frame consumes one skb * and skb-&gt;head is kmalloced using power of two area of memory */ per_mss = max_t(u32, tp-&gt;rx_opt.mss_clamp, tp-&gt;mss_cache) + MAX_TCP_HEADER + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)); per_mss = roundup_pow_of_two(per_mss) + SKB_DATA_ALIGN(sizeof(struct sk_buff)); nr_segs = max_t(u32, TCP_INIT_CWND, tp-&gt;snd_cwnd); nr_segs = max_t(u32, nr_segs, tp-&gt;reordering + 1); /* Fast Recovery (RFC 5681 3.2) : * Cubic needs 1.7 factor, rounded to 2 to include * extra cushion (application might react slowly to POLLOUT) */ sndmem = 2 * nr_segs * per_mss; if (sk-&gt;sk_sndbuf &lt; sndmem) sk-&gt;sk_sndbuf = min(sndmem, sysctl_tcp_wmem[2]);&#125; 设置发送缓冲区大小为较小值123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 100;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 输出信息如下：default,sendbuf:16384now,sendbuf:4608 输出信息总结：设置sendbubf为100时，没有生效，反而设置出来一个较大的值4608 原因探究12345678910111213141516171819202122232425262728293031323334/* * This is meant for all protocols to use and covers goings on * at the socket level. Everything here is generic. */int sock_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)&#123; ··· case SO_SNDBUF: /* Don't error on this BSD doesn't and if you think * about it this is right. Otherwise apps have to * play 'guess the biggest size' games. RCVBUF/SNDBUF * are treated in BSD as hints */ val = min_t(u32, val, sysctl_wmem_max);set_sndbuf: sk-&gt;sk_userlocks |= SOCK_SNDBUF_LOCK; sk-&gt;sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF); /* Wake up sending tasks if we upped the value. */ sk-&gt;sk_write_space(sk); break; ··· default: ret = -ENOPROTOOPT; break; &#125; release_sock(sk); return ret;&#125;#define TCP_SKB_MIN_TRUESIZE (2048 + SKB_DATA_ALIGN(sizeof(struct sk_buff)))#define SOCK_MIN_SNDBUF (TCP_SKB_MIN_TRUESIZE * 2) 设置socket选项SO_SNDBUF会触发系统调用最终调用sock_setsockopt函数，其处理设置选项过程如上：其会将用户设置的缓冲区大小乘以2，然后和SOCK_MIN_SNDBUF（4608）比较，取较大值因此最终较小的缓冲区设置值200没有生效，生效的是4608 设置发送缓冲区大小为中间值缓冲区系统设置值大小：net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 实验动作将缓冲区大小设置为3000123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 3000;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 实验程序输出：default,sendbuf:16384now,sendbuf:6000输出信息总结：设置大小3000生效，sndbuf大小会被设置成为3000*2 设置发送缓冲区大小威较大值缓冲区系统设置值大小：net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 实验动作将缓冲区大小设置为230000123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 230000;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 实现程序输出：default,sendbuf:16384now,sendbuf:425984实验结果分析：设置大小23000（大于系统212992），sendbuf最终结果为212992*2 原因探究12345678case SO_SNDBUF: val = min_t(u32, val, sysctl_wmem_max);set_sndbuf: sk-&gt;sk_userlocks |= SOCK_SNDBUF_LOCK; sk-&gt;sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF); /* Wake up sending tasks if we upped the value. */ sk-&gt;sk_write_space(sk); break; val为用户set的值，其在选择时候会同sysctl_wmem_max比较，选取一个较小的值，如果设置值大于sysctl_wmem_max值的话，val就取系统wmem的最大值。 如上可知：230000 &gt; net.core.wmem_max ,所以用户设置SO_SNDBUF选项最大只能取net.core.wmem_max，所以最终sk_sndbubf值为net.core.wmem_max*2 即425984 其它说明tcp socket记录当前发送队列的占用缓冲区大小的变量为sk_wmem_queued和发送缓冲区判断函数如下：12345678static inline bool sk_stream_memory_free(const struct sock *sk)&#123; if (sk-&gt;sk_wmem_queued &gt;= sk-&gt;sk_sndbuf) return false; return sk-&gt;sk_prot-&gt;stream_memory_free ? sk-&gt;sk_prot-&gt;stream_memory_free(sk) : true;&#125; 从上述判别中我们可以知道，发送缓冲区记录和比对单位均是字节","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"}]},{"title":"PF_INET和AF_INET区别","slug":"pf_inet","date":"2017-06-20T10:12:51.000Z","updated":"2017-06-20T10:12:51.000Z","comments":true,"path":"pf_inet/","link":"","permalink":"https://vcpu.github.io/pf_inet/","excerpt":"","text":"PF_INET和AF_INET区别 在初始化socket时候socket(PF_INET,SOCK_SRTEAM,0) 用PF_INET，表示ip协议 指定地址协议族时候用AF_INET，表示地址为IP协议 Linux AF_INET和PF_INET值相同均为2 123456socket.h#define AF_INET 2 /* Internet IP Protocol *...#define PF_INET AF_INET socket通信协议类型 SOCKET_STREAM: 面向连接TCP SOCK_DGRAM: 无保障UDP","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"PF_INET","slug":"PF-INET","permalink":"https://vcpu.github.io/tags/PF-INET/"},{"name":"AF_INET","slug":"AF-INET","permalink":"https://vcpu.github.io/tags/AF-INET/"}]},{"title":"sockaddr_in和sockaddr的区别","slug":"sockaddr_in","date":"2017-06-20T07:18:11.000Z","updated":"2017-06-20T07:18:11.000Z","comments":true,"path":"sockaddr_in/","link":"","permalink":"https://vcpu.github.io/sockaddr_in/","excerpt":"sockaddr_in在头文件/usr/include/netinet/in.h123456789101112131415161718192021222324252627282930/* Structure describing an Internet socket address. */struct sockaddr_in &#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr&apos;. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)]; &#125;; or struct sockaddr_in &#123; short int sin_family; /* Address family */ unsigned short int sin_port; /* Port number */ struct in_addr sin_addr; /* Internet address */ unsigned char sin_zero[8]; /* Same size as struct sockaddr */&#125;; struct in_addr &#123; union &#123; struct &#123; u_char s_b1,s_b2,s_b3,s_b4; &#125; S_un_b; struct &#123; u_short s_w1,s_w2; &#125; S_un_w; u_long S_addr; &#125; S_un; #define s_addr S_un.S_addr &#125;; 组成包含协议家族、端口、地址、填充 端口和地址，需要是网络字节序号 inet_addr(“127.0.0.1”)把字符串点分十进制地址按照网络字节序转换为4字节的地址","text":"sockaddr_in在头文件/usr/include/netinet/in.h123456789101112131415161718192021222324252627282930/* Structure describing an Internet socket address. */struct sockaddr_in &#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr&apos;. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)]; &#125;; or struct sockaddr_in &#123; short int sin_family; /* Address family */ unsigned short int sin_port; /* Port number */ struct in_addr sin_addr; /* Internet address */ unsigned char sin_zero[8]; /* Same size as struct sockaddr */&#125;; struct in_addr &#123; union &#123; struct &#123; u_char s_b1,s_b2,s_b3,s_b4; &#125; S_un_b; struct &#123; u_short s_w1,s_w2; &#125; S_un_w; u_long S_addr; &#125; S_un; #define s_addr S_un.S_addr &#125;; 组成包含协议家族、端口、地址、填充 端口和地址，需要是网络字节序号 inet_addr(“127.0.0.1”)把字符串点分十进制地址按照网络字节序转换为4字节的地址 sockaddr为通用的socket地址1234struct sockaddr &#123; unsigned short sa_family; // address family, AF_INET char sa_data[14]; // 14 bytes of protocol address &#125;; bind、connect、recv、send等socket参数使用的就是这个结构体","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"sockaddr_in","slug":"sockaddr-in","permalink":"https://vcpu.github.io/tags/sockaddr-in/"},{"name":"sockaddr","slug":"sockaddr","permalink":"https://vcpu.github.io/tags/sockaddr/"}]},{"title":"systemtap使用调试记录（二）","slug":"socket_stp","date":"2017-06-20T06:52:23.000Z","updated":"2017-06-20T06:52:23.000Z","comments":true,"path":"socket_stp/","link":"","permalink":"https://vcpu.github.io/socket_stp/","excerpt":"socket sendbubf探究stp脚本 centos7 3.10.0-514.16.1.el7.x86_64 该systap脚本是在调用协议栈sk-&gt;sk_sndbuf可能改变的位置增加探测点，探究snd_buf变更规律使用","text":"socket sendbubf探究stp脚本 centos7 3.10.0-514.16.1.el7.x86_64 该systap脚本是在调用协议栈sk-&gt;sk_sndbuf可能改变的位置增加探测点，探究snd_buf变更规律使用 脚本socket.stp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217%&#123; #include &lt;linux/tcp.h&gt; #include&lt;linux/rtc.h&gt; #include &lt;net/tcp.h&gt; static const char tcp_state_array[][16] = &#123; \"NULL\", \"ESTABLISHED\", \"SYN_SENT\", \"SYN_RECV\", \"FIN_WAIT1\", \"FIN_WAIT2\", \"TIME_WAIT\", \"CLOSE\", \"CLOSE_WAIT\", \"LAST_ACK\", \"LISTEN\", \"CLOSING\" &#125;;%&#125;function get_short_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%02d:%02d:%02d\", tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_full_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%d/%d/%d,%02d:%02d:%02d\", tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday, tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_conn_lifetime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = jiffies_to_msecs(tcp_time_stamp - info-&gt;estab_t);%&#125;function get_conn_data:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct tcp_sock *tp = tcp_sk(sk); struct stap_info *info = sk-&gt;sk_protinfo; u32 len = tp-&gt;snd_nxt - info-&gt;isn; STAP_RETVALUE = len ? len - 1 : len;%&#125;function filter_http_transtime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = info-&gt;http_filter;%&#125;function get_socket_addr:string (sk:long)&#123; laddr = tcpmib_local_addr(sk) lport = tcpmib_local_port(sk) raddr = tcpmib_remote_addr(sk) rport = tcpmib_remote_port(sk) local_addr = sprintf(\"%s:%d\", ip_ntop(htonl(laddr)), lport) remote_addr = sprintf(\"%s:%d\", ip_ntop(htonl(raddr)), rport) return sprintf(\"local=%s,remote=%s\", local_addr, remote_addr)&#125;function get_socket_state:string (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[sk-&gt;sk_state]);%&#125;function get_socket_sk_sndbuf:string(sk:long)%&#123; struct sock *sk=(struct sock*)STAP_ARG_sk; sprintf(STAP_RETVALUE,\"%d:%d\", sk-&gt;sk_wmem_queued, sk-&gt;sk_sndbuf);%&#125;function socket_state_num2str:string (state:long)%&#123; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[STAP_ARG_state]);%&#125;function sshfilter:long(sk:long)&#123; lport = tcpmib_local_port(sk) if(lport == 22) return 1 return 0&#125;probe kernel.function(\"tcp_send_ack\").call&#123; if(sshfilter($sk)) next printf(\"tcp_send_ack[%s]%s state:%s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk));&#125;probe kernel.function(\"tcp_sendmsg\").call&#123; if(sshfilter($sk)) next printf(\"tcp_sendmsg[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_sendmsg\").return&#123; if(sshfilter($sk)) next printf(\"tcp_sendmsg return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/core/sock.c:711\")&#123; if(sshfilter($sk)) next printf(\"sock:711 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/core/sock.c:715\")&#123; if(sshfilter($sk)) next printf(\"sock:715 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/ip_output.c:1581\")&#123; if(sshfilter($sk)) next printf(\"ip_output:1581 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/ip_output.c:1583\")&#123; if(sshfilter($sk)) next printf(\"ip_output:1583 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/tcp_input.c:304\")&#123; if(sshfilter($sk)) next printf(\"tcp_input:304 return [%s]%s state:%s,sndbubf %s sndmem : %d permss %d\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk),$sndmem,$per_mss); print_backtrace()&#125;probe kernel.function(\"__sk_mem_schedule\").call&#123; if(sshfilter($sk)) next printf(\"__sk_mem_schedule[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"__sk_mem_schedule\").return&#123; if(sshfilter($sk)) next printf(\"__sk_mem_schedule return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_page_frag_refill\").call&#123; if(sshfilter($sk)) next printf(\"sk_page_frag_refill[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_page_frag_refill\").return&#123; if(sshfilter($sk)) next printf(\"sk_page_frag_refill return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_stream_alloc_skb\").call&#123; if(sshfilter($sk)) next printf(\"sk_stream_alloc_skb[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_stream_alloc_skb\").return&#123; if(sshfilter($sk)) next printf(\"sk_stream_alloc_skb return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_v4_connect\").call&#123; if(sshfilter($sk)) next printf(\"tcp_v4_connect[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_v4_connect\").return&#123; if(sshfilter($sk)) next printf(\"tcp_v4_connect return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125; 执行步骤stap -g socket.stp 执行结果123456789101112131415161718192021222324252627282930313233[root@localhost stp]# stap -g socket.stpWARNING: Eliding unused function 'filter_http_transtime': identifier 'filter_http_transtime' at socket.stp:68:10 source: function filter_http_transtime:long (sk:long) ^WARNING: Eliding unused function 'get_conn_data': identifier 'get_conn_data' at :58:10 source: function get_conn_data:long (sk:long) ^WARNING: Eliding unused function 'get_conn_lifetime': identifier 'get_conn_lifetime' at :51:10 source: function get_conn_lifetime:long (sk:long) ^WARNING: Eliding unused function 'get_short_time': identifier 'get_short_time' at :22:10 source: function get_short_time:string() ^WARNING: Eliding unused function 'socket_state_num2str': identifier 'socket_state_num2str' at :104:10 source: function socket_state_num2str:string (state:long) ^sock:711 return [2017/6/20,14:42:35]local=0.0.0.0:0,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:16384sock:715 return [2017/6/20,14:42:35]local=0.0.0.0:0,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:32768tcp_v4_connect[2017/6/20,14:42:35]local=0.0.0.0:3000,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:32768tcp_v4_connect return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:SYN_SENT,sndbubf 1280:32768tcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDtcp_sendmsg[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768sk_stream_alloc_skb[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768sk_stream_alloc_skb return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768tcp_sendmsg return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 2304:32768tcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDtcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992","categories":[{"name":"linux kernel","slug":"linux-kernel","permalink":"https://vcpu.github.io/categories/linux-kernel/"}],"tags":[{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"},{"name":"systemtap","slug":"systemtap","permalink":"https://vcpu.github.io/tags/systemtap/"}]},{"title":"socket send","slug":"socketsend1","date":"2017-06-19T04:43:31.000Z","updated":"2017-06-19T04:43:31.000Z","comments":true,"path":"socketsend1/","link":"","permalink":"https://vcpu.github.io/socketsend1/","excerpt":"用户态发送函数列表1234567891011ssize_t send(int sockfd, const void *buf, size_t len, int flags);ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);int sendmmsg(int sockfd, struct mmsghdr *msgvec, unsigned int vlen, unsigned int flags); ssize_t write(int fd, const void *buf, size_t count);","text":"用户态发送函数列表1234567891011ssize_t send(int sockfd, const void *buf, size_t len, int flags);ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);int sendmmsg(int sockfd, struct mmsghdr *msgvec, unsigned int vlen, unsigned int flags); ssize_t write(int fd, const void *buf, size_t count); 发送函数之间差别 send 有连接协议发送数据使用，send第四个参数flags为0时候，等价于write send(sockfd, buf, len, 0) 等价 write（sockfd, buf, len） send是sendto一部分,send可被sendto替换 send(sockfd, buf, len, flags) 等价于 sendto(sockfd, buf, len, flags, NULL, 0) sendto 无连接和有连接发包都可以使用 sendmsg 可替换上树所有的发包函数 123456789struct msghdr &#123; void *msg_name; /* optional address */ socklen_t msg_namelen; /* size of address */ struct iovec *msg_iov; /* scatter/gather array */ size_t msg_iovlen; /* # elements in msg_iov */ void *msg_control; /* ancillary data, see below */ size_t msg_controllen; /* ancillary data buffer len */ int msg_flags; /* flags (unused) */ &#125;; /proc/sys/net/core/optmem_max可控制每个socket的msg_control大小 sendmsg不使用msg_flags参数 send发包过程概述 阻塞模式下 调用send函数时候，比较要发送数据和套接字发送缓冲区长度（net.ipv4.tcp_wmem）；如果发送缓冲区较小，函数直接返回SOCKET_ERR; 1234567891011121314151617181920if send_len &lt;= tcp_wmem&#123; if is sending&#123; wait if network err return SCOKET_ERR &#125; else&#123; if len &gt; tcp_wmem left&#123; wait if network err return SCOKET_ERR &#125; else&#123; copy data to tcp buf if copy err return SCOKET_ERR return copy data size &#125; &#125;&#125; 剩余缓冲区能容纳发送数据，则直接将数据拷贝到缓冲区中，send直接返回。如果剩余缓冲区不足，发送端阻塞等待，对端在协议栈层接收到数据后会发送ack确认，发送方接收到ack后释放缓冲区空间；如果此时剩余缓冲区大小可放置要发送数据，则直接将数据拷入缓冲区，返回。 Tips：阻塞模式下，数据发送正常，其返回的数据长度一定是发送数据的长度。 非阻塞模式下 send函数将数据拷入协议栈缓冲区，如果缓冲区不足，则send尽力拷贝，并返回拷贝大小；如果缓冲区满则返回-1，同时errno为EAGAIN，让发送端再次尝试发送数据。 发送缓冲区设置socklen_t sendbuflen = 0; socklen_t len = sizeof(sendbuflen); getsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len); printf(&quot;default,sendbuf:%d\\n&quot;, sendbuflen); sendbuflen = 10240; setsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len); getsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len); printf(&quot;now,sendbuf:%d\\n&quot;, sendbuflen); send发包实例解析实际socket使用过程中，常用的是非阻塞模式，我们就以非阻塞模式为例进行分析，预设多种场景如下： 场景1：发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理数据，此时发送端因为10k数据发送完毕，关闭了socket。 场景分析： 发送端关闭socket，主动fin告诉对端发送端数据发送完毕想关闭TCP连接，发送完fin后发送端处于fin wait1状态等待接收端ack确认；发送端协议栈剩余8k数据依然在独立发送，待数据发送完成后，协议栈才会把fin发给接收端；接收端在接收ack完10k数据后，且收到fin信号后，接收端回复ack确认fin信号，两者协商关闭socket。 场景2：发送端预期发送10k数据，已将2k数据拷入缓冲区并实际发出拷入的2k数据（收到对端ack），接收端正在处理数据，此时发送端又发送了8k新数据；（缓冲区充足(8k新数据会被拷入缓冲区)情况我们不讨论）缓冲区不足时候会发生什么？ 场景分析 新发送的10k数据会尽力拷入缓冲区，send返回拷入缓冲区数据长度2k，如果此时缓冲区剩余空间为0时候，客户端强制send数据，会收到EAGAIN信号；其实这种情况客户端正确处理方式是读出缓冲区可写信号再发送数据，而不是自己进行发送尝试。 场景3:发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理接收到1k数据，处理完成后数据接收端关闭了socket，会发发生什么？ 场景分析 数据发送端有监听机制，数据发送端用户态会得到接收端端关闭信号（socket可读信号），这时候用户正确打开方式是调用close关闭socket 如果数据发送端未处理该关闭信号，且数据接收端没有rst强制关闭连接，数据发送端仍然可正常发送数据 如果数据发送端未处理该关闭信号，但是数据接收端已经rst强制关闭连接，数据发送端仍然在send发送数据，send将返回-1 如果是阻塞情况，但是因缓冲区满正在阻塞，如果接收端发送rst，阻塞发送端会退出阻塞返回，发送成功字节数，如果在此调用send，将返回-1 场景4：发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理接收到1k数据，此时网络出现异常 场景分析 接收应用程序在处理完已收到的1k数据后,会继续从缓存区读取余下的1k数据,然后就表现为无数据可读的现象,这种情况需要应用程序来处理超时.一般做法是设定一个select等待的最大时间,如果超出这个时间依然没有数据可读,则认为socket已不可用.发送应用程序会不断的将余下的数据发送到网络上,但始终得不到确认,所以缓存区的可用空间持续为0,这种情况也需要应用程序来处理.如果不由应用程序来处理这种情况超时的情况,也可以通过tcp协议本身来处理,具体可以查看sysctl项中的:net.ipv4.tcp_keepalive_intvlnet.ipv4.tcp_keepalive_probesnet.ipv4.tcp_keepalive_time send特点 send只是将数据放入缓冲区中，并不是真正已经发给对方 非阻塞发送字节可以是1-n，其发送多少完全依赖于剩余的发送缓冲区 socket发送函数解析发送流程图 sendsendtosendmmsgsendmsg 上述流程调用过程如下：-&gt;socketcall -&gt;sock_sendmsg -&gt; __sock_sendmsg -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg)-&gt;[tcp_prot]tcp_sendmsg 内核系统调用send 、sendto、sendmsg、sendmmsg发送函数由glibc提供，声明于/usr/include/sys/socket.h用户态在调用后会进入到sys_socketcall系统调用中，下面代码部分就是其入口1234567891011121314151617181920212223242526SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)&#123;... switch (call) &#123; ... case SYS_SEND: err = sys_send(a0, (void __user *)a1, a[2], a[3]); break; case SYS_SENDTO: err = sys_sendto(a0, (void __user *)a1, a[2], a[3], (struct sockaddr __user *)a[4], a[5]); break; ... case SYS_SENDMSG: err = sys_sendmsg(a0, (struct msghdr __user *)a1, a[2]); break; case SYS_SENDMMSG: err = sys_sendmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3]); break; ... default: err = -EINVAL; break; &#125; return err;&#125; send 是sendto的一种特殊情况,(sendto发送地址为NULL发送地址长度为0) 12345SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len, unsigned int, flags)&#123; return sys_sendto(fd, buff, len, flags, NULL, 0);&#125; sendto -&gt; sock_sendmsg -&gt; __sock_sendmsg -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg) 123456789101112SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len)&#123; ... err = sock_sendmsg(sock, &amp;msg, len);out_put: fput_light(sock-&gt;file, fput_needed);out: return err;&#125; sendmsg 和sendmmsg 完成用户态数据拷贝到内核态后，最终也是调用inet_sendmsg处理，在此就拿sendto情况详细分析 sendto源码实现分析sendto -&gt; sock_sendmsg -&gt; “sock_sendmsg” -&gt;”sock_sendmsg_nosec” -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg) 首先分析sock_sendmsg实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253int sock_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)&#123; struct kiocb iocb; struct sock_iocb siocb; int ret; /*异步IO控制块初始化*/ init_sync_kiocb(&amp;iocb, NULL); iocb.private = &amp;siocb; /*异步控制块调用完毕后，可调用__sock_sendmsg发送数据*/ ret = __sock_sendmsg(&amp;iocb, sock, msg, size); if (-EIOCBQUEUED == ret) ret = wait_on_sync_kiocb(&amp;iocb); return ret;&#125;static inline int __sock_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; int err = security_socket_sendmsg(sock, msg, size); /*调用__sock_sendmsg_nosec*/ return err ?: __sock_sendmsg_nosec(iocb, sock, msg, size);&#125;static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock_iocb *si = kiocb_to_siocb(iocb); si-&gt;sock = sock; si-&gt;scm = NULL; si-&gt;msg = msg; si-&gt;size = size; /*调用inet_sendnsg*/ return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);&#125;int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock *sk = sock-&gt;sk; sock_rps_record_flow(sk); /*如果连接没有分配本地端口且允许分配本地端口，我们就给连接绑定一个本地端口 */ /* We may need to bind the socket. */ if (!inet_sk(sk)-&gt;inet_num &amp;&amp; !sk-&gt;sk_prot-&gt;no_autobind &amp;&amp; inet_autobind(sk)) return -EAGAIN; /*传输层是TCP情况下，调用tcp_sendmsg()*/ return sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size);&#125; 其次分析inet_autobind ，获取可用端口并给，获取后的端口会赋值给inet-&gt;inet_sport/inet_num 1234567891011121314151617181920static int inet_autobind(struct sock *sk)&#123; struct inet_sock *inet; /* We may need to bind the socket. */ lock_sock(sk); inet = inet_sk(sk); if (!inet-&gt;inet_num) &#123; /*针对于TCP情况sk-&gt;sk_prot-&gt;get_port调用的是inet_csk_get_port * inet_csk_get_port工作获取端口，并将其赋值给inet-&gt;inet_num */ if (sk-&gt;sk_prot-&gt;get_port(sk, 0)) &#123; release_sock(sk); return -EAGAIN; &#125; /*获取inet-&gt;inet_num赋值给inet-&gt;inet_sport*/ inet-&gt;inet_sport = htons(inet-&gt;inet_num); &#125; release_sock(sk); return 0;&#125; 最后分析tcp_sendmsg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t size)&#123; struct iovec *iov; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; int iovlen, flags, err, copied = 0; int mss_now = 0, size_goal, copied_syn = 0, offset = 0; bool sg; long timeo; lock_sock(sk); flags = msg-&gt;msg_flags; if (flags &amp; MSG_FASTOPEN) &#123; err = tcp_sendmsg_fastopen(sk, msg, &amp;copied_syn, size); if (err == -EINPROGRESS &amp;&amp; copied_syn &gt; 0) goto out; else if (err) goto out_err; offset = copied_syn; &#125; /* * 获取数据发送超时时间 */ timeo = sock_sndtimeo(sk, flags &amp; MSG_DONTWAIT); /* Wait for a connection to finish. One exception is TCP Fast Open * (passive side) where data is allowed to be sent before a connection * is fully established. */ /* * TCP状态检查，ES和CLOSE_WAIT状态才能发送数据，其它状态都要等待连接建立起来 * 否则直接返回错误 * * 随着协议栈进步，增加一种情况tcp_passive_fastopen即tcp被动快速打开时候，不区分当前TCP处于状态 */ if (((1 &lt;&lt; sk-&gt;sk_state) &amp; ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &amp;&amp; !tcp_passive_fastopen(sk)) &#123; /*等待连接建立，连接建立成功则返回0*/ if ((err = sk_stream_wait_connect(sk, &amp;timeo)) != 0) goto do_error; &#125; /*开启repair功能处理*/ if (unlikely(tp-&gt;repair)) &#123; if (tp-&gt;repair_queue == TCP_RECV_QUEUE) &#123; copied = tcp_send_rcvq(sk, msg, size); goto out_nopush; &#125; err = -EINVAL; if (tp-&gt;repair_queue == TCP_NO_QUEUE) goto out_err; /* 'common' sending to sendq */ &#125; /**/ /* This should be in poll */ clear_bit(SOCK_ASYNC_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /*获取发送mss*/ mss_now = tcp_send_mss(sk, &amp;size_goal, flags); /* Ok commence sending. */ iovlen = msg-&gt;msg_iovlen;//应用层要发送数据块个数 iov = msg-&gt;msg_iov;//要发送数据地址 copied = 0;//已经放到缓冲区的数据长度 err = -EPIPE; /*在发送数据前，如果sk已经关闭或者出现err，直接返回 -EPIPE*/ if (sk-&gt;sk_err || (sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) goto out_err; /*网卡分散聚合*/ sg = !!(sk-&gt;sk_route_caps &amp; NETIF_F_SG); while (--iovlen &gt;= 0) &#123; /*获取用户态数据长度和数据指针并指向下一个用户态要发送数据块*/ size_t seglen = iov-&gt;iov_len; unsigned char __user *from = iov-&gt;iov_base; iov++; /*TCP fast open涉及*/ if (unlikely(offset &gt; 0)) &#123; /* Skip bytes copied in SYN */ if (offset &gt;= seglen) &#123; offset -= seglen; continue; &#125; seglen -= offset; from += offset; offset = 0; &#125; while (seglen &gt; 0) &#123; int copy = 0; int max = size_goal; /*从发送队列尾部取skb，尝试将用户态数据放入skb-&gt;data剩余空间*/ skb = tcp_write_queue_tail(sk); if (tcp_send_head(sk)) &#123; /*另一种mss情况，GSO*/ if (skb-&gt;ip_summed == CHECKSUM_NONE) max = mss_now; copy = max - skb-&gt;len; &#125; if (copy &lt;= 0) &#123;/*skb已经装满数据，后续会申请新的skb来发送数据*/new_segment: /* Allocate new segment. If the interface is SG, * allocate skb fitting to single page. */ if (!sk_stream_memory_free(sk)) goto wait_for_sndbuf; /*申请内存大小为select_size（线性数据区+协议头），申请失败或者不合法，睡眠等待*/ skb = sk_stream_alloc_skb(sk, select_size(sk, sg), sk-&gt;sk_allocation); if (!skb) goto wait_for_memory; /* * Check whether we can use HW checksum. * 检查释放网卡硬件释放可以计算校验和 */ if (sk-&gt;sk_route_caps &amp; NETIF_F_CSUM_MASK) skb-&gt;ip_summed = CHECKSUM_PARTIAL; /*将新分配的skb入sk_write_queue数据发送队列*/ skb_entail(sk, skb); copy = size_goal; max = size_goal; /* All packets are restored as if they have * already been sent. skb_mstamp isn't set to * avoid wrong rtt estimation. * TCP repair */ if (tp-&gt;repair) TCP_SKB_CB(skb)-&gt;sacked |= TCPCB_REPAIRED; &#125; /* Try to append data to the end of skb. */ if (copy &gt; seglen) copy = seglen; /* Where to copy to? */ /*如果数据还有线性区间，直接将数据拷入冰计算校验和*/ if (skb_availroom(skb) &gt; 0) &#123; /* We have some space in skb head. Superb! */ copy = min_t(int, copy, skb_availroom(skb)); err = skb_add_data_nocache(sk, skb, from, copy); if (err) goto do_fault; &#125; else &#123;/*如果没有了线性空间*/ /* * 数据会被复制到分页中 * */ bool merge = true; /*取得当前SKB的分片段数*/ int i = skb_shinfo(skb)-&gt;nr_frags; struct page_frag *pfrag = sk_page_frag(sk); /*检查分也可用空间，如果没有就申请新的页，如果系统内存不足就睡眠等待*/ if (!sk_page_frag_refill(sk, pfrag)) goto wait_for_memory; /*如果不能将数据最佳到最后一个分片*/ if (!skb_can_coalesce(skb, i, pfrag-&gt;page, pfrag-&gt;offset)) &#123; /*分页已经达到最大规格，将当前数据发出去，跳到new_segment重新申请skb*/ if (i == MAX_SKB_FRAGS || !sg) &#123; tcp_mark_push(tp, skb); goto new_segment; &#125; merge = false; &#125; copy = min_t(int, copy, pfrag-&gt;size - pfrag-&gt;offset); /*系统对发送缓冲区申请合法性判断*/ if (!sk_wmem_schedule(sk, copy)) goto wait_for_memory; /*拷贝用户空间数据，同时计算校验和，更新数据skb长度和缓存*/ err = skb_copy_to_page_nocache(sk, from, skb, pfrag-&gt;page, pfrag-&gt;offset, copy); if (err) goto do_error; /* Update the skb. */ /*最后一个分页可以放数据数据页被放入了，就更新分也大小记录*/ if (merge) &#123; skb_frag_size_add(&amp;skb_shinfo(skb)-&gt;frags[i - 1], copy); &#125; else &#123; /*如果不能分页就新增页，并初始化*/ skb_fill_page_desc(skb, i, pfrag-&gt;page, pfrag-&gt;offset, copy); get_page(pfrag-&gt;page); &#125; pfrag-&gt;offset += copy; &#125; /*如果复制数据长度为0，不用加PSH标记*/ if (!copied) TCP_SKB_CB(skb)-&gt;tcp_flags &amp;= ~TCPHDR_PSH; /*更新发送队列中最后一个序号，数据包的最后一个序号*/ tp-&gt;write_seq += copy; TCP_SKB_CB(skb)-&gt;end_seq += copy; skb_shinfo(skb)-&gt;gso_segs = 0; /*已经拷入了copy大小数据，用户态指针后移且更新已经拷贝数据增加*/ from += copy; copied += copy; /*所有数据处理完毕，直接退出*/ if ((seglen -= copy) == 0 &amp;&amp; iovlen == 0) goto out; /*如果skbb还可以继续填充数据或者是带外数据或者是有REPAIR选项，继续使用skb*/ if (skb-&gt;len &lt; max || (flags &amp; MSG_OOB) || unlikely(tp-&gt;repair)) continue; /*检查释放必须立即发送，即检查自上次发送后产生的数据是否已经超过对方通告过的最大接收窗口的一半。如果必须发送则设置紧急数据标示，然后将数据发出去*/ if (forced_push(tp)) &#123; tcp_mark_push(tp, skb); __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH); &#125; else if (skb == tcp_send_head(sk)) /*数据不必立即发送，且数据上只存在这段数据，则将这段数据发出*/ tcp_push_one(sk, mss_now); continue;wait_for_sndbuf:/*套接口缓冲区大小超过限制，此时无法再申请skb放数据，我们设置socket满标志*/ set_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /*系统内存不足处理*/wait_for_memory: /*skb分配失败了，已经拷入发送队列数据，直接调用tcp_push发出去 ～MSG_MORE表示无更多数据 TCP_NAGLE_PUSH 选项调用NAGLE，尽量减少小字节发送数据 */ if (copied) tcp_push(sk, flags &amp; ~MSG_MORE, mss_now, TCP_NAGLE_PUSH, size_goal); /*等待内存空闲，超过timeo时间后返回错误*/ if ((err = sk_stream_wait_memory(sk, &amp;timeo)) != 0) goto do_error; /*啊，内存来了，重新获取MSS和TSO，继续将用户态数据拷入缓冲区*/ mss_now = tcp_send_mss(sk, &amp;size_goal, flags); &#125; &#125;out: /*如果数据已经拷入发送队列，则立即发送*/ if (copied) tcp_push(sk, flags, mss_now, tp-&gt;nonagle, size_goal);out_nopush: release_sock(sk); return copied + copied_syn;do_fault: /*复制数据异常时才进入这里 * skb无负载数据，从发送队列上去除，并更新发送队列等参数*/ if (!skb-&gt;len) &#123; tcp_unlink_write_queue(skb, sk); /* It is the one place in all of TCP, except connection * reset, where we can be unlinking the send_head. */ tcp_check_send_head(sk, skb); sk_wmem_free_skb(sk, skb); &#125;do_error: /*如果已经复制了部分数据，即使发生了错误也可以发送，跳到out就是去发送数据去了*/ if (copied + copied_syn) goto out;out_err: err = sk_stream_error(sk, flags, err); release_sock(sk); return err;&#125; tcp_sendmsg()做了以下事情： 如果使用了TCP Fast Open，则会在发送SYN包的同时携带上数据。 如果连接尚未建立好，不处于ESTABLISHED或者CLOSE_WAIT状态， 那么进程进行睡眠，等待三次握手的完成。 获取当前的MSS、网络设备支持的最大数据长度size_goal。 如果支持GSO，size_goal会是MSS的整数倍。 遍历用户层的数据块数组： 4.1 获取发送队列的最后一个skb，如果是尚未发送的，且长度尚未达到size_goal，那么可以往此skb继续追加数据。 4.2 否则需要申请一个新的skb来装载数据。 4.2.1 如果发送队列的总大小sk_wmem_queued大于等于发送缓存的上限sk_sndbuf，或者发送缓存中尚未发送的数据量超过了用户的设置值： 设置同步发送时发送缓存不够的标志。 如果此时已有数据复制到发送队列了，就尝试立即发送。 等待发送缓存，直到sock有发送缓存可写事件唤醒进程，或者等待超时。 4.2.2 申请一个skb，其线性数据区的大小为：通过select_size()得到的线性数据区中TCP负荷的大小 + 最大的协议头长度。 如果申请skb失败了，或者虽然申请skb成功，但是从系统层面判断此次申请不合法， 等待可用内存，等待时间为2~202ms之间的一个随机数。 4.2.3 如果以上两步成功了，就更新skb的TCP控制块字段，把skb加入到sock发送队列的尾部，增加发送队列的大小，减小预分配缓存的大小。 4.3 接下来就是拷贝消息头中的数据到skb中了。如果skb的线性数据区还有剩余空间，就复制数据到线性数据区中，同时计算校验和。 4.4 如果skb的线性数据区已经用完了，那么就使用分页区： 4.4.1 检查分页是否有可用空间，如果没有就申请新的page。如果申请失败，说明系统内存不足。之后会设置TCP内存压力标志，减小发送缓冲区的上限，睡眠等待内存。 4.4.2 判断能否往最后一个分页追加数据。不能追加时，检查分页数是否达到了上限、或网卡不支持分散聚合。如果是的话，就为此skb设置PSH标志。 然后跳转到4.2处申请新的skb，来继续填装数据。 4.4.3 从系统层面判断此次分页发送缓存的申请是否合法。 4.4.4 拷贝用户空间的数据到skb的分页中，同时计算校验和。更新skb的长度字段，更新sock的发送队列大小和预分配缓存。 4.4.5 如果把数据追加到最后一个分页了，更新最后一个分页的数据大小。否则初始化新的分页。 4.5 拷贝成功后更新：发送队列的最后一个序号、skb的结束序号、已经拷贝到发送队列的数据量。 4.6 尽可能的将发送队列中的skb发送出去。 参考 http://blog.csdn.net/zhangskd/article/details/48207553","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"send","slug":"send","permalink":"https://vcpu.github.io/tags/send/"},{"name":"sendto","slug":"sendto","permalink":"https://vcpu.github.io/tags/sendto/"},{"name":"sendmsg","slug":"sendmsg","permalink":"https://vcpu.github.io/tags/sendmsg/"},{"name":"sendmmsg","slug":"sendmmsg","permalink":"https://vcpu.github.io/tags/sendmmsg/"}]},{"title":"centos环境下脚本执行顺序探究","slug":"centos脚本执行顺序","date":"2017-06-15T07:42:10.000Z","updated":"2017-06-15T07:42:10.000Z","comments":true,"path":"centos脚本执行顺序/","link":"","permalink":"https://vcpu.github.io/centos脚本执行顺序/","excerpt":"centos脚本执行顺序通用角度分析，centos 7 系统中存在如下以下5种常用的脚本路径/etc/rc.d/rc.local/etc/profile/etc/bashrc~/.bash_profile~/.bashrc 通过在除rc外的脚本中加入echo信息，reboot虚拟机并ssh登陆用户，打印出顺序如下I am etc profilei am etc bashrci am ~ bash rci am ~ bash profile","text":"centos脚本执行顺序通用角度分析，centos 7 系统中存在如下以下5种常用的脚本路径/etc/rc.d/rc.local/etc/profile/etc/bashrc~/.bash_profile~/.bashrc 通过在除rc外的脚本中加入echo信息，reboot虚拟机并ssh登陆用户，打印出顺序如下I am etc profilei am etc bashrci am ~ bash rci am ~ bash profile 脚本执行顺序和执行时机 脚本路径 执行顺序 执行时机 /etc/rc.d/rc.local 1 系统起机执行一次，后续均不执行 /etc/profile 2 ssh/su/界面登陆时执行 /etc/bashrc 3 ssh/su/界面登陆时执行 ~/.bash_profile 4 ssh/su/界面登陆以当前用户身份登陆 ~/.bashrc 5 ssh/su/界面登陆以当前用户身份登陆 脚本含义rc.local脚本centos启动时候执行脚本，可以用作默认启动/etc/profile和/etc/bashrc 属于系统的全局变量设置~/profile和~/bashrc 属于给予某一个用户的变量设置 profile和bashrc区别 profile 是用户唯一用来设置环境变量的地方，因为用户可能有多种shell（bash、sh、zsh），环境变量没有必要在每种shell都初始化，只需要统一初始化就行，很显然，profile就是这样的地方 bashrc 是专门给bash做初始化设置的，相对应来讲，其它shell会有专门的shrc、zshrc文件存放 开机启动脚本其它说明centos7 默认是没有执行权限的，想在此处加执行脚本，执行脚本不会执行到，需要增添执行权限 123456[root@localhost rc.d]# ls -alt rc.local-rw-r--r--. 1 root root 491 Jun 13 22:24 rc.local[root@localhost rc.d]# chmod +x rc.local[root@localhost qinlong]# ls -alt /etc/rc.d/rc.local-rwxr-xr-x. 1 root root 535 Jun 13 22:48 /etc/rc.d/rc.local","categories":[{"name":"centos","slug":"centos","permalink":"https://vcpu.github.io/categories/centos/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://vcpu.github.io/tags/linux/"},{"name":"centos","slug":"centos","permalink":"https://vcpu.github.io/tags/centos/"}]},{"title":"TIME_WAIT状态分析","slug":"TIME_WAIT状态分析","date":"2017-06-12T10:19:10.000Z","updated":"2017-06-12T10:19:10.000Z","comments":true,"path":"TIME_WAIT状态分析/","link":"","permalink":"https://vcpu.github.io/TIME_WAIT状态分析/","excerpt":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat","text":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat 我猜你一定被巨大无比的TIME_WAIT网络连接总数吓到了！以我个人的经验，对于一台繁忙的Web服务器来说，如果主要以短连接为主，那么其TIME_WAIT网络连接总数很可能会达到几万，甚至十几万。虽然一个TIME_WAIT网络连接耗费的资源无非就是一个端口、一点内存，但是架不住基数大，所以这始终是一个需要面对的问题。 TIMEWAIT是什么因为TCP连接是双向的，所以在关闭连接的时候，两个方向各自都需要关闭。先发FIN包的一方执行的是主动关闭；后发FIN包的一方执行的是被动关闭。主动关闭的一方会进入TIME_WAIT状态，并且在此状态停留两倍的MSL时长。穿插一点MSL的知识：MSL指的是报文段的最大生存时间，如果报文段在网络活动了MSL时间，还没有被接收，那么会被丢弃。关于MSL的大小，RFC 793协议中给出的建议是两分钟，不过实际上不同的操作系统可能有不同的设置，以Linux为例，通常是半分钟，两倍的MSL就是一分钟，也就是60秒，并且这个数值是硬编码在内核中的，也就是说除非你重新编译内核，否则没法修改它： #define TCP_TIMEWAIT_LEN (60*HZ) 如果每秒的连接数是一千的话，那么一分钟就可能会产生六万个TIME_WAIT。为什么主动关闭的一方不直接进入CLOSED状态，而是进入TIME_WAIT状态，并且停留两倍的MSL时长呢？这是因为TCP是建立在不可靠网络上的可靠的协议。例子：主动关闭的一方收到被动关闭的一方发出的FIN包后，回应ACK包，同时进入TIME_WAIT状态，但是因为网络原因，主动关闭的一方发送的这个ACK包很可能延迟，从而触发被动连接一方重传FIN包。极端情况下，这一去一回，就是两倍的MSL时长。如果主动关闭的一方跳过TIME_WAIT直接进入CLOSED，或者在TIME_WAIT停留的时长不足两倍的MSL，那么当被动关闭的一方早先发出的延迟包到达后，就可能出现类似下面的问题： ▪ 旧的TCP连接已经不存在了，系统此时只能返回RST包 ▪ 新的TCP连接被建立起来了，延迟包可能干扰新的连接不管是哪种情况都会让TCP不再可靠，所以TIME_WAIT状态有存在的必要性。 如何控制TIME_WAIT的数量？从前面的描述我们可以得出这样的结论：TIME_WAIT这东西没有的话不行，不过太多可能也是个麻烦事。下面让我们看看有哪些方法可以控制TIME_WAIT数量，这里只说一些常规方法，另外一些诸如SO_LINGER之类的方法太过偏门，略过不谈。ip_conntrack：顾名思义就是跟踪连接。一旦激活了此模块，就能在系统参数里发现很多用来控制网络连接状态超时的设置，其中自然也包括TIME_WAIT：shell&gt; modprobe ip_conntrackshell&gt; sysctl net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait我们可以尝试缩小它的设置，比如十秒，甚至一秒，具体设置成多少合适取决于网络情况而定，当然也可以参考相关的案例。不过就我的个人意见来说，ip_conntrack引入的问题比解决的还多，比如性能会大幅下降，所以不建议使用。 tcp_tw_recycle：顾名思义就是回收TIME_WAIT连接。可以说这个内核参数已经变成了大众处理TIME_WAIT的万金油，如果你在网络上搜索TIME_WAIT的解决方案，十有八九会推荐设置它，不过这里隐藏着一个不易察觉的陷阱：当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。（tcp_tw_recycle和tcp_timestamps导致connect失败问题。同时开启情况下，60s内同一源ip主机socket 请求中timestamp必须是递增的） tcp_tw_reuse：顾名思义就是复用TIME_WAIT连接。当创建新连接的时候，如果可能的话会考虑复用相应的TIME_WAIT连接。通常认为「tcp_tw_reuse」比「tcp_tw_recycle」安全一些，这是因为一来TIME_WAIT创建时间必须超过一秒才可能会被复用；二来只有连接的时间戳是递增的时候才会被复用。官方文档里是这样说的：如果从协议视角看它是安全的，那么就可以使用。这简直就是外交辞令啊！按我的看法，如果网络比较稳定，比如都是内网连接，那么就可以尝试使用。不过需要注意的是在哪里使用，既然我们要复用连接，那么当然应该在连接的发起方使用，而不能在被连接方使用。举例来说：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端，此类情况使用「tcp_tw_reuse」是无效的，因为服务端是被连接方，所以不存在复用连接一说。让我们延伸一点来看，比如说服务端是PHP，它查询另一个MySQL服务端，然后主动断开连接，于是TIME_WAIT就落在了PHP一侧，此类情况下使用「tcp_tw_reuse」是有效的，因为此时PHP相对于MySQL而言是客户端，它是连接的发起方，所以可以复用连接。说明：如果使用tcp_tw_reuse，请激活tcp_timestamps，否则无效。 tcp_max_tw_buckets：顾名思义就是控制TIME_WAIT总数。官网文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为的降低它。如果缩小了它，那么系统会将多余的TIME_WAIT删除掉，日志里会显示：「TCP: time wait bucket table overflow」。需要提醒大家的是物极必反，曾经看到有人把「tcp_max_tw_buckets」设置成0，也就是说完全抛弃TIME_WAIT，这就有些冒险了，用一句围棋谚语来说：入界宜缓。…有时候，如果我们换个角度去看问题，往往能得到四两拨千斤的效果。前面提到的例子：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端。这里的关键在于主动关闭连接的是服务端！在关闭TCP连接的时候，先出手的一方注定逃不开TIME_WAIT的宿命，套用一句歌词：把我的悲伤留给自己，你的美丽让你带走。如果客户端可控的话，那么在服务端打开KeepAlive，尽可能不让服务端主动关闭连接，而让客户端主动关闭连接，如此一来问题便迎刃而解了。 原文连接于https://huoding.com/2013/12/31/316","categories":[{"name":"TCP","slug":"TCP","permalink":"https://vcpu.github.io/categories/TCP/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"}]},{"title":"f-stack安装运行","slug":"f-stack安装","date":"2017-06-12T09:50:55.000Z","updated":"2017-06-22T17:25:55.000Z","comments":true,"path":"f-stack安装/","link":"","permalink":"https://vcpu.github.io/f-stack安装/","excerpt":"mac os virtual box跑f-stack环境搭建基本环境kernel版本3.10.0-514.el7.x86_64CentOS-7-x86_64-Minimal-1611.isoMac osxOreacle Virtual Box5.1.22kernel-devel 操作步骤在Oreacle Virtual Box上安装centos 7虚拟机配置开启NAT网卡和桥接网卡 网卡配置芯片类型选择Add two more virtual network adapters with “Intel PRO/1000 MT Server (82545EM)” type in order to provide virtual network hardware to the virtual machine that is supported by Intel DPDK. 上述说明来自于一篇国外文档说明，使用的Inetl 82545EM，但是经过尝试Intel PRO/1000MT桌面(82540EN)也是可以的。所以不要太迷信啦。","text":"mac os virtual box跑f-stack环境搭建基本环境kernel版本3.10.0-514.el7.x86_64CentOS-7-x86_64-Minimal-1611.isoMac osxOreacle Virtual Box5.1.22kernel-devel 操作步骤在Oreacle Virtual Box上安装centos 7虚拟机配置开启NAT网卡和桥接网卡 网卡配置芯片类型选择Add two more virtual network adapters with “Intel PRO/1000 MT Server (82545EM)” type in order to provide virtual network hardware to the virtual machine that is supported by Intel DPDK. 上述说明来自于一篇国外文档说明，使用的Inetl 82545EM，但是经过尝试Intel PRO/1000MT桌面(82540EN)也是可以的。所以不要太迷信啦。 桥接网卡用来ssh登陆管理串口，NAT网卡用来运行DPDK驱动，跑nginx 在Virtual Box上制作地址映射 centos7虚拟机上网口配置信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s17TYPE=\"Ethernet\"BOOTPROTO=\"dhcp\"DEFROUTE=\"yes\"PEERDNS=\"yes\"PEERROUTES=\"yes\"IPV4_FAILURE_FATAL=\"no\"IPV6INIT=\"yes\"IPV6_AUTOCONF=\"yes\"IPV6_DEFROUTE=\"yes\"IPV6_PEERDNS=\"yes\"IPV6_PEERROUTES=\"yes\"IPV6_FAILURE_FATAL=\"no\"IPV6_ADDR_GEN_MODE=\"stable-privacy\"NAME=\"enp0s17\"UUID=\"2ea1ed66-7bcd-4153-a495-39c25d5f0ff9\"DEVICE=\"enp0s17\"ONBOOT=\"yes\"[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s8TYPE=EthernetBOOTPROTO=dhcpDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8UUID=6c930d05-bc17-4316-998e-f01a7233cbd3DEVICE=enp0s8ONBOOT=yes[root@localhost ~]# ifconfigenp0s8: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.55.183 netmask 255.255.255.0 broadcast 192.168.55.255 inet6 fe80::f97d:539:4010:eaff prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:d5:ee:00 txqueuelen 1000 (Ethernet) RX packets 521 bytes 58437 (57.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 155 bytes 23680 (23.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0enp0s17: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255 inet6 fe80::a15d:3b87:fec0:f3c1 prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:28:39:6c txqueuelen 1000 (Ethernet) RX packets 2 bytes 1180 (1.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 10 bytes 1308 (1.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0``` #### 开启CPU flags(SSE 4.1/SSE 4.2)```shellVBoxManage setextradata \"VM name\" VBoxInternal/CPUM/SSE4.1 1VBoxManage setextradata \"VM name\" VBoxInternal/CPUM/SSE4.2 1 Note: 上述CPU flags默认是不开启的，启动f-stack上ngx会err日志如下 如果不开启sse cpu选项，在启动ngx会报如下问题/usr/local/nginx_fstack/sbin/nginx /data/f-stack/config.ini -c 1 –proc-type=primary –num-procs=1 –proc-id=0ERROR: This system does not support “SSE4_1”.Please check that RTE_MACHINE is set correctly. 做NAT网卡10.0.2.15需要在vbox上做主机地址和端口映射才能访问 安装详细步骤1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253yum install -y git gcc openssl-devel bccd /datayum install kernel-devel-`uname -r` -ymkdir /data/f-stackgit clone https://github.com/F-Stack/f-stack.git /data/f-stack# Compile DPDKcd /data/f-stack/dpdkmake config T=x86_64-native-linuxapp-gcmake# set hugepage echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesmkdir /mnt/hugemount -t hugetlbfs nodev /mnt/huge# insmod komodprobe uioinsmod build/kmod/igb_uio.koinsmod build/kmod/rte_kni.ko# set ip addressexport myaddr=`ifconfig enp0s17 | grep \"inet\" | grep -v \":\" | awk -F ' ' '&#123;print $2&#125;'`export mymask=`ifconfig enp0s17 | grep \"netmask\" | awk -F ' ' '&#123;print $4&#125;'`export mybc=`ifconfig enp0s17 | grep \"broadcast\" | awk -F ' ' '&#123;print $6&#125;'`export myhw=`ifconfig enp0s17 | grep \"ether\" | awk -F ' ' '&#123;print $2&#125;'`export mygw=`route -n | grep 0.0.0.0 | grep enp0s17 | grep UG | awk -F ' ' '&#123;print $2&#125;'`sed \"s/addr=192.168.1.2/addr=$&#123;myaddr&#125;/\" -i /data/f-stack/config.inised \"s/netmask=255.255.255.0/netmask=$&#123;mymask&#125;/\" -i /data/f-stack/config.inised \"s/broadcast=192.168.1.255/broadcast=$&#123;mybc&#125;/\" -i /data/f-stack/config.inised \"s/gateway=192.168.1.1/gateway=$&#123;mygw&#125;/\" -i /data/f-stack/config.ini# Compile F-Stack libexport FF_PATH=/data/f-stackexport FF_DPDK=/data/f-stack/dpdk/buildcd /data/f-stack/libmake# Compile Nginxcd ../app/nginx-1.11.10./configure --prefix=/usr/local/nginx_fstack --with-ff_modulemakemake install# offload NIC（if there is only one NIC，the follow commands must run in a script）ifconfig enp0s17 downpython /data/f-stack/dpdk/tools/dpdk-devbind.py --bind=igb_uio enp0s17# start Nginxcd ../.../start.sh -b /usr/local/nginx_fstack/sbin/nginx -c config.ini 测试在vbox主机上访问映射地址和端口192.168.55.165:8080 -&gt; 10.0.2.15:80curl http://192.168.55.165:808012345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 附录f-stack ngx配置文件/data/f-stack/config.ini123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[dpdk]## Hexadecimal bitmask of cores to run on.lcore_mask=3## Port mask, enable and disable ports.## Default: all ports are enabled.#port_mask=1channel=4## Number of ports.nb_ports=1promiscuous=1numa_on=1## TCP segment offload, default: disabled.tso=0## Port config section## According to dpdk.nb_ports: port0, port1...[port0]addr=10.0.2.15netmask=255.255.255.0broadcast=10.0.2.255gateway=10.0.2.2## Packet capture path, this will hurt performance#pcap=./a.pcap## Kni config: if enabled and method=reject,## all packets that do not belong to the following tcp_port and udp_port## will transmit to kernel; if method=accept, all packets that belong to## the following tcp_port and udp_port will transmit to kernel.#[kni]#enable=1#method=reject#tcp_port=80,443#udp_port=53## FreeBSD network performance tuning configurations.## Most native FreeBSD configurations are supported.[freebsd.boot]hz=100kern.ipc.maxsockets=262144net.inet.tcp.syncache.hashsize=4096net.inet.tcp.syncache.bucketlimit=100net.inet.tcp.tcbhashsize=65536[freebsd.sysctl]kern.ipc.somaxconn=32768kern.ipc.maxsockbuf=16777216net.inet.tcp.fast_finwait2_recycle=1net.inet.tcp.sendspace=16384net.inet.tcp.recvspace=8192net.inet.tcp.nolocaltimewait=1net.inet.tcp.cc.algorithm=htcpnet.inet.tcp.sendbuf_max=16777216net.inet.tcp.recvbuf_max=16777216net.inet.tcp.sendbuf_auto=1net.inet.tcp.recvbuf_auto=1net.inet.tcp.sendbuf_inc=16384net.inet.tcp.recvbuf_inc=524288net.inet.tcp.inflight.enable=0net.inet.tcp.sack=1net.inet.tcp.blackhole=1net.inet.tcp.msl=2000net.inet.tcp.delayed_ack=0net.inet.udp.blackhole=1net.inet.ip.redirect=0 f-stack ngx正常启动信息[root@localhost f-stack]# ./start.sh -b /usr/local/nginx_fstack/sbin/nginx -c config.ini/usr/local/nginx_fstack/sbin/nginx config.ini -c 1 –proc-type=primary –num-procs=1 –proc-id=0 EAL: Detected 1 lcore(s)EAL: Probing VFIO support…EAL: PCI device 0000:00:08.0 on NUMA socket -1EAL: probe driver: 8086:100f rte_em_pmdEAL: PCI device 0000:00:11.0 on NUMA socket -1EAL: probe driver: 8086:100f rte_em_pmdcreate mbuf pool on socket 0create ring:arp_ring_0_0 success, 2047 ring entries are now free!Port 0 MAC: 08 00 27 28 39 6cTSO is disabledset port 0 to promiscuous mode ok Checking link status………………..donePort 0 Link Up - speed 1000 Mbps - full-duplexlink_elf_lookup_symbol: missing symbol hash tablelink_elf_lookup_symbol: missing symbol hash tablenetisr_init: forcing maxthreads from 1 to 0Timecounters tick every 10.000 msecTimecounter “ff_clock” frequency 100 Hz quality 1f-stack-0: Ethernet address: 08:00:27:28:39:6c f-stack 环境安装完成后重启后应该重新设置的参数下述命令可放入/etc/rd.c/rd.local启动文件中，在机器重启后执行一次下面环境设置12345678echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesmount -t hugetlbfs nodev /mnt/hugemodprobe uioinsmod /data/f-stack/dpdk/build/kmod/igb_uio.koinsmod /data/f-stack/dpdk/build/kmod/rte_kni.koifconfig enp0s17 downpython /data/f-stack/dpdk/tools/dpdk-devbind.py --bind=igb_uio enp0s17/data/f-stack/start.sh -b /usr/local/nginx_fstack/sbin/nginx -c /data/f-stack/config.ini 如果kernel-devel yum无法找到内核对应版本可去centos官网查找下载wget https://buildlogs.centos.org/c7.1511.00/kernel/20151119220809/3.10.0-327.el7.x86_64/kernel-devel-3.10.0-327.el7.x86_64.rpmrpm -ivh kernel-devel-3.10.0-327.el7.x86_64.rpm","categories":[{"name":"DPDK","slug":"DPDK","permalink":"https://vcpu.github.io/categories/DPDK/"}],"tags":[{"name":"DPDK","slug":"DPDK","permalink":"https://vcpu.github.io/tags/DPDK/"},{"name":"f-stack","slug":"f-stack","permalink":"https://vcpu.github.io/tags/f-stack/"},{"name":"nginx","slug":"nginx","permalink":"https://vcpu.github.io/tags/nginx/"}]},{"title":"bind()实现源码分析","slug":"bind","date":"2017-06-12T09:35:01.000Z","updated":"2017-06-12T09:35:01.000Z","comments":true,"path":"bind/","link":"","permalink":"https://vcpu.github.io/bind/","excerpt":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。","text":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。 sys_bind12345678910111213141516171819202122SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (sock) &#123; err = move_addr_to_kernel(umyaddr, addrlen, &amp;address); if (err &gt;= 0) &#123; err = security_socket_bind(sock, (struct sockaddr *)&amp;address, addrlen); if (!err) err = sock-&gt;ops-&gt;bind(sock, (struct sockaddr *) &amp;address, addrlen);//inet_bind &#125; fput_light(sock-&gt;file, fput_needed); &#125; return err;&#125; sockfd_lookup_light 和move_addr_to_kernel分别为根据fd从当前进程取出socket和把参数从用户空间考入地址空间 bind系统调用最重要函数为sock-&gt;ops-&gt;bind 在TCP协议情况下inet_stream_ops中bind成员函数为inet_bind 后续为对此函数的分析 inet_bind实现较为复杂，现在版本和原始版本相比，支持端口复用了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123; struct sockaddr_in *addr = (struct sockaddr_in *)uaddr; struct sock *sk = sock-&gt;sk; struct inet_sock *inet = inet_sk(sk); struct net *net = sock_net(sk); unsigned short snum; int chk_addr_ret; int err; /* If the socket has its own bind function then use it. (RAW) */ /*raw socket才会用到，tcp_proc无此函数*/ if (sk-&gt;sk_prot-&gt;bind) &#123; err = sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len); goto out; &#125; err = -EINVAL; /*地址长度检验*/ if (addr_len &lt; sizeof(struct sockaddr_in)) goto out; /*bind地址中协议检查，必须是下面两种情况 * 1.绑定的地址协议为AF_INET * 2.绑定协议为0（AF_UNSPEC）同时地址也为0 * 否则直接退出inet_bind ,返回地址不支持错误码 */ if (addr-&gt;sin_family != AF_INET) &#123; /* Compatibility games : accept AF_UNSPEC (mapped to AF_INET) * only if s_addr is INADDR_ANY. */ err = -EAFNOSUPPORT; if (addr-&gt;sin_family != AF_UNSPEC || addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY)) goto out; &#125; /*获取根据IP地址得出地址类型 RTN_LOCAL 本机地址 RTN_MULTICAST 多播 RTN_BROADCAST 广播 RTN_UNICAST */ chk_addr_ret = inet_addr_type(net, addr-&gt;sin_addr.s_addr); /* Not specified by any standard per-se, however it breaks too * many applications when removed. It is unfortunate since * allowing applications to make a non-local bind solves * several problems with systems using dynamic addressing. * (ie. your servers still start up even if your ISDN link * is temporarily down) */ err = -EADDRNOTAVAIL; /* 地址类型必须是本机，多播，组播中的一个，否则直接返回，报地址参数异常 * */ if (!net-&gt;ipv4_sysctl_ip_nonlocal_bind &amp;&amp; !(inet-&gt;freebind || inet-&gt;transparent) &amp;&amp; addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY) &amp;&amp; chk_addr_ret != RTN_LOCAL &amp;&amp; chk_addr_ret != RTN_MULTICAST &amp;&amp; chk_addr_ret != RTN_BROADCAST) goto out; snum = ntohs(addr-&gt;sin_port); err = -EACCES; /* * 要绑定的端口小于1024时候，要求运行该应用程序的为超级权限 * 否则返回并报权限不运行的错误 */ if (snum &amp;&amp; snum &lt; PROT_SOCK &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_BIND_SERVICE)) goto out; /* We keep a pair of addresses. rcv_saddr is the one * used by hash lookups, and saddr is used for transmit. * * In the BSD API these are the same except where it * would be illegal to use them (multicast/broadcast) in * which case the sending device address is used. */ lock_sock(sk); /* Check these errors (active socket, double bind). */ err = -EINVAL; /*bind动作发生在最初状态，其TCP状态是CLOSE且没有绑定过 * 否则直接判别为异常 */ if (sk-&gt;sk_state != TCP_CLOSE || inet-&gt;inet_num) goto out_release_sock; /*inet_rcv_saddr 用作hash表查找使用 *inet_saddr作为发包源地址 *当为广播和组播时候发送地址为0 */ inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr; if (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST) inet-&gt;inet_saddr = 0; /* Use device */ /* Make sure we are allowed to bind here. */ /* TCP时候该函数负责查询该端口是否被使用，没有被使用返回0，否则返回非0 *如果已经被使用，则退出bind函数，并返回地址和端口已经被使用错误-EADDRINUSE *sk-&gt;sk_prot-&gt;get_port= inet_csk_get_port */ if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) &#123; inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0; err = -EADDRINUSE; goto out_release_sock; &#125; /* * 更新sk-&gt;sk_userlocks标记，表明本地地址和端口已经绑定 */ if (inet-&gt;inet_rcv_saddr) sk-&gt;sk_userlocks |= SOCK_BINDADDR_LOCK; if (snum) sk-&gt;sk_userlocks |= SOCK_BINDPORT_LOCK; inet-&gt;inet_sport = htons(inet-&gt;inet_num); inet-&gt;inet_daddr = 0; inet-&gt;inet_dport = 0; sk_dst_reset(sk); err = 0;out_release_sock: release_sock(sk);out: return err;&#125;EXPORT_SYMBOL(inet_bind); 绑定地址长度和协议检查 长度异常返回-EINVAL 表示参数异常，协议不支持 -EAFNOSUPPORT 对绑定地址进行类型检查inet_addr_type，必须是本机地址，组播和广播地址类型 -EADDRNOTAVAIL 否则报地址参数异常 如果端口小于1024 ，必须为超级权限ns_capable 否则 err = -EACCES 权限不允许 sk-&gt;sk_prot-&gt;get_port = inet_csk_get_port 四层端口检查，看是否被使用 更新sk-&gt;skuserlocks标记，代表地址和端口已经被绑定 扩展函数： inet_csk_get_port TCP四层端口检查 inet_addr_type 地址类型判别 ns_capable 超级权限检查 inet_csk_get_port123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202int inet_csk_get_port(struct sock *sk, unsigned short snum)&#123; struct inet_hashinfo *hashinfo = sk-&gt;sk_prot-&gt;h.hashinfo; struct inet_bind_hashbucket *head; struct inet_bind_bucket *tb; int ret, attempts = 5; struct net *net = sock_net(sk); int smallest_size = -1, smallest_rover; kuid_t uid = sock_i_uid(sk); int attempt_half = (sk-&gt;sk_reuse == SK_CAN_REUSE) ? 1 : 0; /*禁止上下半部，防止进程冲突*/ local_bh_disable(); /* * 如果没有bind端口 */ if (!snum) &#123;/*没有指定端口会自动选择端口*/ int remaining, rover, low, high;again: /*获取端口的取值范围*/ inet_get_local_port_range(net, &amp;low, &amp;high);/*后文辉对其进行分析*/ if (attempt_half) &#123; int half = low + ((high - low) &gt;&gt; 1); if (attempt_half == 1) high = half; else low = half; &#125; /*取值范围内端口数*/ remaining = (high - low) + 1; /*随机选择端口*/ smallest_rover = rover = net_random() % remaining + low; smallest_size = -1; do &#123; /*保留端口检查,服务端可以设置 /proc/sys/net/ipv4/ip_local_reserved_ports */ if (inet_is_reserved_local_port(rover)) goto next_nolock;/*端口加1继续*/ /*根据端口号和HASHsize从确定hash桶，并锁住它，后续便利查找*/ head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, rover, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == rover) &#123; /*判断端口是否可以复用，如果可以复用即使在链表中也一样复用*/ if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; (tb-&gt;num_owners &lt; smallest_size || smallest_size == -1)) &#123; /*记录下端口的使用个数和端口*/ smallest_size = tb-&gt;num_owners; smallest_rover = rover; /*系统绑定端口已经超过最大端口数了，要去检查inet_csk_bind_conflict绑定是否存在冲突*/ if (atomic_read(&amp;hashinfo-&gt;bsockets) &gt; (high - low) + 1 &amp;&amp; !inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; /*ok，绑定没有冲突*/ snum = smallest_rover; goto tb_found; &#125; &#125; /*端口冲突检查*/ if (!inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; snum = rover; goto tb_found; &#125; /*此端口在链表中但是不能复用，继续下一个*/ goto next; &#125; break;/*不在bind表中，端口可以使用，直接跳出循环*/ next: spin_unlock(&amp;head-&gt;lock); next_nolock: /*已经找到最大端口了，从最小开始找*/ if (++rover &gt; high) rover = low; &#125; while (--remaining &gt; 0);/*en,最大5次查找机会*/ /* Exhausted local port range during search? It is not * possible for us to be holding one of the bind hash * locks if this test triggers, because if 'remaining' * drops to zero, we broke out of the do/while loop at * the top level, not from the 'break;' statement. */ ret = 1; /*没有找到端口，那就最后一次机会*/ if (remaining &lt;= 0) &#123; if (smallest_size != -1) &#123; snum = smallest_rover; goto have_snum; &#125; if (attempt_half == 1) &#123; /* OK we now try the upper half of the range */ attempt_half = 2; goto again; &#125; goto fail; &#125; /* OK, here is the one we will use. HEAD is * non-NULL and we hold it's mutex. */ /*找到可用的端口了*/ snum = rover; &#125; else &#123; /*指定绑定了端口，在绑定的链表中查找，如果查找到，代表已经被绑定*/have_snum: head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, snum, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == snum) goto tb_found;/*端口已经被绑定*/ &#125; /*在绑定链表中没有发现，后续会创建*/ tb = NULL; goto tb_not_found; tb_found: if (!hlist_empty(&amp;tb-&gt;owners)) &#123; /*要bind的sk标记SK_FORCE_REUSE可以强制复用*/ if (sk-&gt;sk_reuse == SK_FORCE_REUSE) goto success; if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size == -1) &#123; /* 是否可以复用的判别 * fastreuseport Google添加选项&amp;&amp; 已经开启端口复用 &amp;&amp; 当前socket uid和查找到的uid相符合 * 当前socket也可以放到bind hash中，后续会将其加入 */ goto success; &#125; else &#123; ret = 1; /*端口绑定冲突，自动分配端口绑定冲突会走到此处，在自动分配端口时候进行了下列类似判别 *所以此判断基本不会执行知道跳到tb_not_found这个时候tb不为null的 */ if (inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, true)) &#123; if (((sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size != -1 &amp;&amp; --attempts &gt;= 0) &#123; spin_unlock(&amp;head-&gt;lock); goto again; &#125; goto fail_unlock; &#125; &#125; &#125;tb_not_found: ret = 1; /*绑定时没有发现过tb，直接创建一个*/ if (!tb &amp;&amp; (tb = inet_bind_bucket_create(hashinfo-&gt;bind_bucket_cachep, net, head, snum)) == NULL) goto fail_unlock; if (hlist_empty(&amp;tb-&gt;owners)) &#123;/*没有绑定过socket*/ if (sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) tb-&gt;fastreuse = 1; else tb-&gt;fastreuse = 0; /*设置了SO_REUSEPORT选项*/ if (sk-&gt;sk_reuseport) &#123; tb-&gt;fastreuseport = 1; tb-&gt;fastuid = uid; &#125; else tb-&gt;fastreuseport = 0; &#125; else &#123;/*如果绑定过socket*/ if (tb-&gt;fastreuse &amp;&amp; (!sk-&gt;sk_reuse || sk-&gt;sk_state == TCP_LISTEN)) tb-&gt;fastreuse = 0; if (tb-&gt;fastreuseport &amp;&amp; (!sk-&gt;sk_reuseport || !uid_eq(tb-&gt;fastuid, uid))) tb-&gt;fastreuseport = 0; &#125;success:/*找到可用端口，添加绑定表*/ if (!inet_csk(sk)-&gt;icsk_bind_hash) inet_bind_hash(sk, tb, snum);/*sk被放到tb-&gt;owners中*/ WARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb); ret = 0;fail_unlock: spin_unlock(&amp;head-&gt;lock);fail: local_bh_enable(); return ret;&#125; 如果端口为0；则自动选取端口选择过程如下： 先在[low,half] or [half,high]中随机选取一个端口，作为循环获取端口的起始端口，开始以下流程 步骤1: 保留端口检查，不满足，端口加1，重试次数减1，继续从步骤1开始 步骤2: 从当前端口映射的hash桶中取出列表头，遍历检查该端口是否被使用 步骤2-1:没有被使用，直接退出循环，tb为NULL，创建tb，跳转到tb_not_found将该端口连同创建的tb加入该hash桶的链表中，sk也被放到tb-&gt;owners中管理，结束退出 步骤2-2: 端口被使用了，检查端口使用是否冲突 步骤2-2-1:没有冲突，推出循环，跳转到tb_found,复用检查成功，sk被放到tb-&gt;owners中，结束退出 步骤2-2-2:存在冲突，直接端口+1，继续循环查找 步骤3:如果上半部分已经查找完毕，继续[half,high]中选择一个端口，进行步骤1 attempt_halfsk-&gt;sk_reuse == SK_CAN_REUSE 取端口范围 [low ,half]否则 取端口范围 [half,high] 该值会影响上述选择端口的流程从上半端还是从下半端选择端口 如果sk-&gt;sk_reuse被置SK_CAN_REUSE标记则先从下半端开始选择端口 否则直接从上半端选择端口 small_size和small_rover what’s the fuck!!! 疑惑了好久small_size和small_rover在3.10的版本中根本就没有使用基本用不到3.10版本的端口查找原则是确定端口查找区间，随机选择端口，只要该端口能复用就直接使用，已经完全去除了优先选择复用端口数较小的端口这一原则了（3.2kernel）So amazing！这两个变量可以去除了 inet_get_local_port_range1234567891011void inet_get_local_port_range(struct net *net, int *low, int *high)&#123; unsigned int seq; do &#123; seq = read_seqbegin(&amp;net-&gt;ipv4_sysctl_local_ports.lock); *low = net-&gt;ipv4_sysctl_local_ports.range[0]; *high = net-&gt;ipv4_sysctl_local_ports.range[1]; &#125; while (read_seqretry(&amp;net-&gt;ipv4_sysctl_local_ports.lock, seq));&#125; 12sysctl -a|grep ip_local_port_rangenet.ipv4.ip_local_port_range = 32768 60999 上述读取端口范围是用户态的ip_local_port_range，默认是3w多以后的，可以调整此参数扩大端口范围 上述read_seqbegin这种方式读取数据，是一种顺序锁，适用于读多写少的方式用方式，后续专门处博文研究 tcp端口冲突检查inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict1234567891011121314151617181920212223242526272829303132333435const struct inet_connection_sock_af_ops ipv4_specific = &#123; .queue_xmit = ip_queue_xmit, .send_check = tcp_v4_send_check, .rebuild_header = inet_sk_rebuild_header, .sk_rx_dst_set = inet_sk_rx_dst_set, .conn_request = tcp_v4_conn_request, .syn_recv_sock = tcp_v4_syn_recv_sock, .net_header_len = sizeof(struct iphdr), .setsockopt = ip_setsockopt, .getsockopt = ip_getsockopt, .addr2sockaddr = inet_csk_addr2sockaddr, .sockaddr_len = sizeof(struct sockaddr_in), .bind_conflict = inet_csk_bind_conflict,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_ip_setsockopt, .compat_getsockopt = compat_ip_getsockopt,#endif .mtu_reduced = tcp_v4_mtu_reduced,&#125;;static int tcp_v4_init_sock(struct sock *sk)&#123; struct inet_connection_sock *icsk = inet_csk(sk); tcp_init_sock(sk); icsk-&gt;icsk_af_ops = &amp;ipv4_specific;#ifdef CONFIG_TCP_MD5SIG tcp_sk(sk)-&gt;af_specific = &amp;tcp_sock_ipv4_specific;#endif return 0;&#125; 从上文得知inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict 函数是inet_csk_bind_conflict af_ops在tcp_v4_init_sock初始化 inet_csk_bind_conflict分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960int inet_csk_bind_conflict(const struct sock *sk, const struct inet_bind_bucket *tb, bool relax)&#123; struct sock *sk2; int reuse = sk-&gt;sk_reuse; int reuseport = sk-&gt;sk_reuseport; kuid_t uid = sock_i_uid((struct sock *)sk); /* * Unlike other sk lookup places we do not check * for sk_net here, since _all_ the socks listed * in tb-&gt;owners list belong to the same net - the * one this bucket belongs to. */ sk_for_each_bound(sk2, &amp;tb-&gt;owners) &#123; /*不会冲突情况1:socket绑定设备不同*/ if (sk != sk2 &amp;&amp; !inet_v6_ipv6only(sk2) &amp;&amp; (!sk-&gt;sk_bound_dev_if || !sk2-&gt;sk_bound_dev_if || sk-&gt;sk_bound_dev_if == sk2-&gt;sk_bound_dev_if)) &#123; /* *不会冲突情况2:地址不同 */ if ((!reuse || !sk2-&gt;sk_reuse || sk2-&gt;sk_state == TCP_LISTEN) &amp;&amp; (!reuseport || !sk2-&gt;sk_reuseport || (sk2-&gt;sk_state != TCP_TIME_WAIT &amp;&amp; !uid_eq(uid, sock_i_uid(sk2))))) &#123; /* * 不会冲突情况3: * 条件A: (reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state ！= TCP_LISTEN) * 条件B：(reuseport * &amp;&amp; sk2-&gt;sk_reuseport * &amp;&amp;(sk2-&gt;sk_state == TCP_TIME_WAIT || uid_eq(uid, sock_i_uid(sk2)))) * 条件A和条件B只要有一个成立，就不会冲突 * 条件A成立条件： * 链上sock和待检查sock开启地址复用且链上状态不是监听状态 * 条件B成立条件： * 链上sock和待检查sock开启端口复用且链表上状态为TW * 链上sock和待检查sock开启端口复用且两个sock的uid相同 */ if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; /*没有开启relax，要绑定方不能复用，已绑定方不能复用，以绑定方处理监听状态*/ if (!relax &amp;&amp; reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state != TCP_LISTEN) &#123; if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; &#125; &#125; return sk2 != NULL;&#125; 在端口自动选择时可以重用端口条件为： a设备不同b绑定ip地址不同c要绑定sock和已绑定sock地址允许重用，且已绑定socket不处于监听状态d 链上sock和待检查sock开启端口复用且链表上状态为TWe 链上sock和待检查sock开启端口复用且两个sock的uid相同 关于条件c的补充条件：即使c满足，也需要看relax的值确定，relax为TRUE时可复用，为fase时候不能复用 自动端口时候relax为false，所以条件c消失，仅仅剩下a、b、d、e四个条件","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"bind","slug":"bind","permalink":"https://vcpu.github.io/tags/bind/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"}]},{"title":"connect()实现源码分析","slug":"connect","date":"2017-06-09T10:59:10.000Z","updated":"2017-06-09T10:59:10.000Z","comments":true,"path":"connect/","link":"","permalink":"https://vcpu.github.io/connect/","excerpt":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 实例:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr)","text":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 实例:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr) 系统调用12345678910111213141516171819202122232425262728293031323334353637383940414243SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)&#123; unsigned long a[AUDITSC_ARGS]; unsigned long a0, a1; int err; unsigned int len; if (call &lt; 1 || call &gt; SYS_SENDMMSG) return -EINVAL; len = nargs[call]; if (len &gt; sizeof(a)) return -EINVAL; /* copy_from_user should be SMP safe. */ if (copy_from_user(a, args, len)) return -EFAULT; err = audit_socketcall(nargs[call] / sizeof(unsigned long), a); if (err) return err; a0 = a[0]; a1 = a[1]; switch (call) &#123; case SYS_SOCKET: err = sys_socket(a0, a1, a[2]); break; case SYS_BIND: err = sys_bind(a0, (struct sockaddr __user *)a1, a[2]); break; case SYS_CONNECT: err = sys_connect(a0, (struct sockaddr __user *)a1, a[2]); break; ... default: err = -EINVAL; break; &#125; return err;&#125; 系统调用sys_socketcall会携带（fd,serveraddr,serveraddrlen）参数 系统中断处理函数sys_socketcall会将参数从用户态考入到内核态局部变量a中 调用sys_connect函数 sys_connect(a0, (struct sockaddr __user *)a1, a[2]); sys_connect执行入口分析123456789101112131415161718192021222324SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,int,addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; err = move_addr_to_kernel(uservaddr, addrlen, &amp;address); if (err &lt; 0) goto out_put; err = security_socket_connect(sock, (struct sockaddr *)&amp;address, addrlen); if (err) goto out_put; err = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)&amp;address, addrlen, sock-&gt;file-&gt;f_flags);out_put: fput_light(sock-&gt;file, fput_needed);out: return err; 根据fd描述符号从当前进程current的files指针中的struct fd_table中的fd成员取出file fdt-&gt;fd是一个数组用来管理当前进程的file指针 从file中privatedata中获取到socket变量 把connect连接的服务端地址存入内核空间中move_addr_to_kernel sock-&gt;ops-&gt;connect 以tco为例，此处会调用inet_stream_connect 函数集合中的inet_stream_connect inet_stream_connect分析12345678910int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; int err; lock_sock(sock-&gt;sk); err = __inet_stream_connect(sock, uaddr, addr_len, flags); release_sock(sock-&gt;sk); return err;&#125; inet_stream_connect() 为tcp socket时候connect动作调用的函数改函数会调用__inet_stream_connect函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; struct sock *sk = sock-&gt;sk; int err; long timeo; //socket地址长度检查，不合法返回 if (addr_len &lt; sizeof(uaddr-&gt;sa_family)) return -EINVAL; // 地址协议族检查，如果不合法则关闭连接 if (uaddr-&gt;sa_family == AF_UNSPEC) &#123; err = sk-&gt;sk_prot-&gt;disconnect(sk, flags); sock-&gt;state = err ? SS_DISCONNECTING : SS_UNCONNECTED; goto out; &#125; switch (sock-&gt;state) &#123; //非法参数 default: err = -EINVAL; goto out; //该socket和对端连接已经建立 case SS_CONNECTED: err = -EISCONN; goto out; //该socket和对端连接建立中 case SS_CONNECTING: err = -EALREADY; /* Fall out of switch with err, set for this state */ break; //该socket和对未连接 case SS_UNCONNECTED: err = -EISCONN; //如果未连接，但是socket还不是TCP_CLOSE状态错误返回 if (sk-&gt;sk_state != TCP_CLOSE) goto out; //tcp调用tcp_v4_connect，发送syn err = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len); if (err &lt; 0) goto out; //发送syn后sock状态从未连接更新为连接中 sock-&gt;state = SS_CONNECTING; /* Just entered SS_CONNECTING state; the only * difference is that return value in non-blocking * case is EINPROGRESS, rather than EALREADY. */ err = -EINPROGRESS; break; &#125; //默认情况下未设置非阻塞socket标志，timeo不为0，设置非阻塞，该值为0 timeo = sock_sndtimeo(sk, flags &amp; O_NONBLOCK); //发送syn后等待后续握手完成 /* * 阻塞socket * inet_wait_for_connect 会等待协议栈层的处理 * 1.等待超过timeo，connect返回EINPROGRESS 表明正在处理 * 2.收到信号 * 3.正常完成握手，返回0 * 非阻塞socket * 直接退出connect函数并返回EINPROGRESS，表明协议栈正在处理 */ if ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; int writebias = (sk-&gt;sk_protocol == IPPROTO_TCP) &amp;&amp; tcp_sk(sk)-&gt;fastopen_req &amp;&amp; tcp_sk(sk)-&gt;fastopen_req-&gt;data ? 1 : 0; /* Error code is set above */ if (!timeo || !inet_wait_for_connect(sk, timeo, writebias)) goto out; err = sock_intr_errno(timeo); if (signal_pending(current)) goto out; &#125; /* Connection was closed by RST, timeout, ICMP error * or another process disconnected us. */ if (sk-&gt;sk_state == TCP_CLOSE) goto sock_error; /* sk-&gt;sk_err may be not zero now, if RECVERR was ordered by user * and error was received after socket entered established state. * Hence, it is handled normally after connect() return successfully. */ //TCP握手完成，连接已经建立 sock-&gt;state = SS_CONNECTED; err = 0;out: return err;//异常处理，关闭连接sock_error: err = sock_error(sk) ? : -ECONNABORTED; sock-&gt;state = SS_UNCONNECTED; if (sk-&gt;sk_prot-&gt;disconnect(sk, flags)) sock-&gt;state = SS_DISCONNECTING; goto out;&#125; __inet_stream_connect检查地址长度和协议族 检查sock状态，正常情况下状态为SS_UNCONNECTED sk-&gt;sk_prot-&gt;connect tcp_v4_connect来发送syn 在syn包发完以后会有两种处理情况 情况1:立即返回，针对于非阻塞socket，此时协议栈正在处理握手connect会返回-EINPROGRESS情况2:阻塞运行 阻塞时间超时后，connect返回-EINPROGRESS收到信号，connect返回-ERESTARTSYS,-EINTR inet_wait_for_connect函数分析12345678910111213141516171819202122232425262728293031323334353637static long inet_wait_for_connect(struct sock *sk, long timeo, int writebias)&#123; DEFINE_WAIT(wait); prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); sk-&gt;sk_write_pending += writebias; /* Basic assumption: if someone sets sk-&gt;sk_err, he _must_ * change state of the socket from TCP_SYN_*. * Connect() does not allow to get error notifications * without closing the socket. */ while ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; release_sock(sk);/*等下要睡眠了释放sk锁*/ timeo = schedule_timeout(timeo); /* * 调用schedule_timeout sleep until timeout * 收到信号后，timeout值返回剩余等待时间 * 超时timeout后，返回0 */ /*进程被唤醒后新上sk锁*/ lock_sock(sk); /*进程有带处理信号，或者睡眠超时，推出循环*/ if (signal_pending(current) || !timeo) break; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); &#125; /*等待结束后，将进程从等待队列删除，标记为TASK_RUNNING*/ finish_wait(sk_sleep(sk), &amp;wait); sk-&gt;sk_write_pending -= writebias; return timeo;&#125; 睡眠前进程被设置成TASK_INTERRUPTIBLE状态 SO_SNDTIMEO选项对上述的睡眠非常重要 SO_SNDTIMEO被设置，则睡眠时间会安装设置值 SO_SNDTIMEO没有被设置，则在没有收到信号前一只阻塞 睡眠结束，进程从睡眠队列中删除，并标记为TASK_RUNNING prepare_to_wait实现分析1234567891011void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)&#123; unsigned long flags; wait-&gt;flags &amp;= ~WQ_FLAG_EXCLUSIVE; spin_lock_irqsave(&amp;q-&gt;lock, flags); if (list_empty(&amp;wait-&gt;task_list)) __add_wait_queue(q, wait); set_current_state(state); spin_unlock_irqrestore(&amp;q-&gt;lock, flags);&#125; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); 把wait放入q队列中，设置当前进程状态为TASK_INTERRUPTIBLE TASK_INTERRUPTIBLE 是一种睡眠信号 标记TASK_INTERRUPTIBLE的信号会被唤醒并处理信号 阻塞socket唤醒机制[root@localhost stp]# stap bt.stp sock_def_wakeup WARNING: Missing unwind data for a module, rerun with ‘stap -d e1000’—————-START————————-In process [swapper/2]RIP: ffffffff81558150RSP: ffff88003fd03970 EFLAGS: 00000246RAX: 0000000000004308 RBX: ffff88003a82a6c0 RCX: 0000000000000000RDX: 0000000050000000 RSI: 0000000000ca00c8 RDI: ffff88003a82a6c0RBP: ffff88003fd03988 R08: ffff88003db89708 R09: ffff88003e001800R10: ffffffff815dabca R11: 0000000000000000 R12: ffff88001bfa3700R13: ffff880002db6762 R14: 0000000000000218 R15: ffff880002db675aFS: 0000000000000000(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 000000008005003bCR2: 00007ffaf3049072 CR3: 000000003b0b7000 CR4: 00000000000406e0 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815cbc09 : tcp_finish_connect+0xc9/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000]—————-END————————- 12345678910111213141516171819202122232425262728293031323334353637383940void tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123; struct tcp_sock *tp = tcp_sk(sk); struct inet_connection_sock *icsk = inet_csk(sk); tcp_set_state(sk, TCP_ESTABLISHED); if (skb != NULL) &#123; icsk-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb); security_inet_conn_established(sk, skb); &#125; /* Make sure socket is routed, for correct metrics. */ icsk-&gt;icsk_af_ops-&gt;rebuild_header(sk); tcp_init_metrics(sk); tcp_init_congestion_control(sk); /* Prevent spurious tcp_cwnd_restart() on first data * packet. */ tp-&gt;lsndtime = tcp_time_stamp; tcp_init_buffer_space(sk); if (sock_flag(sk, SOCK_KEEPOPEN)) inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp)); if (!tp-&gt;rx_opt.snd_wscale) __tcp_fast_path_on(tp, tp-&gt;snd_wnd); else tp-&gt;pred_flags = 0; if (!sock_flag(sk, SOCK_DEAD)) &#123; /*握手完成唤醒所有进程*/ sk-&gt;sk_state_change(sk); sk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT); &#125;&#125; sock_def_wakeup -&gt;wake_up_interruptible_all 上述过程发声在三次握手完成后，TCP从syn send或者syn rcv切换到establish状态时候发生 tcp_finish_connect-&gt;sk-&gt;sk_state_change[sock_def_wakeup] 此次唤醒是全部唤醒sk上等待队列的进程","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"}]},{"title":"socket()实现源码分析","slug":"socket","date":"2017-06-09T09:22:34.000Z","updated":"2017-06-22T10:13:08.000Z","comments":true,"path":"socket/","link":"","permalink":"https://vcpu.github.io/socket/","excerpt":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0","text":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0 (1).接口说明：按照顺序可传入如下参数： PF_INEAT SOCK_STREAM,SOCK_DGRAM,SOCK_RAW IPPROTO_TCP,IPPROTO_UDP,IPPROTO_IP 返回值说明 EAFNOSUPPORT 不支持地址类型 EMFILE 进程文件表溢出 ENFILE 核心内存不足无法建立新的socket EINVAL 参数domain/type/protocol不合法 EACCES 权限不允许 ENOBUFS/ENOMEM 内存不足 EPROTONOSUPPORT domain指定的类型不支持参数type或者protocol (2).内核调用栈 (3).结构体说明 struct socket 面向用户态的结构体基于虚拟文件系统创建创建socket时最先创建的结构体 struct sock 网络层socket struct inet_sock INET域socket表示提供INET域的一些属性，TTL、 组播、 地址 、端口 struct raw_socket、struct udp—sock、 struct inet_connection_sock 是对struct inet_sock的扩展struct raw_socket要处理ICMPstruct udp_sock udp协议socketstruct inet_connection_sock面向连接socketstruct tcp_sock TCP协议socket ，对inet_connection_sock扩展，增加了滑动窗口等拥塞控制属性struct inet_timewait_sock网络层超时控制使用struct tcp_timewait_sock TCP协议超时控制使用 (4).struct socket创建源码分析(4.1).sock_alloc函数123456789101112131415161718192021static struct socket *sock_alloc(void)&#123; struct inode *inode; struct socket *sock; inode = new_inode_pseudo(sock_mnt-&gt;mnt_sb); if (!inode) return NULL; sock = SOCKET_I(inode); kmemcheck_annotate_bitfield(sock, type); inode-&gt;i_ino = get_next_ino(); inode-&gt;i_mode = S_IFSOCK | S_IRWXUGO; inode-&gt;i_uid = current_fsuid(); inode-&gt;i_gid = current_fsgid(); inode-&gt;i_op = &amp;sockfs_inode_ops; this_cpu_add(sockets_in_use, 1); return sock;&#125; 一起申请两块内存struct socket和struct inode 两块内存用struct socket_alloc联系起来 inode是linux用来刻画一个存放在内存中的文件的 socket是一种网络文件类型，可以通过文件描述符使用read和write等文件操作函数操作socket 有了inode就支持了虚拟文件系统的操作 (4.2).sock_alloc-&gt;new_inode_pseudo-&gt;alloc_inode12345678910111213141516171819202122232425262728293031323334struct inode *new_inode_pseudo(struct super_block *sb)&#123; struct inode *inode = alloc_inode(sb); if (inode) &#123; spin_lock(&amp;inode-&gt;i_lock); inode-&gt;i_state = 0; spin_unlock(&amp;inode-&gt;i_lock); INIT_LIST_HEAD(&amp;inode-&gt;i_sb_list); &#125; return inode;&#125;static struct inode *alloc_inode(struct super_block *sb)&#123; struct inode *inode; if (sb-&gt;s_op-&gt;alloc_inode) inode = sb-&gt;s_op-&gt;alloc_inode(sb); else inode = kmem_cache_alloc(inode_cachep, GFP_KERNEL); if (!inode) return NULL; if (unlikely(inode_init_always(sb, inode))) &#123; if (inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode) inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode(inode); else kmem_cache_free(inode_cachep, inode); return NULL; &#125; return inode;&#125; alloc_inode获取内存有两种方式 1.通过自己alloc_inode分配 2.从高速缓存中分配 (4.3).alloc_inode -&gt; sock_alloc_inode12345678910111213141516171819202122232425static struct inode *sock_alloc_inode(struct super_block *sb)&#123; struct socket_alloc *ei; struct socket_wq *wq; ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL); if (!ei) return NULL; wq = kmalloc(sizeof(*wq), GFP_KERNEL); if (!wq) &#123; kmem_cache_free(sock_inode_cachep, ei); return NULL; &#125; init_waitqueue_head(&amp;wq-&gt;wait); wq-&gt;fasync_list = NULL; RCU_INIT_POINTER(ei-&gt;socket.wq, wq); ei-&gt;socket.state = SS_UNCONNECTED; ei-&gt;socket.flags = 0; ei-&gt;socket.ops = NULL; ei-&gt;socket.sk = NULL; ei-&gt;socket.file = NULL; return &amp;ei-&gt;vfs_inode;&#125; socket结构体最终会调用上述函数申请内存 该函数会在sock_init中被注册和挂载到系统上 (4.4).sock_init 中sock_allok_inode挂载过程123456789101112131415161718192021222324err = register_filesystem(&amp;sock_fs_type); if (err) goto out_fs; sock_mnt = kern_mount(&amp;sock_fs_type); if (IS_ERR(sock_mnt)) &#123; err = PTR_ERR(sock_mnt); goto out_mount; ... static struct file_system_type sock_fs_type = &#123; .name = \"sockfs\", .mount = sockfs_mount, .kill_sb = kill_anon_super,&#125;;static struct dentry *sockfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)&#123; return mount_pseudo(fs_type, \"socket:\", &amp;sockfs_ops, &amp;sockfs_dentry_operations, SOCKFS_MAGIC);&#125;static const struct super_operations sockfs_ops = &#123; .alloc_inode = sock_alloc_inode, .destroy_inode = sock_destroy_inode, .statfs = simple_statfs,&#125;; sock_init -&gt; register mount -&gt; sock_fs_type-&gt;sockfs_mount-&gt;sockfs_ops-&gt;sock_alloc_node (4.5).pf-&gt;create 即TCP／IP协议族的创建函数inet_create初始化步骤(4.5.1).PF_INET协议族的create函数inet_create会被组册1234567(void)sock_register(&amp;inet_family_ops);static const struct net_proto_family inet_family_ops = &#123; .family = PF_INET, .create = inet_create, .owner = THIS_MODULE,&#125;; (4.5.2).注册过程123456789101112131415161718192021int sock_register(const struct net_proto_family *ops)&#123; int err; if (ops-&gt;family &gt;= NPROTO) &#123; printk(KERN_CRIT \"protocol %d &gt;= NPROTO(%d)\\n\", ops-&gt;family, NPROTO); return -ENOBUFS; &#125; spin_lock(&amp;net_family_lock); if (rcu_dereference_protected(net_families[ops-&gt;family], lockdep_is_held(&amp;net_family_lock))) err = -EEXIST; else &#123; rcu_assign_pointer(net_families[ops-&gt;family], ops); err = 0; &#125; spin_unlock(&amp;net_family_lock); printk(KERN_INFO \"NET: Registered protocol family %d\\n\", ops-&gt;family); return err;&#125; 协议族选项ops会根基协议族类型PF_INET被放置到net_families系统全局变量中 (4.5.3).__sock_create使用过程1234567891011121314151617181920socket.c/__sock_create...rcu_read_lock(); pf = rcu_dereference(net_families[family]); err = -EAFNOSUPPORT; if (!pf) goto out_release; /* * We will call the -&gt;create function, that possibly is in a loadable * module, so we have to bump that loadable module refcnt first. */ if (!try_module_get(pf-&gt;owner)) goto out_release; /* Now protected by module ref count */ rcu_read_unlock(); err = pf-&gt;create(net, sock, protocol, kern); if (err &lt; 0) goto out_module_put; 根据socket传输过来的协议族PF_INET查找全局变量net_families获取ops 通过ops-&gt;create调用inet_create根据具体协议创建网络层socket struct sock (4.6).inet_create都干了什么？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140static int inet_create(struct net *net, struct socket *sock, int protocol, int kern)&#123; struct sock *sk; struct inet_protosw *answer; struct inet_sock *inet; struct proto *answer_prot; unsigned char answer_flags; int try_loading_module = 0; int err; if (protocol &lt; 0 || protocol &gt;= IPPROTO_MAX) return -EINVAL; sock-&gt;state = SS_UNCONNECTED;//步骤1:设置socket状态SS_UNCONNECTED /* Look for the requested type/protocol pair. */lookup_protocol: err = -ESOCKTNOSUPPORT; rcu_read_lock();／／步骤2:根据socket协议找到inet处理函数 connect、bind、accept、listen、等 list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123; err = 0; /* Check the non-wild match. */ if (protocol == answer-&gt;protocol) &#123; if (protocol != IPPROTO_IP) break; &#125; else &#123; /* Check for the two wild cases. */ if (IPPROTO_IP == protocol) &#123; protocol = answer-&gt;protocol; break; &#125; if (IPPROTO_IP == answer-&gt;protocol) break; &#125; err = -EPROTONOSUPPORT; &#125; if (unlikely(err)) &#123; if (try_loading_module &lt; 2) &#123; rcu_read_unlock(); /* * Be more specific, e.g. net-pf-2-proto-132-type-1 * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM) */ if (++try_loading_module == 1) request_module(\"net-pf-%d-proto-%d-type-%d\", PF_INET, protocol, sock-&gt;type); /* * Fall back to generic, e.g. net-pf-2-proto-132 * (net-pf-PF_INET-proto-IPPROTO_SCTP) */ else request_module(\"net-pf-%d-proto-%d\", PF_INET, protocol); goto lookup_protocol; &#125; else goto out_rcu_unlock; &#125; err = -EPERM; if (sock-&gt;type == SOCK_RAW &amp;&amp; !kern &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_RAW)) goto out_rcu_unlock;／／步骤3: 把协协议的inet操作集合赋值给socket结构的ops sock-&gt;ops = answer-&gt;ops; answer_prot = answer-&gt;prot; answer_flags = answer-&gt;flags; rcu_read_unlock(); WARN_ON(answer_prot-&gt;slab == NULL); err = -ENOBUFS; ／／步骤4:申请struct sock结构体，并切把协议操作集合赋值给sock结构体 ／／sk-&gt;sk_prot = sk-&gt;sk_prot_creator =协议操作集合; sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot); if (sk == NULL) goto out; err = 0; if (INET_PROTOSW_REUSE &amp; answer_flags) sk-&gt;sk_reuse = SK_CAN_REUSE;／／步骤5：inet_sock进行相关初始化 inet = inet_sk(sk); inet-&gt;is_icsk = (INET_PROTOSW_ICSK &amp; answer_flags) != 0; inet-&gt;nodefrag = 0; if (SOCK_RAW == sock-&gt;type) &#123; inet-&gt;inet_num = protocol; if (IPPROTO_RAW == protocol) inet-&gt;hdrincl = 1; &#125; if (net-&gt;sysctl_ip_no_pmtu_disc) inet-&gt;pmtudisc = IP_PMTUDISC_DONT; else inet-&gt;pmtudisc = IP_PMTUDISC_WANT; inet-&gt;inet_id = 0; sock_init_data(sock, sk); sk-&gt;sk_destruct = inet_sock_destruct; sk-&gt;sk_protocol = protocol; sk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv; inet-&gt;uc_ttl = -1; inet-&gt;mc_loop = 1; inet-&gt;mc_ttl = 1; inet-&gt;mc_all = 1; inet-&gt;mc_index = 0; inet-&gt;mc_list = NULL; inet-&gt;rcv_tos = 0; sk_refcnt_debug_inc(sk); if (inet-&gt;inet_num) &#123; /* It assumes that any protocol which allows * the user to assign a number at socket * creation time automatically * shares. */ inet-&gt;inet_sport = htons(inet-&gt;inet_num); /* Add to protocol hash chains. */ sk-&gt;sk_prot-&gt;hash(sk); &#125;／／步骤6:调用协议层初始化函数tcp_v4_init_sock()进行始化 if (sk-&gt;sk_prot-&gt;init) &#123; err = sk-&gt;sk_prot-&gt;init(sk); if (err) sk_common_release(sk); &#125;out: return err;out_rcu_unlock: rcu_read_unlock(); goto out;&#125; 设置socket状态SS_UNCONNECTED 根据协议类型找到具体的协议类型操作集合，例如协议处理函数tcp_proc和inet层处理函数集合inet_stream_ops socket-&gt;ops 获得协议操作集合inet_stream_ops 申请sock，并把tcp_proc赋值给它 sk-&gt;sk_prot = sk-&gt;sk_prot_creator=tcp_proc 把申请的sock和inet_sock进行初始化 sk-&gt;sk_prot-&gt;init(sk) 调用tcp_proc深度初始化TCP相关信息 尽管流程主要干了上述的事情，仍需要深入探究的问题是：a. inet_protosw inet_protosw初始化过程如何？b. inet_sock和sock是什么关系？c. 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ (4.6.1).inet_protosw初始化过程如何？12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485static struct inet_protosw inetsw_array[] =&#123; &#123; .type = SOCK_STREAM, .protocol = IPPROTO_TCP, .prot = &amp;tcp_prot, .ops = &amp;inet_stream_ops, .flags = INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_UDP, .prot = &amp;udp_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_PERMANENT, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_ICMP, .prot = &amp;ping_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_REUSE, &#125;, &#123; .type = SOCK_RAW, .protocol = IPPROTO_IP, /* wild card */ .prot = &amp;raw_prot, .ops = &amp;inet_sockraw_ops, .flags = INET_PROTOSW_REUSE, &#125;&#125;;//inet_init for (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q) inet_register_protosw(q); //inet_protosw放入全局inetsw管理void inet_register_protosw(struct inet_protosw *p)&#123; struct list_head *lh; struct inet_protosw *answer; int protocol = p-&gt;protocol; struct list_head *last_perm; spin_lock_bh(&amp;inetsw_lock); if (p-&gt;type &gt;= SOCK_MAX) goto out_illegal; /* If we are trying to override a permanent protocol, bail. */ answer = NULL; last_perm = &amp;inetsw[p-&gt;type]; list_for_each(lh, &amp;inetsw[p-&gt;type]) &#123; answer = list_entry(lh, struct inet_protosw, list); /* Check only the non-wild match. */ if (INET_PROTOSW_PERMANENT &amp; answer-&gt;flags) &#123; if (protocol == answer-&gt;protocol) break; last_perm = lh; &#125; answer = NULL; &#125; if (answer) goto out_permanent; /* Add the new entry after the last permanent entry if any, so that * the new entry does not override a permanent entry when matched with * a wild-card protocol. But it is allowed to override any existing * non-permanent entry. This means that when we remove this entry, the * system automatically returns to the old behavior. */ list_add_rcu(&amp;p-&gt;list, last_perm);out: spin_unlock_bh(&amp;inetsw_lock); return;out_permanent: pr_err(\"Attempt to override permanent protocol %d\\n\", protocol); goto out;out_illegal: pr_err(\"Ignoring attempt to register invalid socket type %d\\n\", p-&gt;type); goto out;&#125; inet_init 会把inet_protosw方式inet_sw中 inet_protosw很重要，其含有协议的具体操作函数tcp_close,tcp_v4_connect,tcp_recvmsg等 inet_protosw，内还包含inet层操作函数 inet_bind,inet_accept,inet_bind,inet_listen等 (4.6.2). inet_sock和sock是什么关系？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566struct sock *sk_alloc(struct net *net, int family, gfp_t priority, struct proto *prot)&#123; struct sock *sk; sk = sk_prot_alloc(prot, priority | __GFP_ZERO, family); if (sk) &#123; sk-&gt;sk_family = family; /* * See comment in struct sock definition to understand * why we need sk_prot_creator -acme */ sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; sock_lock_init(sk); sock_net_set(sk, get_net(net)); atomic_set(&amp;sk-&gt;sk_wmem_alloc, 1); sock_update_classid(sk); sock_update_netprioidx(sk); &#125; return sk;&#125;static struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority, int family)&#123; struct sock *sk; struct kmem_cache *slab; slab = prot-&gt;slab; if (slab != NULL) &#123; sk = kmem_cache_alloc(slab, priority &amp; ~__GFP_ZERO); if (!sk) return sk; if (priority &amp; __GFP_ZERO) &#123; if (prot-&gt;clear_sk) prot-&gt;clear_sk(sk, prot-&gt;obj_size); else sk_prot_clear_nulls(sk, prot-&gt;obj_size); &#125; &#125; else sk = kmalloc(prot-&gt;obj_size, priority);//申请内存大小为prot的objsize if (sk != NULL) &#123; kmemcheck_annotate_bitfield(sk, flags); if (security_sk_alloc(sk, family, priority)) goto out_free; if (!try_module_get(prot-&gt;owner)) goto out_free_sec; sk_tx_queue_clear(sk); &#125; return sk;out_free_sec: security_sk_free(sk);out_free: if (slab != NULL) kmem_cache_free(slab, sk); else kfree(sk); return NULL;&#125; 从上述sk_alloc -&gt; sk_prot_alloc -&gt; obj_size 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct proto tcp_prot = &#123; .name = \"TCP\", .owner = THIS_MODULE, .close = tcp_close, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, .sendpage = tcp_sendpage, .backlog_rcv = tcp_v4_do_rcv, .release_cb = tcp_release_cb, .hash = inet_hash, .unhash = inet_unhash, .get_port = inet_csk_get_port, .enter_memory_pressure = tcp_enter_memory_pressure, .stream_memory_free = tcp_stream_memory_free, .sockets_allocated = &amp;tcp_sockets_allocated, .orphan_count = &amp;tcp_orphan_count, .memory_allocated = &amp;tcp_memory_allocated, .memory_pressure = &amp;tcp_memory_pressure, .sysctl_wmem = sysctl_tcp_wmem, .sysctl_rmem = sysctl_tcp_rmem, .max_header = MAX_TCP_HEADER, .obj_size = sizeof(struct tcp_sock), .slab_flags = SLAB_DESTROY_BY_RCU, .twsk_prot = &amp;tcp_timewait_sock_ops, .rsk_prot = &amp;tcp_request_sock_ops, .h.hashinfo = &amp;tcp_hashinfo, .no_autobind = true,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_tcp_setsockopt, .compat_getsockopt = compat_tcp_getsockopt,#endif#ifdef CONFIG_MEMCG_KMEM .init_cgroup = tcp_init_cgroup, .destroy_cgroup = tcp_destroy_cgroup, .proto_cgroup = tcp_proto_cgroup,#endif&#125;; struct tcp_sock 包含strcut inet_sock 包含 struct sock 上述结构体为互相包含的关系 实际上在申请sock时候，申请内存大小为tcp_sock大小，也就是说三个结构体共同诞生了 (4.6.3). 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ struct socket会在inet_create函数中获取到ops sock-&gt;ops = answer-&gt;ops;struct sock在sk_allloc函数中获取pro sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; (5).socket与文件系统socket与文件系统关联通过sock_map_fd完成 其步骤如下： 1:获取fd get_unused_fd_flags 该函数从当前进程管理的files获取可用的fd 2:申请file sock_alloc_file 将struct socket放到file的private_data管理 file-&gt;private_data = sock 3:将file根据当前fd安装到current-&gt;files中 files有一个指针fdtfdt-&gt;fd是一个类型为file指针的数组，数组下标为fdrcu_assign_pointer(fdt-&gt;fd[fd], file); 将file安装fd为数组下标放到current-&gt;files管理","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"}]},{"title":"systemtap使用调试记录（一）","slug":"systemtap使用调试记录（一）","date":"2017-06-05T10:15:52.000Z","updated":"2017-06-05T10:15:52.000Z","comments":true,"path":"systemtap使用调试记录（一）/","link":"","permalink":"https://vcpu.github.io/systemtap使用调试记录（一）/","excerpt":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法","text":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法 二、centos7安装方法yum install *.rpm 安装上述3个（debugifo,devel,debuginfo-common）rpm包 yum install systemtap stap -ve &apos;probe begin { log(&quot;hello world&quot;) exit() }&apos; 测试正常结果如下： [root@localhost qinlong]# stap -ve ‘probe begin { log(“hello world”) exit() }’Pass 1: parsed user script and 120 library scripts using 227352virt/40488res/3260shr/37400data kb, in 260usr/30sys/338real ms.Pass 2: analyzed script: 1 probe, 2 functions, 0 embeds, 0 globals using 228540virt/41804res/3420shr/38588data kb, in 10usr/0sys/6real ms.Pass 3: translated to C into “/tmp/stap5CqHmN/stap_f7a5084b8a638f5ce64a31271684ef1f_1133_src.c” using 228672virt/42408res/3996shr/38720data kb, in 0usr/0sys/0real ms.Pass 4: compiled C into “stap_f7a5084b8a638f5ce64a31271684ef1f_1133.ko” in 1000usr/330sys/1247real ms.Pass 5: starting run.hello worldPass 5: run completed in 10usr/40sys/362real ms. 三、通用案例1.函数调用栈打印123456789[root@localhost stp]# cat bt.stp probe kernel.function(@1)&#123; print(&quot;----------------START-------------------------\\n&quot;) printf(&quot;In process [%s]\\n&quot;, execname()) print_regs() print_backtrace() print(&quot;----------------END-------------------------\\n&quot;) exit() &#125; 打印内核函数的调用栈 [root@localhost stp]# stap bt.stp tcp_sendmsg—————-START————————-In process [sshd]RIP: ffffffff815c1ee0RSP: ffff88003d217d28 EFLAGS: 00000202RAX: ffffffff81aa20a0 RBX: ffff88003d217e38 RCX: 0000000000000024RDX: ffff88003d217da8 RSI: ffff88003b3b87c0 RDI: ffff88003d217e38RBP: ffff88003d217d50 R08: 0000000000000000 R09: 0000000000000000R10: ffff88003d217da8 R11: 0000000000000000 R12: ffff88003d217e38R13: 0000000000000001 R14: ffff88003d217e28 R15: ffff8800274d3480FS: 00007f03e5514840(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033CR2: 00007f19c6dc8000 CR3: 0000000035a5c000 CR4: 00000000000406e0 0xffffffff815c1ee0 : tcp_sendmsg+0x0/0xc40 [kernel] 0xffffffff815ed254 : inet_sendmsg+0x64/0xb0 [kernel] 0xffffffff81554e07 : sock_aio_write+0x157/0x180 [kernel] 0xffffffff811fdf3d : do_sync_write+0x8d/0xd0 [kernel] 0xffffffff811fe8a5 : vfs_write+0x1b5/0x1e0 [kernel] 0xffffffff811ff2cf : sys_write+0x7f/0xe0 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel]—————-END————————- 2.函数的调用过程1234567[root@localhost stp]# cat socket-trace.stpprobe kernel.function(&quot;*@net/socket.c&quot;).call&#123; printf(&quot;%s -&gt; %s\\n&quot;,thread_indent(1),ppfunc())&#125;probe kernel.function(&quot;*@net/socket.c&quot;).return&#123; printf(&quot;%s&lt;-%s\\n&quot;,thread_indent(-1),ppfunc())&#125; thread_indent(1) 打印程序名称（线程id）ppfunc() 打印出执行函数符号 kernel.function(“@net/socket.c”).call调用net/socket.c 文件中函数时候会触发函数体执行打印动作kernel.function(“@net/socket.c”).return调用net/socket.c文件中函数执行完成返回后会触发函数体打印动作 [root@localhost stp]# stap socket-trace.stp 0 dndX11(3295): -&gt; SyS_recvmsg 0 dndX11(3295): -&gt; sys_recvmsg 0 dndX11(3295): -&gt; sockfd_lookup_light 0 dndX11(3295):&lt;-sockfd_lookuplight 1 dndX11(3295): -&gt; sys_recvmsg 3 dndX11(3295): -&gt; sock_recvmsg 7 dndX11(3295):&lt;-sock_recvmsg 8 dndX11(3295):&lt;-_sys_recvmsg 9 dndX11(3295):&lt;-sys_recvmsg 10 dndX11(3295):&lt;-SyS_recvmsg25274 dndX11(3295): -&gt; SyS_recvmsg25279 dndX11(3295): -&gt; sys_recvmsg25281 dndX11(3295): -&gt; sockfd_lookup_light25284 dndX11(3295):&lt;-sockfd_lookuplight25285 dndX11(3295): -&gt; sys_recvmsg25288 dndX11(3295): -&gt; sock_recvmsg25291 dndX11(3295):&lt;-sock_recvmsgx 3.打印协议栈函数中某一行数据/home/qinlong/rpmbuild/SOURCES/linux-3.10.0-514.16.1.el7/net/ipv4/tcp.c局部源码如下：12345678910111213141065 int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,1066 size_t size)1067 &#123;1068 struct iovec *iov;1069 struct tcp_sock *tp = tcp_sk(sk);1070 struct sk_buff *skb;1071 int iovlen, flags, err, copied = 0;1072 int mss_now = 0, size_goal, copied_syn = 0, offset = 0;1073 bool sg;1074 long timeo;10751076 lock_sock(sk);10771078 flags = msg-&gt;msg_flags; 12[root@localhost ~]# stap -L &apos;kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;)&apos;kernel.statement(&quot;tcp_sendmsg@net/ipv4/tcp.c:1078&quot;) $iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int $mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 执行上述函数，可确代码具体的函数局部变量12345678910$iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int$mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 根据以上变量打印出size值123[root@localhost ~]# stap -e &apos;probe kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;) &#123;printf(&quot;size %d \\n&quot;,$size)&#125;&apos;size 36size 44","categories":[{"name":"linux kernel","slug":"linux-kernel","permalink":"https://vcpu.github.io/categories/linux-kernel/"}],"tags":[{"name":"systemtap","slug":"systemtap","permalink":"https://vcpu.github.io/tags/systemtap/"}]}]}