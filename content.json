{"meta":{"title":"i博客","subtitle":"仰望星空前，还需脚踏实地。","description":null,"author":"vcpu.me","url":"http://vcpu.me"},"pages":[{"title":"","date":"2017-06-22T09:22:34.000Z","updated":"2017-06-29T10:17:20.000Z","comments":true,"path":"about/index.html","permalink":"http://vcpu.me/about/index.html","excerpt":"","text":"小程一枚，坐标上海浦东 很喜欢写博客，但刚刚行动，文章如有疏漏，多包涵 本博客用于分享个人技术总结和心得 搞过3年的负载均衡产品，同时也接触过审计，FW，DPI等安全设备，也开发过webrtc直播系统等 对于tcp/ip，socket，协议分析，协议栈、webrtc、二三层转发、linux系统、高性能等都有些自己的看法 常年用服务端c语言开发，也会些shell，python，go 最喜欢阅读开源代码，从中体会实现思想、提升自己，有时候读了好代码会惊起一身鸡皮疙瘩 感到遗憾的是自己还没有成为过开源软件的开发者，现在也在努力中 不常在如有事，请此处留言"},{"title":"categories","date":"2017-06-05T04:59:19.000Z","updated":"2017-06-05T05:00:31.000Z","comments":false,"path":"categories/index.html","permalink":"http://vcpu.me/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-06-05T04:50:47.000Z","updated":"2017-06-05T04:51:50.000Z","comments":false,"path":"tags/index.html","permalink":"http://vcpu.me/tags/index.html","excerpt":"","text":""},{"title":"","date":"2017-06-22T14:58:14.000Z","updated":"2017-06-22T14:58:14.000Z","comments":true,"path":"top/index.html","permalink":"http://vcpu.me/top/index.html","excerpt":"","text":"AV.initialize(\"l5u4v8AcoPCp9wlPGVJTG6Ny-gzGzoHsz\", \"RQdYIOd2snvd1muAW65nsn25\"); var time=0 var title=\"\" var url=\"\" var query = new AV.Query('Counter');//表名 query.notEqualTo('id',0); //id不为0的结果 query.descending('time'); //结果按阅读次数降序排序 query.limit(20); //最终只返回10条结果 query.find().then(function (todo) { for (var i=0;i"}],"posts":[{"title":"openvSwitch","slug":"openvswitch","date":"2017-07-31T10:00:00.000Z","updated":"2017-08-02T02:10:38.000Z","comments":true,"path":"openvswitch/","link":"","permalink":"http://vcpu.me/openvswitch/","excerpt":"","text":"openvSwitch简介openvSwitch 是SDN常用的虚拟交换机，其将普通交换机的数据平面和控制平面相分离，SDN交换机只负责数据的转发，而控制指令则由更上一级的控制器下发 OpenvSwitch（虚拟交换机）常用模块 ovs-vswitchd 主要模块、实现交换功能，含有支持流交换的Linux内核模块,基于流交换它和上层controller通信采用OPENFLOW协议，与ovsdb-server通信采用OVSDB协议，和内核通信采用netlink通信支持多个独立datapatch(网桥) 12root 2225 1 0 20:05 ? 00:00:00 ovs-vswitchd: monitoring pid 2226 (healthy)root 2226 2225 0 20:05 ? 00:00:00 ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswitch/ovs-vswitchd.pid --detach --monitor ovsdb-server 虚拟交换机配置信息（接口、交换内容、VLAN等）存放的数据库服务，ovs-vswitchd会根据ovsdb-server中的配置工作。ovsdb-server 和ovs-vswitchd 使用OVSDB(JSON-RPC)方式通信12root 2201 1 0 20:05 ? 00:00:00 ovsdb-server: monitoring pid 2202 (healthy)root 2202 2201 0 20:05 ? 00:00:00 ovsdb-server /etc/openvswitch/conf.db -vconsole:emer -vsyslog:err -vfile:info --remote=punix:/var/run/openvswitch/db.sock --private-key=db:Open_vSwitch,SSL,private_key --certificate=db:Open_vSwitch,SSL,certificate --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert --no-chdir --log-file=/var/log/openvswitch/ovsdb-server.log --pidfile=/var/run/openvswitch/ovsdb-server.pid --detach --monitor openvSwitch编译编译环境：ubuntun16.04123456789101112131415161718192021cat build.shapt-get install build-essential libssl-dev linux-headers-$(uname -r) apt-get install graphviz autoconf automake bzip2 debhelper dh-autoreconf libssl-dev libtool openssl procps python-all python-qt4 python-twisted-conch python-zopeinterface python-six dkms module-assistant ipsec-tools racoon libc6-dev module-init-tools netbase python-argparse uuid-runtime -ygit clone https://github.com/openvswitch/ovs.gitcd ovs./boot.sh./configure --with-linux=/lib/modules/`uname -r`/buildmakemake installmake modules_install/sbin/modprobe openvswitchovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschemaovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detachovs-vsctl --no-wait initovs-vswitchd --pidfile --detach make modules_install 错误ubbuntun16.04报错如下,该错误忽略即可 123456789101112131415161718192021222324252627282930313233343536373839root@controller-VirtualBox:~/ovs# make modules_installcd datapath/linux &amp;&amp; make modules_installmake[1]: Entering directory &apos;/root/ovs/datapath/linux&apos;make -C /lib/modules/4.8.0-36-generic/build M=/root/ovs/datapath/linux modules_installmake[2]: Entering directory &apos;/usr/src/linux-headers-4.8.0-36-generic&apos; INSTALL /root/ovs/datapath/linux/openvswitch.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-geneve.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-gre.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-lisp.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-stt.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-vxlan.koAt main.c:158:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory DEPMOD 4.8.0-36-genericmake[2]: Leaving directory &apos;/usr/src/linux-headers-4.8.0-36-generic&apos;depmod `sed -n &apos;s/#define UTS_RELEASE &quot;\\([^&quot;]*\\)&quot;/\\1/p&apos; /lib/modules/4.8.0-36-generic/build/include/generated/utsrelease.h`make[1]: Leaving directory &apos;/root/ovs/datapath/linux&apos; 建立ovs接口连接两个namespace组成二层网络环境搭建拓扑123456789101112131415161718192021 br0 +--------------------------------------+ +--+ +--+ +---+ | tap1 tap2| +---+ | +--+ +--+ | | | | | | +--------------------------------------+ | | | | | | | | |+------------------+ +-------------------+| tap1 | | tap2 ||192.168.1.102/24 | | 192.168.1.102/24 || | | || | | || | | || namespace ns1 | | namespace ns1 || | | |+------------------+ +-------------------+ 实现脚本1234567891011121314151617181920ip netns add ns1ip netns add ns2ovs-vsctl add-br br0ovs-vsctl add-port br0 tap1 -- set Interface tap1 type=internalip link set tap1 netns ns1ip netns exec ns1 ip link set dev tap1 upovs-vsctl add-port br0 tap2 -- set Interface tap2 type=internalip link set tap2 netns ns2ip netns exec ns2 ip link set dev tap2 upip netns exec ns1 ip addr add 192.168.1.102/24 dev tap1ip netns exec ns2 ip addr add 192.168.1.101/24 dev tap2ip netns exec ns1 ip link set lo upip netns exec ns2 ip link set lo upip netns exec ns1 ping -c 4 192.168.1.101ip netns exec ns1 ping -c 4 192.168.1.102 建立vlan二层网络环境搭建拓扑12345678910111213141516171819202122 br0 trunk vlan tag 10,11 br1 +------------------------+ +------------------------+ | | tag10 tag10 | | | trunk_br0 +-----------------------+trunk_br1 | | +-----------------------+ | | | tag11 tag11 | | |tap1 tap2 | | tap3 | +------------------------+ +------------------------+ |tag 10 tag11| tag10| | | | | | |192.168.1.101/24 | | 192.168.1.102/24 | 192.168.1.103/24 +-------+ +-------+ +-------+ | tap1 | | tap2 | |tap3 | | | | | | | | | | | | | | | | | | | | | | | | | +-------+ +-------+ +-------+ ns1 ns2 ns3 环境实现脚本1234567891011121314151617181920212223242526272829303132333435363738ip netns add ns1ip netns add ns2ovs-vsctl add-br br0ovs-vsctl add-port br0 tap1 -- set Interface tap1 type=internalovs-vsctl set Port tap1 tag=10ip link set tap1 netns ns1ip netns exec ns1 ip link set dev tap1 upovs-vsctl add-port br0 tap2 -- set Interface tap2 type=internalovs-vsctl set Port tap2 tag=11ip link set tap2 netns ns2ip netns exec ns2 ip link set dev tap2 upip netns exec ns1 ip addr add 192.168.1.101/24 dev tap1ip netns exec ns2 ip addr add 192.168.1.102/24 dev tap2ip netns exec ns1 ip link set lo upip netns exec ns2 ip link set lo upovs-vsctl add-br br1ovs-vsctl add-port br1 tap3 -- set Interface tap3 type=internalovs-vsctl add-port br0 trunk_br0 trunks=10,11 -- set Interface trunk_br0 type=patch options:peer=trunk_br1ovs-vsctl add-port br1 trunk_br1 trunks=10,11 -- set Interface trunk_br1 type=patch options:peer=trunk_br0ip netns add ns3ip link set tap3 netns ns3ip netns exec ns3 ip addr add 192.168.1.103/24 dev tap3ip netns exec ns3 ip link set dev tap3 upovs-vsctl set Port tap3 tag=10ip netns exec ns3 ping -c 4 192.168.1.101ip netns exec ns3 ping -c 4 192.168.1.102 说明： br0和br1两个交换机之间连接使用的是patch口，在创建时候需要指明peer（对端口）选项 12ovs-vsctl add-port br0 trunk_br0 trunks=10,11 -- set Interface trunk_br0 type=patch options:peer=trunk_br1ovs-vsctl add-port br1 trunk_br1 trunks=10,11 -- set Interface trunk_br1 type=patch options:peer=trunk_br0 br0和br1两个交换机之间连接在trunk口附加上tag10和tag11 结论 ns3:tap3:vlan10 能ping通ns1:tap1:vlan10 因为ns3和ns1属于同一个vlan；同时无法ping通ns2 ovs vlan报文转发原理探究环境搭建拓扑1234567891011121314151617181920212223242526272829303132 first_ns second_ns third_ns +-----------+ +-----------+ +-----------+ | | | | | | | | | | | | | | | | | | | first_br | second_br | third_br| +-----------+ +-----------+ +-----------+10.0.0.4/24 10.0.0.5/24 | 10.0.0.6/24 | | | | | | |tag 10 | 无 tag | trunk 11,12 +------------------------------------------+ | first_br second_br third_br | | | br0 | | | +------------------------------------------+ | tag 10 | | | | |10.0.0.1/24 +------------+ | | | | | | | | +------------+ ns1 搭建网络脚本1234567891011121314151617181920212223242526272829303132 ovs-vsctl add-br br0ovs-vsctl add-port br0 first_br -- set Interface first_br type=internalovs-vsctl set Port first_br tag=10 ip netns add firstip link set first_br netns firstip netns exec first ip addr add 10.0.0.4/24 dev first_brip netns exec first ip link set dev first_br upip netns add ns1ovs-vsctl add-port br0 tap1 -- set Interface tap1 type=internalovs-vsctl set Port tap1 tag=10ip link set tap1 netns ns1ip netns exec ns1 ip link set lo upip netns exec ns1 ip link set dev tap1 upip netns exec ns1 ip addr add 10.0.0.1/24 dev tap1ovs-vsctl add-port br0 second_br -- set Interface second_br type=internalip netns add secondip link set second_br netns secondip netns exec second ip addr add 10.0.0.5/24 dev second_brip netns exec second ip link set dev second_br upovs-vsctl add-port br0 third_br trunks=11,12 -- set Interface third_br type=internalip netns add thirdip link set third_br netns thirdip netns exec third ip addr add 10.0.0.6/24 dev third_brip netns exec third ip link set dev third_br up 实验过程：进入netns1，一直ping 10.0.0.4，在netns first、second、third分别抓包 实验记录 first抓取报文12345root@controller-VirtualBox:~# ip netns exec first tcpdump -n -e -i first_br arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on first_br, link-type EN10MB (Ethernet), capture size 262144 bytes15:47:54.636790 9a:03:f1:61:48:9d &gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 10.0.0.4 tell 10.0.0.1, length 2815:47:54.636808 4e:cc:d6:5a:53:f4 &gt; 9a:03:f1:61:48:9d, ethertype ARP (0x0806), length 42: Reply 10.0.0.4 is-at 4e:cc:d6:5a:53:f4, length 28 抓到arp广播包 second抓取报文1234root@controller-VirtualBox:~# ip netns exec second tcpdump -n -e -i second_br arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on second_br, link-type EN10MB (Ethernet), capture size 262144 bytes15:49:40.345271 9a:03:f1:61:48:9d &gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 46: vlan 10, p 0, ethertype ARP, Request who-has 10.0.0.4 tell 10.0.0.1, length 28 抓到arp广播包 third抓取报文123root@controller-VirtualBox:~# ip netns exec third tcpdump -n -e -i third_br arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on third_br, link-type EN10MB (Ethernet), capture size 262144 bytes 没有抓到arp广播包 结论 trunk port（1）这个port不配置tag，配置trunks，如果trunks为空，则所有的VLAN都trunk，也就意味着对于所有的VLAN的包，本身带什么VLAN ID，就是携带者什么VLAN ID，（2）如果没有设置VLAN，就属于VLAN 0，全部允许通过。（3）如果trunks不为空，则仅仅带着这些VLAN ID的包通过。 access port（1）这个port配置tag，从这个port进来的包会被打上这个tag，（2）从其他的trunk port中进来的本身就带有VLAN ID的包，如果VLAN ID等于tag，则会从这个port发出，（3）从其他的access port上来的包，如果tag相同，也会被forward到这个port。（4）从access port发出的包不带VLAN ID。（5）如果一个本身带VLAN ID的包到达access port，即便VLAN ID等于tag，也会被抛弃。 ovs bonding链路冗余实验拓扑123456789101112131415161718192021 192.168.0.101/24+---------+ +-------------------------------+| | | || tap0+------------+tap0 br0 || | ++ || | | +-------------+ || | +---------|bond0|---------------++---------+ +-------------+ ns1 br0_tap0 | | br0_tap1 | | | | br1_tap0 | | br1_tap1 192.168.0.102/24 +--------------++---------+ +---------|bond1|---------------+| +------------+ +--------------+ || | | || | | br1 || | | || | +-------------------------------++---------+ ns2 环境搭建脚本1234567891011121314151617181920212223242526272829ovs-vsctl add-br br0ovs-vsctl add-br br1ip link add br0_tap0 type veth peer name br1_tap0ip link add br0_tap1 type veth peer name br1_tap1ip link set br0_tap0 upip link set br0_tap1 upip link set br1_tap0 upip link set br1_tap1 upovs-vsctl add-bond br0 bond0 br0_tap0 br0_tap1 ovs-vsctl add-bond br1 bond1 br1_tap0 br1_tap1ip netns add ns1ip netns add ns2ovs-vsctl add-port br0 tap1 -- set Interface tap1 type=internalip link set tap1 netns ns1ip netns exec ns1 ip link set dev tap1 upip netns exec ns1 ip addr add 192.168.1.101/24 dev tap1ovs-vsctl add-port br1 tap2 -- set Interface tap2 type=internalip link set tap2 netns ns2ip netns exec ns2 ip link set dev tap2 upip netns exec ns2 ip addr add 192.168.1.102/24 dev tap2ovs-vsctl set Port bond0 lacp=active ovs-vsctl set Port bond1 lacp=active 环境搭建完成后查看bond12345678910111213141516171819202122232425262728293031323334root@controller-VirtualBox:~# ovs-appctl bond/show---- bond1 ----bond_mode: active-backupbond may use recirculation: no, Recirc-ID : -1bond-hash-basis: 0updelay: 0 msdowndelay: 0 mslacp_status: negotiatedlacp_fallback_ab: falseactive slave mac: 9e:d9:94:98:26:85(br1_tap0)slave br1_tap0: enabled active slave may_enable: trueslave br1_tap1: enabled may_enable: true---- bond0 ----bond_mode: active-backupbond may use recirculation: no, Recirc-ID : -1bond-hash-basis: 0updelay: 0 msdowndelay: 0 mslacp_status: negotiatedlacp_fallback_ab: falseactive slave mac: 6a:d8:f8:f3:2b:6d(br0_tap0)slave br0_tap0: enabled active slave may_enable: trueslave br0_tap1: enabled may_enable: true 查看结论： 默认建立的bond为主备模式（bond_mode: active-backup）br1_tap0和br0_tap0 流量走这对口 抓包结果123456789101112131415161718抓取 br0_tap0root@controller-VirtualBox:~# tcpdump -n -e -i br0_tap0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br0_tap0, link-type EN10MB (Ethernet), capture size 262144 bytes17:12:45.022854 6a:d8:f8:f3:2b:6d &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:12:45.023409 9e:d9:94:98:26:85 &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:13:15.024627 6a:d8:f8:f3:2b:6d &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:13:15.025299 9e:d9:94:98:26:85 &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:13:45.025411 6a:d8:f8:f3:2b:6d &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:13:45.025724 9e:d9:94:98:26:85 &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 110在ns1 ping ns2，流量只走在主机接口上（br1_tap0和br0_tap0）root@controller-VirtualBox:~# tcpdump -n -e -i br0_tap0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br0_tap0, link-type EN10MB (Ethernet), capture size 262144 bytes17:15:29.980585 d2:25:e0:f4:59:83 &gt; 3a:df:e7:d3:68:84, ethertype IPv4 (0x0800), length 98: 192.168.1.101 &gt; 192.168.1.102: ICMP echo request, id 6006, seq 4, length 6417:15:29.980608 3a:df:e7:d3:68:84 &gt; d2:25:e0:f4:59:83, ethertype IPv4 (0x0800), length 98: 192.168.1.102 &gt; 192.168.1.101: ICMP echo reply, id 6006, seq 4, length 6417:15:31.003922 d2:25:e0:f4:59:83 &gt; 3a:df:e7:d3:68:84, ethertype IPv4 (0x0800), length 98: 192.168.1.101 &gt; 192.168.1.102: ICMP echo request, id 6006, seq 5, length 64 抓取br1_tap01234567root@controller-VirtualBox:~# tcpdump -n -e -i br1_tap0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br1_tap0, link-type EN10MB (Ethernet), capture size 262144 bytes17:14:15.027630 6a:d8:f8:f3:2b:6d &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:14:15.028468 9e:d9:94:98:26:85 &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:14:45.028801 6a:d8:f8:f3:2b:6d &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 11017:14:45.029541 9e:d9:94:98:26:85 &gt; 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 110 另外补充 查看lacpovs-appctl lacp/show active-backup 主-备 无法提升吞吐 balance-slb, 根据包的 source MAC + vlan tag來均衡流量 banlnce-tcp, 根据包的 L2/L3/L4 header来均衡流量 banlance-tcp必须让硬件交换机设置802.3ad，balance-slb则设不设均可，设了流量提高比较大。 ovs-vsctl set Port bond0 bond_mode=balance-slb 观察流量命令 cat /proc/net/dev LACP是链路汇聚控制协议（具体研究待定） openvswitch概念补充几个重要的概念 Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。 Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。 Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。 Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。 datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。 Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在 flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。 运行原理内核模块实现了多个“数据路径（DataPath）”（类似于网桥），每个都可以有多个“vports”（类似于桥内的端口）。每个数据路径也通过关联流表（flow table）来设置操作，而这些流表中的流都是用户空间在报文头和元数据的基础上映射的关键信息，一般的操作都是将数据包转发到另一个vport。当一个数据包到达一个vport，内核模块所做的处理是提取其流的关键信息并在流表中查找这些关键信息。当有一个匹配的流时它执行对应的操作。如果没有匹配，它会将数据包送到用户空间的处理队列中（作为处理的一部分，用户空间可能会设置一个流用于以后遇到相同类型的数据包可以在内核中执行操作）。细节如下图所示： ovs管理组件 ovs-dpctl：一个工具，用来配置交换机内核模块，可以控制转发规则。 ovs-vsctl：主要是获取或者更改ovs-vswitchd的配置信息，此工具操作的时候会更新ovsdb-server中的数据库。 ovs-appctl：主要是向OVS守护进程发送命令的，一般用不上。 a utility that sends commands to running Open vSwitch daemons (ovs-vswitchd) ovsdbmonitor：GUI工具来显示ovsdb-server中数据信息。（Ubuntu下是可以使用apt-get安装，可以远程获取OVS数据库和OpenFlow的流表） ovs-controller：一个简单的OpenFlow控制器 ovs-ofctl：用来控制OVS作为OpenFlow交换机工作时候的流表内容。 ovs-pki：OpenFlow交换机创建和管理公钥框架； ovs-tcpundump：tcpdump的补丁，解析OpenFlow的消息； brocompat.ko : Linux bridge compatibility module openvswitch.ko : Open vSwitch switching datapath ovs流表实验网络拓扑 拓扑实现脚本12345678910111213141516171819202122232425ovs-vsctl add-br ovs-switchovs-vsctl add-port ovs-switch p0 -- set Interface p0 ofport_request=100ovs-vsctl set Interface p0 type=internalip netns add ns0ip link set p0 netns ns0 ip netns exec ns0 ip addr add 192.168.1.100/24 dev p0ip netns exec ns0 ifconfig p0 promisc upovs-vsctl add-port ovs-switch p1 -- set Interface p1 ofport_request=101ovs-vsctl set Interface p1 type=internalip netns add ns1ip link set p1 netns ns1ip netns exec ns1 ip addr add 192.168.1.101/24 dev p1 ip netns exec ns1 ifconfig p1 promisc upovs-vsctl add-port ovs-switch p2 -- set Interface p2 ofport_request=102ovs-vsctl set Interface p2 type=internalip netns add ns2ip link set p2 netns ns2ip netns exec ns2 ip addr add 192.168.1.102/24 dev p2ip netns exec ns2 ifconfig p2 promisc up ovs-ofctl show ovs-switch 显示虚拟交换机流表的统计信息12345678910111213141516171819202122root@controller-VirtualBox:~# ovs-ofctl show ovs-switchOFPT_FEATURES_REPLY (xid=0x2): dpid:0000aaa801224e40n_tables:254, n_buffers:0capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IPactions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst 100(p0): addr:fc:7f:00:00:36:b8 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max 101(p1): addr:56:a7:a1:27:a9:b8 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max 102(p2): addr:d2:af:d5:d2:c9:52 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max LOCAL(ovs-switch): addr:aa:a8:01:22:4e:40 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps maxOFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0 ovs-dpctl show 查看内核接口状态12345678910root@controller-VirtualBox:~# ovs-dpctl showsystem@ovs-system: lookups: hit:334 missed:22 lost:0 flows: 0 masks: hit:371 total:0 hit/pkt:1.04 port 0: ovs-system (internal) port 1: ovs-switch (internal) port 2: p0 (internal) port 3: p1 (internal) port 4: p2 (internal) ovs-ofctl dump-flows ovs-switch查看ovs-switch的流表接口p1发出去的icmp发包源地址变成10.10.10.10ovs-ofctl del-flows ovs-switch “in_port=100”删除in_port=100的流表接口p1发出去的icmp报文源地址均变成10.10.10.101ovs-ofctl add-flow ovs-switch &quot;priority=1 idle_timeout=0,in_port=100,actions=mod_nw_src:10.10.10.10,normal&quot; 所有的icmp定向到p2(in_port=102)1ovs-ofctl add-flow ovs-switch idle_timeout=0,dl_type=0x0800,nw_proto=1,actions=output:102 在该组网下数据包转发过程测试产生数据包源 p0(06:fc:2c:fd:84:f5) -&gt; p1(56:a7:a1:27:a9:b8)123456789101112root@controller-VirtualBox:~# ovs-appctl ofproto/trace ovs-switch in_port=100,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8 -generateFlow: in_port=100,vlan_tci=0x0000,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000bridge(&quot;ovs-switch&quot;)-------------------- 0. priority 0 NORMAL -&gt; no learned MAC for destination, floodingFinal flow: unchangedMegaflow: recirc_id=0,in_port=100,vlan_tci=0x0000/0x1fff,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000Datapath actions: 1,3,4 数据包转发过程：上述产生的数据包最终被广播到接口1，3，4 port 1: ovs-switch (internal) port 2: p0 (internal) port 3: p1 (internal) port 4: p2 (internal) 配置接口p1为tag101后 产生数据包源 p0(06:fc:2c:fd:84:f5) -&gt; p1(56:a7:a1:27:a9:b8)12345678910111213root@controller-VirtualBox:~# ovs-vsctl set Port p1 tag=101root@controller-VirtualBox:~# ovs-appctl ofproto/trace ovs-switch in_port=100,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8 -generateFlow: in_port=100,vlan_tci=0x0000,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000bridge(&quot;ovs-switch&quot;)-------------------- 0. priority 0 NORMAL -&gt; no learned MAC for destination, floodingFinal flow: unchangedMegaflow: recirc_id=0,in_port=100,vlan_tci=0x0000/0x1fff,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000 数据包转发过程：上述产生数据包最终被广播到接口1，4 因为接口2已经配置了tag101和产生数据包不在同一个广播域 修改从p0口发出的数据包强制加上vlantag 101, 产生数据包p0(06:fc:2c:fd:84:f5) -&gt; p1(56:a7:a1:27:a9:b8)12345678910111213141516ovs-ofctl add-flow ovs-switch &quot;priority=3,in_port=100,dl_vlan=0xffff,actions=mod_vlan_vid:101,normal&quot;root@controller-VirtualBox:~# ovs-appctl ofproto/trace ovs-switch in_port=100,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8 -generateFlow: in_port=100,vlan_tci=0x0000,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000bridge(&quot;ovs-switch&quot;)-------------------- 0. in_port=100,vlan_tci=0x0000, priority 3 mod_vlan_vid:101 NORMAL -&gt; learned that 06:fc:2c:fd:84:f5 is on port p0 in VLAN 101 -&gt; forwarding to learned portFinal flow: in_port=100,dl_vlan=101,dl_vlan_pcp=0,vlan_tci1=0x0000,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000Megaflow: recirc_id=0,in_port=100,vlan_tci=0x0000,dl_src=06:fc:2c:fd:84:f5,dl_dst=56:a7:a1:27:a9:b8,dl_type=0x0000Datapath actions: 3 数据包转发过程 产生的流从p0-&gt;p1 ，数据包被完整转发到port 3 ，也就是说明该数据包被完整添加上了tag101 产生一条p1-&gt;p0流量，该流量携带tag101，发送给接口p0123456789101112root@controller-VirtualBox:~# ovs-appctl ofproto/trace ovs-switch in_port=101,dl_dst=06:fc:2c:fd:84:f5,dl_src=56:a7:a1:27:a9:b8 -generateFlow: in_port=101,vlan_tci=0x0000,dl_src=56:a7:a1:27:a9:b8,dl_dst=06:fc:2c:fd:84:f5,dl_type=0x0000bridge(&quot;ovs-switch&quot;)-------------------- 0. priority 0 NORMAL -&gt; forwarding to learned portFinal flow: unchangedMegaflow: recirc_id=0,in_port=101,vlan_tci=0x0000/0x1fff,dl_src=56:a7:a1:27:a9:b8,dl_dst=06:fc:2c:fd:84:f5,dl_type=0x0000Datapath actions: push_vlan(vid=101,pcp=0),2 ovs配合使用Controller之一 Floodlight操作系统环境：ubuntun 16.04ovs版本信息：(Open vSwitch) 2.5.210.30.10.145/24 依赖包安装12apt-get install build-essential ant maven python-devapt-get install build-essential openjdk-8-jdk ant maven python-dev 下载floodlight12345678git clone git://github.com/floodlight/floodlight.gitcd floodlight/git submodule initgit submodule updateantjava -jar target/floodlight.jar# nohup java -jar target/floodlight.jar &gt; floodlight.log 2&gt;&amp;1 &amp; 在被管理的ovs上虚拟机上执行加入命令ovs-vsctl set-controller ovs-switch tcp:10.30.10.145:6653ovs-vsctl set Bridge ovs-switch fail-mode=secure 其它说明：（1）10.30.10.145为floodlight controller的安装服务地址（2）一堆教程说端口是6633，新版本发生了变化更改为了6653，具体变化版本号未研究（3）采用的是secure，不停的连接Controller 当 OVS 交换机连接到 Floodlight 控制器后，理论上所有的流表规则应该交给控制器来建立。由于 OVS 交换机和控制器之间是通过网络通讯来传递数据的，所以网络连接失败会影响到 Flow 的建立，为了处理该问题提供两种模式： standlone: 默认模式。如果 OVS 交换机超过三次无法正常连接到 OpenFlow 控制器，OVS 交换机自己会负责建立流表。在这种模式下，OVS 和常见的 L2 交换机相似。与此同时，OVS 也会继续尝试连接控制器，一旦网络连接恢复，OVS 会再次切换到使用控制器进行流表管理。 secure: 在 secure 模式下，如果 OVS 无法正常连接到 OpenFlow 控制器，OVS 会不停的尝试与控制器重新建立连接，而不会自己负责建立流表。 访问测试http://10.30.10.145:8080/ui/pages/index.html 备注说明博文的篇幅太长有碍观瞻，深入研究内容请见后续博文openvSwitch XX。","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://vcpu.me/tags/ovs/"},{"name":"虚拟交换机","slug":"虚拟交换机","permalink":"http://vcpu.me/tags/虚拟交换机/"},{"name":"ubuntun16.04","slug":"ubuntun16-04","permalink":"http://vcpu.me/tags/ubuntun16-04/"}]},{"title":"iptables","slug":"iptables","date":"2017-07-27T10:00:00.000Z","updated":"2017-07-27T03:24:52.000Z","comments":true,"path":"iptables/","link":"","permalink":"http://vcpu.me/iptables/","excerpt":"filter表根据已配置好的规则操作本机的包（INPUT）、转发的包（FORWARD）、本机发出的包（OUTPUT）；最终决定是否放行（ACCEPT）、丢弃（DROP）、拒绝（REJECT）、产生告警日志（LOG）。 nat表根据配置好的规则，在包刚刚到达FW时修改目的地址（PREROUTING）、本机产生包修改目的地址（OUTPUT）、离开转发系统前修改源地址（POSTROUTING）；最终实现网络SNAT、REDIRECT、DNAT mangle表主要用于修改数据包的TOS（Type Of Service，服务类型）、TTL（Time To Live，生存周期）指以及为数据包设置Mark标记，以实现Qos(Quality Of Service，服务质量)调整以及策略路由等应用，由于需要相应的路由设备支持，因此应用并不广泛。包含五个规则链——PREROUTING，POSTROUTING，INPUT，OUTPUT，FORWARD。","text":"filter表根据已配置好的规则操作本机的包（INPUT）、转发的包（FORWARD）、本机发出的包（OUTPUT）；最终决定是否放行（ACCEPT）、丢弃（DROP）、拒绝（REJECT）、产生告警日志（LOG）。 nat表根据配置好的规则，在包刚刚到达FW时修改目的地址（PREROUTING）、本机产生包修改目的地址（OUTPUT）、离开转发系统前修改源地址（POSTROUTING）；最终实现网络SNAT、REDIRECT、DNAT mangle表主要用于修改数据包的TOS（Type Of Service，服务类型）、TTL（Time To Live，生存周期）指以及为数据包设置Mark标记，以实现Qos(Quality Of Service，服务质量)调整以及策略路由等应用，由于需要相应的路由设备支持，因此应用并不广泛。包含五个规则链——PREROUTING，POSTROUTING，INPUT，OUTPUT，FORWARD。 iptables工作的netfilter框架挂载点转发流程 1.数据包从网络A经过网卡中断送入内核协议栈，首先进入prerouting挂载点，路由前的操作可加入到此挂载点完成，如图的对目的地址的修改，修改后的地址会跟进修改后的地址会继续进入转发路由查找但是查找路由的目的地址变成了目的NAT后的地 2.经过转发路由查找，如果是发给本机的报文，交给本机的应用程序处理；在具体的应用程序处理之前会经过input挂载点，如图你可以进行上本机的报文限制，直接在input挂载点丢弃报文，使其无法到达具体的应用程序。 2-1.应用程序在处理完成后，根据本机路由表酱数据发出，在本机报文发出前，会经过output挂载点，此处你的策略只针对于本机发出报文有效。如果你想修改本机发出报文的源地址，你需要在此实现。 3.经过转发路由查找，如果是转发的报文，在具体的转发报文之前，你可以通过在forward挂载点操作进行过滤等动作。 4.无论是转发报文还是本机发出的报文，它们最终会经过postrouting挂载点，送到最终物理发包流程；也就是说postrouting是数据包离开本netfilter协议栈的最后一个流程，你可以在这个流程中修改源IP地址。 iptables基本命令 [-t 表名]：该规则所操作的哪个表，可以使用filter、nat等，如果没有指定则默认为filter -A：新增一条规则，到该规则链列表的最后一行 -I：插入一条规则，原本该位置上的规则会往后顺序移动，没有指定编号则为1 -D：从规则链中删除一条规则，要么输入完整的规则，或者指定规则编号加以删除 -R：替换某条规则，规则替换不会改变顺序，而且必须指定编号。 -P：设置某条规则链的默认动作 -nL：-L、-n，查看当前运行的防火墙规则列表 chain名：指定规则表的哪个链，如INPUT、OUPUT、FORWARD、PREROUTING等 [规则编号]：插入、删除、替换规则时用，–line-numbers显示号码 [-i|o 网卡名称]：i是指定数据包从哪块网卡进入，o是指定数据包从哪块网卡输出 [-p 协议类型]：可以指定规则应用的协议，包含tcp、udp和icmp等 [-s 源IP地址]：源主机的IP地址或子网地址 [–sport 源端口号]：数据包的IP的源端口号 [-d目标IP地址]：目标主机的IP地址或子网地址 [–dport目标端口号]：数据包的IP的目标端口号 -m：extend matches，这个选项用于提供更多的匹配参数，如： -m state –state ESTABLISHED,RELATED -m tcp –dport 22 -m multiport –dports 80,8080 -m icmp –icmp-type 8 &lt;-j 动作&gt;：处理数据包的动作，包括ACCEPT、DROP、REJECT等iptables命令 iptables -nL 查看当前iptables filter规则 iptables -nL 等价于iptables -nL -t filter iptables -nL -t nat查看nat规则iptables -nL –line-number 显示规则链编号iptables -D FORWARD 2删除FORWARD链第二条规则iptables -A INPUT -j REJECT –reject-with icmp-host-prohibited拒绝所有报文，并回应主机已被封锁–reject-with icmp-net-prohibited 拒绝数据包回应一条icmp改造主机被封锁–reject-with tcp-reset 拒绝数据包 tcp回应rst–reject-with icmp-net-prohibited 拒绝数据包回应一条icmp告知网络禁止 实现在外界和本机访问主机的80端口就相当于访问8080端口12iptables -t nat -A PREROUTING -p -m tcp --dport 80 -j REDIRECT --to-ports 8080iptables -t nat -A OUTPUT -p tcp -m --dport 80 -j REDIRECT --to-ports 8080 设置input默认丢包，forward和output默认放通123iptables -P INPUT DROPiptables -P FORWARD ACCEPTiptables -P OUTPUT ACCEPT iptables -m state –state NEW,ESTABLISHED1iptables -I INPUT 3 -p icmp -m state --state NEW -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包，没有安全策略情况下，主机A和主机B可以相互ping通测试结果：执行上述命令后效果为：主机A ping主机B ping不通；主机B ping主机A能通一个包 1iptables -I INPUT 3 -p icmp -m state --state ESTABLISHED -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包测试结果：主机A ping主机B能ping通，主机B ping主机A并不能ping通 1iptables -I INPUT 3 -p icmp -m state --state RELATED -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包测结果：主机A和主机B均不通 1iptables -I INPUT 3 -s 10.30.10.141 -p tcp -m state --state NEW -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包，没有安全策略情况下，主机A和主机B可以相互ssh连接测试结果：主机A无法 telnet 主机B(10.30.10.141) 22端口， 主机B telnet 主机A 可完成三次握手 1iptables -I INPUT 3 -s 10.30.10.141 -p tcp -m state --state ESTABLISHED -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包，没有安全策略情况下，主机A和主机B可以相互ssh连接测试结果：主机A可正常ssh主机B，主机B无法ssh主机A 1iptables -I INPUT 3 -s 10.30.10.141 -p tcp -m state --state NEW,ESTABLISHED -j ACCEPT 背景：策略放在主机A, 默认策略是INPUT丢包，没有安全策略情况下，主机A和主机B可以相互ssh连接测试结果：主机A和主机B可以相互ssh连接 其它说明指定要匹配包的的状态，当前有4种状态可用：INVALID，ESTABLISHED，NEW和RELATED。 INVALID意味着这个包没有已知的流或连接与之关联，也可能是它包含的数据或包头有问题。 ESTABLISHED意思是包是完全有效的，而且属于一个已建立的连接，这个连接的两端都已经有数据发送 RELATED说明包正在建立一个新的连接，这个连接是和一个已建立的连接相关的。比如，FTP data transfer，ICMP error 和一个TCP或UDP连接相关。 NEW状态可以理解成允许首包建立会话，这里首包不适一个包的意思（icmp是第一个包的意思、TCP是三次握手的意思）。 10.30.0.0/16 网断允许访问本机的80和443端口1iptables -A INPUT -s 10.30.0.0/16 -p tcp -m tcp -m multiport --dports 80,443 -j ACCEPT 10.30.10.0／24网段所有报文均不允许访问本机tcp的80端口，直接丢弃数据包1iptables -A INPUT -s 10.30.10.0/24 -p tcp -m tcp --dport 80 -j DROP 完全信任主机10.30.10.301iptables -A INPUT -s 10.30.10.30 -j ACCEPT 放行lo口所有规则并且插入到规则2位置1iptables -I INPUT 2 -i lo -j ACCEPT 当有人连接22端口时候产生日志，并对日志写入进行限制1iptables -R INPUT 1 -p tcp --dport 22 -m limit --limit 3/minute --limit-burst 8 -j LOG iptables -A PREROUTING -i eth0 -d 202.110.123.100 -j DNAT –to 10.30.10.30目的NAT，访问202.110.123.100，最终相当于访问10.30.10.30 iptables -A POSTROUTING -o eth0 -s 1.1.1.1 -j SNAT –to 202.110.123.200源NAT，1.1.1.1内网机器上外网，其报文源地址全部变成 202.110.123.200 iptables -A POSTROUTING -o eth0 -s 192.168.1.200 -j SNAT –to NASQUERADE源NAT，1.1.1.1内网机器上外网，其报文根据接口情况进行变更 ubuntun iptables规则开机启动步骤1:将规则保存到1iptables-save &gt; /etc/iptables.up.rules 步骤2:系统启动时候加载这些配置,vim /etc/network/interfaces;在最后一行加入12345root@controller:~# cat /etc/network/interfaces# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackpre-up iptables-restore &lt; /etc/iptables.up.rules","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"iptables","slug":"iptables","permalink":"http://vcpu.me/tags/iptables/"},{"name":"netfilter框架原理","slug":"netfilter框架原理","permalink":"http://vcpu.me/tags/netfilter框架原理/"}]},{"title":" vxlan","slug":"vxlan1","date":"2017-07-19T10:00:00.000Z","updated":"2017-07-19T05:43:20.000Z","comments":true,"path":"vxlan1/","link":"","permalink":"http://vcpu.me/vxlan1/","excerpt":"云计算网络要求虚拟机迁移（1）二层网络且链路冗余可靠虚拟机迁移从一个物理机迁移到另一个物理机，要求虚拟机不间断业务；因此虚拟机迁移时候必须要保证IP和MAC等网络参数维持不变，放眼此要求也迁移的两台虚拟机处于二层网络时才能满足；而且要求网络本身能多链路冗余和可靠。（2）二层网络下STP（网络生成树）协议复杂，且搞不定大网络规模网络（3）各个厂商虚拟化网络技术，虽然可以简化拓扑，但网络规模和灵活性上欠缺，适合小规模网络构建，例如数据中心内部网络（4）大规模网络扩展TRILL/SPB/FabricPath/VPLS等技术，课解决上述问题，但是对网络硬件有要求，部署成本高 虚拟机规模受网络规格限制大二层网络环境下，所有交换机均需要记录下所有地址和接口映射；一般场景下核心交换机可满足此需求，但是针对于接入交换机却搞不定这件事情；当然你也可以提升所有交换机档次，那么网络建设成本也相应提升。 网络广播隔离（1）主流的vlan技术，会限制vlan数目为4094个（2）vlan技术的静态配置型技术，这样使得数据中心网络几乎为所有vlan被允许通过，导致任何一个vlan 的未知广播数据会在整网泛滥，无节制消耗网络交换能力和带宽。（3）对于小规模的云计算虚拟化环境，现有的网络技术如虚拟机接入感知(VEPA/802.1Qbg)、数据中心二层网络扩展(IRF/vPC/TRILL/FabricPath)、数据中心间二层技术(OTV/EVI/TRILL)等可以很好的满足业务需求，上述限制不成为瓶颈。然而，完全依赖于物理网络设备本身的技术改良，目前看来并不能完全解决大规模云计算环境下的问题。","text":"云计算网络要求虚拟机迁移（1）二层网络且链路冗余可靠虚拟机迁移从一个物理机迁移到另一个物理机，要求虚拟机不间断业务；因此虚拟机迁移时候必须要保证IP和MAC等网络参数维持不变，放眼此要求也迁移的两台虚拟机处于二层网络时才能满足；而且要求网络本身能多链路冗余和可靠。（2）二层网络下STP（网络生成树）协议复杂，且搞不定大网络规模网络（3）各个厂商虚拟化网络技术，虽然可以简化拓扑，但网络规模和灵活性上欠缺，适合小规模网络构建，例如数据中心内部网络（4）大规模网络扩展TRILL/SPB/FabricPath/VPLS等技术，课解决上述问题，但是对网络硬件有要求，部署成本高 虚拟机规模受网络规格限制大二层网络环境下，所有交换机均需要记录下所有地址和接口映射；一般场景下核心交换机可满足此需求，但是针对于接入交换机却搞不定这件事情；当然你也可以提升所有交换机档次，那么网络建设成本也相应提升。 网络广播隔离（1）主流的vlan技术，会限制vlan数目为4094个（2）vlan技术的静态配置型技术，这样使得数据中心网络几乎为所有vlan被允许通过，导致任何一个vlan 的未知广播数据会在整网泛滥，无节制消耗网络交换能力和带宽。（3）对于小规模的云计算虚拟化环境，现有的网络技术如虚拟机接入感知(VEPA/802.1Qbg)、数据中心二层网络扩展(IRF/vPC/TRILL/FabricPath)、数据中心间二层技术(OTV/EVI/TRILL)等可以很好的满足业务需求，上述限制不成为瓶颈。然而，完全依赖于物理网络设备本身的技术改良，目前看来并不能完全解决大规模云计算环境下的问题。 如何满足云计算网络要求？ so you can use vxlan云计算，虚拟化服务器迁移不改变IP地址，也不用修改主机路由等；这种使用场景二层网络可满足，传统网络通常采用VLAN进行通信隔离和广播隔离实现2层网络；Vxlan是实现如何在三层网络中进行二层传输的overlay技术。Vxlan技术是SDN解决方案中最流行的技术。 vxlan &amp; openflow different？overlay只是一组组网方案，openflow是一个控制协议；实际使用网络中是通过openflow协议控制vSwitch构建overlay网络。 why vxlan（overlay）?云计算需要弹性的二层网络，这种弹性overlay网络可满足overlay网络的本质是在三层网络中实现二层网络的扩展。其也就具备了三层网络的优点。三层网络优点如下：（1）三层网络路由方式转发，突破网络结构限制（2）具有良性大规模扩展能力（3）对网络设备本身无要求（4）故障自愈能力强（5）负责均衡能力强 overlay组网，设备互联采用三层，ARP表不需要泛红到全网，tor交换机维护mac地址表页也会很小vxlan可以解决度租户问题overlay组网避免二层带来的广播、组播、单播问题vlan报文头 &amp; vxlan报文头vlan报文结构 vxlan报文结构 vxlan封装端口默认是4798支持多达16M（（2^24-1）/1024^2）租户vxlan数据平面 &amp; 控制平面（1）数据平面-隧道机制已经知道，VTEP为虚拟机的数据包加上了层包头，这些新的报头之有在数据到达目的VTEP后才会被去掉。中间路径的网络设备只会根据外层包头内的目的地址进行数据转发，对于转发路径上的网络来说，一个Vxlan数据包跟一个普通IP包相比，除了个头大一点外没有区别。 由于VXLAN的数据包在整个转发过程中保持了内部数据的完整，因此VXLAN的数据平面是一个基于隧道的数据平面。 (2) 控制平面—-改进的二层协议 VXLAN不会在虚拟机之间维持一个长连接，所以VXLAN需要一个控制平面来记录对端地址可达情况。控制平面的表为(VNI，内层MAC，外层vtep_ip)。Vxlan学习地址的时候仍然保存着二层协议的特征，节点之间不会周期性的交换各自的路由表，对于不认识的MAC地址，VXLAN依靠组播来获取路径信息(如果有SDN Controller，可以向SDN单播获取)。 另一方面，VXLAN还有自学习的功能，当VTEP收到一个UDP数据报后，会检查自己是否收到过这个虚拟机的数据，如果没有，VTEP就会记录源vni/源外层ip/源内层mac对应关系，避免组播学习 vxlan实验同网段ping主机虚拟主机1123456789101112root@controller-VirtualBox:~# cat vxlan1.sh ovs-vsctl add-br br0ovs-vsctl add-br br1ovs-vsctl add-port br0 enp0s8ifconfig enp0s8 0 ifconfig br0 192.168.55.151/24 uproute add default gw 192.168.55.254 br0ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.55.188ifconfig br1 10.0.0.1/24 up 虚拟主机2 1234567891011root@controller-VirtualBox:~# cat vxlan1.sh ovs-vsctl add-br br0ovs-vsctl add-br br1ovs-vsctl add-port br0 enp0s8ifconfig enp0s8 0 ifconfig br0 192.168.55.188/24 uproute add default gw 192.168.55.254 br0ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.55.151ifconfig br1 10.0.0.2/24 up 此实验下vxlan ARP数据包处理过程步骤1：虚拟机1中ping 虚拟机2的10.0.0.2，arp表中无10.0.0.2的MAC地址，虚拟机1广播ARP请求步骤2：br1即VTEP1会将报文封装起来。封装时候目的IP是确定的，源ip会经过路由抉择进行选择（VNI、源地址为VTEP1地址192.168.55.151、目的地址为VTEP2地址192.168.55.188）步骤3：被封装流量经过物理网络到达VTEP2步骤4：VTEP2接受到封装后报文，获取记录（VNI、内层源MAC、内层源IP），解除封装并在VNI中广播步骤5：虚拟机2收到广播流量后，br1回复ARP请求,br1即VETP2会将ARP回复报文进行封装,因为回复数据要发给192.168.55.151，经过路由抉择发送源ip为192.168.55.188步骤6：虚拟机1VTEP1收到报文后，学习收到报文的内层源MAC、内层源IP，解封将流量发给虚拟机1的br1步骤7：br1获取MAC地址，ARP交互结束 vxlan实验不同网段ping主机不同网断vxlan只需要将网关设置为开启vxlan的接口br1虚拟主机112345678910111213root@controller-VirtualBox:~# cat vxlan1.sh ovs-vsctl add-br br0ovs-vsctl add-br br1ovs-vsctl add-port br0 enp0s8ifconfig enp0s8 0 ifconfig br0 192.168.55.151/24 uproute add default gw 192.168.55.254 br0ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.55.188ifconfig br1 10.0.0.1/24 uproute add -net 10.0.1.0 netmask 255.255.255.0 gw 10.0.0.1 dev br1 虚拟主机2123456789101112root@controller-VirtualBox:~# cat vxlan1.sh ovs-vsctl add-br br0ovs-vsctl add-br br1ovs-vsctl add-port br0 enp0s8ifconfig enp0s8 0 ifconfig br0 192.168.55.188/24 uproute add default gw 192.168.55.254 br0ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=192.168.55.151ifconfig br1 10.0.1.1/24 uproute add -net 10.0.0.0 netmask 255.255.255.0 gw 10.0.1.1 dev br1 来自牛人的总结总要有结尾，此总结来自 http://www.jianshu.com/p/d8351b6bf41b 1、Overlay网络的优点简单说就两句话：一是在三层网络中利用封装技术提高二层网络扩展性的同时规避了传统二层网络的种种弊端；二是通过引入VNID的概念，满足了云计算多租户vlan不够，以及租户间网络隔离的问题。 2、Overlay技术的诞生实际就是为了解决云计算大环境下传统网络的种种问题，从它的技术构想到落地方案都是按照云计算的思路来的，所以对于一个规模较小且相对稳定的网络环境中是没有必要用overlay技术的，用了反而把网络搞复杂了。 3、Overlay的所有技术路线中，VXLAN的运用最广泛，得到了最多的主流网络厂商的支持，未来很长一段时间里很可能成为overlay技术的代名词； 4、对于overlay的软硬件模式之争，是仁者见仁、智者见智的，我觉得目前来看软件模式更加容易落地，如果虚拟化平台是开源的需要自己去研发vSwitch是有一些落地难度的，但是vmware NSX相对还是比较成熟的，但是也需要谨慎，最好在一些测试环境或者非重要的环境中先试用。而硬件的overlay方式存在一个所有硬件交换机都需要更换的问题，成本比较高，设备的更换的风险也高，而且硬件overlay方案的捆绑太厉害，需要谨慎考虑。不过我在测试了cisco的ACI后感觉还是很有特点的，后面会单独把ACI拿出来介绍一下，如果是一个完全全新的数据中心倒是可以考虑尝试硬件的overlay模式。 5、在网络的世界里并不是说一个技术叫好就会叫座，比如当年的ATM和Ethernet之争，会有很多客观因素来决定的，比如用户使用习惯，技术落地难度等等，所以对于overlay的技术走向或者SDN的技术走向，还是有很多不确定因素的，各个流派间的博弈也是愈演愈烈，最终鹿死谁手真的很难说，因此这也是对我们用户来说比较头痛的事情，一个正在发展且没有什么标准化的技术，落地的风险比较大，只能抱着边尝试边观望的态度，不轻易落地也不能轻易掉队。","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"overlay","slug":"overlay","permalink":"http://vcpu.me/tags/overlay/"},{"name":"云计算组网","slug":"云计算组网","permalink":"http://vcpu.me/tags/云计算组网/"},{"name":"vxlan原理","slug":"vxlan原理","permalink":"http://vcpu.me/tags/vxlan原理/"},{"name":"vxlan实验","slug":"vxlan实验","permalink":"http://vcpu.me/tags/vxlan实验/"}]},{"title":" 虚拟网络","slug":"network1","date":"2017-07-14T10:00:00.000Z","updated":"2017-07-14T07:31:24.000Z","comments":true,"path":"network1/","link":"","permalink":"http://vcpu.me/network1/","excerpt":"network namespace是什么？当前系统的网络空间，拥有单独的网络协议栈，网络设备、网卡、路由表、转发表、ARP表、ip地址表、iptables、socket等与网络有关的组建均独立，就像进入了另一个网络环境且该网络空间可以实现交换机、路由器、防火墙等功能 使用netns在linux系统上搭建网络网络描述： 该网络由四部分组成：外网、虚拟网络空间net0、虚拟网络空间net1、虚拟网络空间net2 net0和net2分别为网段10.0.1.0/24 10.0.2.0/24的网络空间，并且接入网桥 net1为网络空间，分别以接口eth0和eth1接入虚拟网桥，分别和net0和net2同一个网段 net1为网络空间，以eth2接入网桥，和物理接口enp0s8同一个网段 enp0s8为虚拟机的桥接物理网卡（虚拟机接口混杂模式开启） 192.168.55.165物理机器，接入虚拟机虚拟bridge另外一端均属于ubuntun虚拟机上搭建虚拟网络 淡黄色部分属于虚拟网桥 net0和net2的默认网关是net1 net1的默认网关是办公网络交换机192.168.55.254 经过net1出去流量做snat 搭建网络最终效果： net0和net1和net2 网络空间中均可以上外网","text":"network namespace是什么？当前系统的网络空间，拥有单独的网络协议栈，网络设备、网卡、路由表、转发表、ARP表、ip地址表、iptables、socket等与网络有关的组建均独立，就像进入了另一个网络环境且该网络空间可以实现交换机、路由器、防火墙等功能 使用netns在linux系统上搭建网络网络描述： 该网络由四部分组成：外网、虚拟网络空间net0、虚拟网络空间net1、虚拟网络空间net2 net0和net2分别为网段10.0.1.0/24 10.0.2.0/24的网络空间，并且接入网桥 net1为网络空间，分别以接口eth0和eth1接入虚拟网桥，分别和net0和net2同一个网段 net1为网络空间，以eth2接入网桥，和物理接口enp0s8同一个网段 enp0s8为虚拟机的桥接物理网卡（虚拟机接口混杂模式开启） 192.168.55.165物理机器，接入虚拟机虚拟bridge另外一端均属于ubuntun虚拟机上搭建虚拟网络 淡黄色部分属于虚拟网桥 net0和net2的默认网关是net1 net1的默认网关是办公网络交换机192.168.55.254 经过net1出去流量做snat 搭建网络最终效果： net0和net1和net2 网络空间中均可以上外网 搭建命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081root@controller-VirtualBox:/home/controller# cat br2.sh#new bridge &amp; startip addr flush dev enp0s8brctl addbr br0brctl addif br0 enp0s8ifconfig br0 192.168.55.244/24 upip route add default via 192.168.55.254# ip link add br0 type bridge# ip link set dev br0 up#add net0:eth0 &lt;-&gt; br0:tap0 ip link add net0_eth0 type veth peer name tap0 ip netns add net0 ip link set dev net0_eth0 netns net0 ip netns exec net0 ip link set dev net0_eth0 name eth0 ip netns exec net0 ip addr add 10.0.1.1/24 dev eth0 ip netns exec net0 ip link set dev eth0 up ip link set dev tap0 master br0 ip link set dev tap0 up#add net1:eth0 &lt;-&gt; br0:tap1 ip link add net1_eth0 type veth peer name tap1 ip netns add net1 ip link set dev net1_eth0 netns net1 ip netns exec net1 ip link set dev net1_eth0 name eth0 ip netns exec net1 ip addr add 10.0.1.2/24 dev eth0 ip netns exec net1 ip link set dev eth0 up ip link set dev tap1 master br0 ip link set dev tap1 up#add net2:eth0 &lt;-&gt; br0:tap2 ip link add net2_eth0 type veth peer name tap2 ip netns add net2 ip link set dev net2_eth0 netns net2 ip netns exec net2 ip link set dev net2_eth0 name eth0 ip netns exec net2 ip addr add 10.0.2.1/24 dev eth0 ip netns exec net2 ip link set dev eth0 up ip link set dev tap2 master br0 ip link set dev tap2 up#connect net2:eth1 &lt;-&gt; br0:tap3 ip link add net2_eth1 type veth peer name tap3 ip link set dev net2_eth1 netns net1 ip netns exec net1 ip link set dev net2_eth1 name eth1 ip netns exec net1 ip addr add 10.0.2.2/24 dev eth1 ip netns exec net1 ip link set dev eth1 up ip link set dev tap3 master br0 ip link set dev tap3 up#add route#ip netns exec net2 ip route add 10.0.1.0/24 via 10.0.2.2 dev eth0#ip netns exec net0 ip route add 10.0.2.0/24 via 10.0.1.2 dev eth0ip netns exec net2 route add default gw 10.0.2.2ip netns exec net0 route add default gw 10.0.1.2#open gateway forwardip netns exec net1 sysctl net.ipv4.ip_forward=1#connect net1:eth2 &lt;-&gt; br0:tap4 ip link add net1_eth2 type veth peer name tap4 ip link set dev net1_eth2 netns net1 ip netns exec net1 ip link set dev net1_eth2 name eth2 ip netns exec net1 ip addr add 192.168.55.233/24 dev eth2 ip netns exec net1 ip link set dev eth2 up ip link set dev tap4 master br0 ip link set dev tap4 up#add net1 gateway 192.168.55.254ip netns exec net1 route add default gw 192.168.55.254#add snatip netns exec net1 iptables -t nat -A POSTROUTING -s 10.0.1.0/24 -o eth2 -j MASQUERADEip netns exec net1 iptables -t nat -A POSTROUTING -s 10.0.2.0/24 -o eth2 -j MASQUERADEip netns exec net0 ping -c 3 8.8.8.8ip netns exec net2 ping -c 3 8.8.8.8 Linux虚拟网络设备tun/tap网络设备概念？网络设备工作在驱动和协议栈之间，负责衔接它们之间的交互。它帮助驱动和协议栈只关注本身事情。 虚拟网络设备和物理物理设备区别 ？物理网卡其实就是物理设备，比如物理网卡eth0，它分别连接内核协议栈河外面的物理网络，从物理网络收到的数据包会通过接口转发给内核协议栈，从内核协议栈发出包也会通过物理设备转发最终通过物理网络发出去 虚拟设备和物理设备对于内核网络设备管理模块来讲地位一致且无区别。只不过物理设备往往把数据包送到外网，虚拟设备要看具体实现了。 tun/tap ？tun/tap连接的应用程序，可以理解其为运行的另一台服务器，这台服务器可用于加密、隧道等数据加工；处理完成后从新借用一个地址将处理完后数据包封装，发出。 1234567891011121314151617181920212223242526272829+----------------------------------------------------------------+| || +--------------------+ +--------------------+ || | User Application A | | User Application B |&lt;-----+ || +--------------------+ +--------------------+ | || | 1 | 5 | ||...............|......................|...................|.....|| ↓ ↓ | || +----------+ +----------+ | || | socket A | | socket B | | || +----------+ +----------+ | || | 2 | 6 | ||.................|.................|......................|.....|| ↓ ↓ | || +------------------------+ 4 | || | Newwork Protocol Stack | | || +------------------------+ | || | 7 | 3 | ||................|...................|.....................|.....|| ↓ ↓ | || +----------------+ +----------------+ | || | eth0 | | tun0 | | || +----------------+ +----------------+ | ||192.168.55.188 | | 10.0.1.1 | || | 8 +---------------------+ || | |+----------------|-----------------------------------------------+ ↓ Physical Network 上述图表述的应用场景是VPN场景：发到10.0.1.0/24 网络数据通过应用程序B这个隧道，利用192.168.55.188发出到远端。 tun/tap场景下数据包流程1.应用程序A是一个普通的程序，通过socket A发送了一个数据包，假设这个数据包的目的IP地址是10.0.1.22.socket将这个数据包丢给协议栈3.协议栈根据数据包的目的IP地址，匹配本地路由规则，知道这个数据包应该由tun0出去，于是将数据包交给tun04.tun0收到数据包之后，发现另一端被进程B打开了，于是将数据包丢给了进程B5.进程B收到数据包之后，做一些跟业务相关的处理，然后构造一个新的数据包，将原来的数据包嵌入在新的数据包中，最后通过socket B将数据包转发出去，这时候新数据包的源地址变成了eth0的地址，而目的IP地址变成了一个其它的地址，比如是192.168.55.2546.socket B将数据包丢给协议栈7.协议栈根据本地路由，发现这个数据包应该要通过eth0发送出去，于是将数据包交给eth08.eth0通过物理网络将数据包发送出去 192.168.55.254收到数据包后，打开数据包，取出原始数据，转发给10.0.1.2，收到10.0.1.2应答后，从新构造应答包并讲原始报文封装里面；走路由返回给程序B，应用程序B解封装，最终将数据包回复给应用程序A 至此一个完整的隧道交互完成了，tun/tap承担了奖协议栈数据包从新交付到应用程序作用，使得数据包有机会在用户态进行封装。 tun另一端是一个用户程序tun0是一个Tun/Tap虚拟设备，从上图中可以看出它和物理设备eth0的差别，它们的一端虽然都连着协议栈，但另一端不一样，eth0的另一端是物理网络，这个物理网络可能就是一个交换机，而tun0的另一端是一个用户层的程序，协议栈发给tun0的数据包能被这个应用程序读取到，并且应用程序能直接向tun0写数据。 tun和tap区别用户层程序通过tun设备只能读写IP数据包，而通过tap设备能读写链路层数据包，类似于普通socket和raw socket的差别一样，处理数据包的格式不一样。 实例解析tun/tap可用于linux用户态从内核查路由重新收到数据包，处理完成后再发出12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;net/if.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;linux/if_tun.h&gt;#include&lt;stdlib.h&gt;#include&lt;stdio.h&gt;#include &lt;unistd.h&gt;int tun_alloc(int flags)&#123; struct ifreq ifr; int fd, err; char *clonedev = \"/dev/net/tun\"; if ((fd = open(clonedev, O_RDWR)) &lt; 0) &#123; return fd; &#125; memset(&amp;ifr, 0, sizeof(ifr)); ifr.ifr_flags = flags; if ((err = ioctl(fd, TUNSETIFF, (void *) &amp;ifr)) &lt; 0) &#123; close(fd); return err; &#125; printf(\"Open tun/tap device: %s for reading...\\n\", ifr.ifr_name); return fd;&#125;int main()&#123; int tun_fd, nread; char buffer[1500]; /* Flags: IFF_TUN - TUN device (no Ethernet headers) * IFF_TAP - TAP device * IFF_NO_PI - Do not provide packet information */ tun_fd = tun_alloc(IFF_TUN | IFF_NO_PI); if (tun_fd &lt; 0) &#123; perror(\"Allocating interface\"); exit(1); &#125; while (1) &#123; nread = read(tun_fd, buffer, sizeof(buffer)); if (nread &lt; 0) &#123; perror(\"Reading from interface\"); close(tun_fd); exit(1); &#125; printf(\"Read %d bytes from tun/tap device\\n\", nread); &#125; return 0;&#125; 执行步骤如下：（1）linux运行窗口1编译运行tun12root@controller-VirtualBox:/home/controller# gcc tun.c -o tunroot@controller-VirtualBox:/home/controller# ./tun （2）linux运行窗口2 查看建立的网络设备tun0123456789101112131415root@controller-VirtualBox:/home/controller# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:5f:1b:a9 brd ff:ff:ff:ff:ff:ff inet 192.168.55.188/24 brd 192.168.55.255 scope global dynamic enp0s8 valid_lft 9908sec preferred_lft 9908sec inet6 fe80::174:582b:9b7c:3df4/64 scope link valid_lft forever preferred_lft forever4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 500 link/none （3）linux运行窗口2给tun0配置地址并激活1234root@controller-VirtualBox:/home/controller# ip addr add 10.0.1.1/24 dev tun0root@controller-VirtualBox:/home/controller#root@controller-VirtualBox:/home/controller#root@controller-VirtualBox:/home/controller# ip link set tun0 up （4）linux运行窗口2 ping 10.0.1.2 （5）linux运行窗口1 查看出现下面效果，用户态socket已经通过tun0接收到ping包了1234567891011root@controller-VirtualBox:/home/controller# ./tunOpen tun/tap device: tun0 for reading...Read 48 bytes from tun/tap deviceRead 48 bytes from tun/tap deviceRead 48 bytes from tun/tap deviceRead 84 bytes from tun/tap deviceRead 84 bytes from tun/tap deviceRead 84 bytes from tun/tap deviceRead 84 bytes from tun/tap deviceRead 84 bytes from tun/tap deviceRead 84 bytes from tun/tap device","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"network namespace","slug":"network-namespace","permalink":"http://vcpu.me/tags/network-namespace/"},{"name":"网络设备tun/tap","slug":"网络设备tun-tap","permalink":"http://vcpu.me/tags/网络设备tun-tap/"}]},{"title":" 网络基本命令","slug":"cmd1","date":"2017-07-13T10:00:00.000Z","updated":"2017-07-13T10:59:59.000Z","comments":true,"path":"cmd1/","link":"","permalink":"http://vcpu.me/cmd1/","excerpt":"ARP相关命令ip n显示ARP表全部清除linux arp表项1arp -n|awk &apos;/^[1-9]/&#123;system(&quot;arp -d &quot;$1)&#125;&apos; 清除接口eth0所有的mac1ip neigh flush dev eth0 arp -v 额外显示接口类型和arp表项统计信息1234root@controller-VirtualBox:/home/controller# arp -vAddress HWtype HWaddress Flags Mask Iface10.0.1.2 ether 22:dc:c1:9c:8c:b4 C eth0Entries: 1 Skipped: 0 Found: 1 arp -a 显示全部ARP缓存12root@controller-VirtualBox:/home/controller# arp -a? (10.0.1.2) at 22:dc:c1:9c:8c:b4 [ether] on eth0 arp -n 可快速查询ARP缓存1234root@controller-VirtualBox:/home/controller# arp -nAddress HWtype HWaddress Flags Mask Iface10.0.1.2 ether 22:dc:c1:9c:8c:b4 C eth0root@controller-VirtualBox:/home/controller#","text":"ARP相关命令ip n显示ARP表全部清除linux arp表项1arp -n|awk &apos;/^[1-9]/&#123;system(&quot;arp -d &quot;$1)&#125;&apos; 清除接口eth0所有的mac1ip neigh flush dev eth0 arp -v 额外显示接口类型和arp表项统计信息1234root@controller-VirtualBox:/home/controller# arp -vAddress HWtype HWaddress Flags Mask Iface10.0.1.2 ether 22:dc:c1:9c:8c:b4 C eth0Entries: 1 Skipped: 0 Found: 1 arp -a 显示全部ARP缓存12root@controller-VirtualBox:/home/controller# arp -a? (10.0.1.2) at 22:dc:c1:9c:8c:b4 [ether] on eth0 arp -n 可快速查询ARP缓存1234root@controller-VirtualBox:/home/controller# arp -nAddress HWtype HWaddress Flags Mask Iface10.0.1.2 ether 22:dc:c1:9c:8c:b4 C eth0root@controller-VirtualBox:/home/controller# arptables增加一条规则，限制10.0.10.220 ARP packet进入到环境中1arptables -A INPUT -s 10.0.10.220 -j DROP 删除arotables规则1arptables -D INPUT -s 10.0.10.220 -j DROP 查看arptables 规则1arptables -L arpwatcharpwatch命令用来监听网络上arp的记录。-d：启动排错模式；-f&lt;记录文件&gt;：设置存储ARP记录的文件，预设为/var/lib／arpwatch/arp.dat；-i&lt;接口&gt;：指定监听ARP的接口，预设的接口为eth0；-r&lt;记录文件&gt;：从指定的文件中读取ARP记录，而不是从网络上监听。 读出监听的ARP地址对应表 必须要杀死arpwatch进程后才才能读出下面数据123456789101112131415161718192021root@controller-VirtualBox:/home/controller# cat /var/lib/arpwatch/arp.dat3c:97:0e:3c:8c:53 192.168.55.155 1499942089 br0e0:69:95:4c:d4:ac 192.168.55.169 1499942233 br028:d2:44:86:46:be 192.168.55.193 1499942137 br068:f7:28:89:49:bd 192.168.55.199 1499942147 br03c:97:0e:3d:22:0c 192.168.55.137 1499942189 br068:f7:28:63:19:4f 192.168.55.192 1499942097 br010:c3:7b:6e:e6:95 192.168.55.174 1499942112 br020:dc:e6:fe:d7:fe 192.168.55.253 1499942035 br0b0:83:fe:6c:ec:a9 192.168.55.135 1499942042 br008:00:27:5f:1b:a9 192.168.55.244 1499942227 br000:15:c6:26:4d:48 192.168.55.254 1499942227 br090:e6:ba:ea:81:93 192.168.55.98 1499942212 br08c:a6:df:98:94:50 192.168.55.197 1499942083 br074:27:ea:54:8b:61 192.168.55.198 1499942096 br03c:97:0e:a7:a8:4f 192.168.55.116 1499942182 br050:7b:9d:f1:6e:47 192.168.55.158 1499942145 br03c:97:0e:3c:8d:fd 192.168.55.143 1499942234 br028:d2:44:bd:02:89 192.168.55.113 1499942210 br0d4:61:fe:2b:73:13 192.168.55.220 1499942214 br068:f7:28:06:1a:02 192.168.55.132 1499942226 br0 路由相关命令route -n12345root@controller-VirtualBox:/home/controller# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.1.2 0.0.0.0 UG 0 0 0 eth010.0.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 ip r 查看路由1234root@controller-VirtualBox:/home/controller# ip rdefault via 192.168.55.254 dev br0169.254.0.0/16 dev br0 scope link metric 1000192.168.55.0/24 dev br0 proto kernel scope link src 192.168.55.244 ip r get xx.xx.xx.xx123root@controller-VirtualBox:/home/controller# ip r get 8.8.8.88.8.8.8 via 10.0.1.2 dev eth0 src 10.0.1.1 cache 增加默认路由route add default gw 10.0.1.2ip route add default via 10.0.1.2 ip -s link 查看网络统计数据12345678910111213root@controller-VirtualBox:/home/controller# ip -s link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 RX: bytes packets errors dropped overrun mcast 0 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 0 0 0 0 0 05: eth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 8e:90:7e:35:80:8e brd ff:ff:ff:ff:ff:ff link-netnsid 0 RX: bytes packets errors dropped overrun mcast 4621478 12933 0 17 0 0 TX: bytes packets errors dropped carrier collsns 2720 32 0 0 0 0 ip -s -s link ls eth0 显示具体接口的详细信息12345678910115: eth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 8e:90:7e:35:80:8e brd ff:ff:ff:ff:ff:ff link-netnsid 0 RX: bytes packets errors dropped overrun mcast 5197968 14512 0 20 0 0 RX errors: length crc frame fifo missed 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 9034 97 0 0 0 0 TX errors: aborted fifo window heartbeat transns 0 0 0 0 2root@controller-VirtualBox:/home/controller# 地址配置相关命令配置地址ip addr add 10.0.1.1/24 dev eth0 ip addr 显示所有接口信息ip addr show eth0 显示eth0接口信息1234567root@controller-VirtualBox:/home/controller# ip addr show eth05: eth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 8e:90:7e:35:80:8e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.1.1/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::8c90:7eff:fe35:808e/64 scope link valid_lft forever preferred_lft forever ip addr del x.x.x.x/mask dev eth0 删除接口eth0地址12345678root@controller-VirtualBox:/home/controller# ip addr del 10.0.1.1/24 dev eth0root@controller-VirtualBox:/home/controller# ip addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:005: eth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 8e:90:7e:35:80:8e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::8c90:7eff:fe35:808e/64 scope link valid_lft forever preferred_lft forever 关闭和开启接口ip link set eth0 up/down linux网桥相关brctl show 查看网桥配置12345678910root@controller-VirtualBox:/home/controller# brctl showbridge name bridge id STP enabled interfacesbr0 8000.0800275f1ba9 no enp0s8 tap0 tap1 tap2 tap3 tap4### brctl showmacs br0 查看网桥学习的mac表项 brctl showmacs br0查看br0的mac表项12345678root@controller-VirtualBox:/home/controller# brctl showmacs br0port no mac addr is local? ageing timer 1 00:0c:29:1f:73:38 no 202.66 1 00:0c:29:f1:60:5d no 248.12 1 00:0e:c6:cf:da:c7 no 0.00 1 00:15:c6:26:4d:22 no 1.22 1 00:15:c6:26:4d:48 no 0.73 1 00:50:56:9f:cc:e2 no 161.74 brctl showstp bbr0查看接口信息123456789101112root@controller-VirtualBox:/home/controller# brctl showstp br0br0 bridge id 8000.0800275f1ba9 designated root 8000.0800275f1ba9 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 300.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 14.22 flags 虚拟命名空间相关命令增加虚拟网络命名空间ip netns add net0 显示所有的虚拟网络命名空间EULER:~ # ip netns listnet0也可通过查看/var/run/netns目录下的文件来listEULER:~ # ls /var/run/netns/net0 进入虚拟机网络环境ip netns exec net0 command 如EULER:~ # ip netns exec net0 bash #打开虚拟网络环境net0的bash窗口EULER:~ # ip addr #显示所有虚拟网络环境的设备EULER:~ # exit #退出该网络虚拟环境exit 增加一对veth虚拟网卡EULER:~ # ip link add type veth 将veth0添加到net0虚拟网络环境ip link set veth0 netns net0 将虚拟网卡veth1改名并添加到net1虚拟网络环境中ip link set dev veth1 name net1-bridge netns net1 设置虚拟网络环境net0的veth0设备处于激活状态ip netns exec net0 ip link set veth0 up 为虚拟网络环境net0的veth0设备增加IP地址ip netns exec net0 ip address add 10.0.1.1/24 dev veth0 ip netns 列出当前网络虚拟的namespace1234root@controller-VirtualBox:/home/controller# ip netnsnet2 (id: 2)net1 (id: 1)net0 (id: 0) ls /var/run/netns/ 列出当前网络虚拟的namespaceroot@controller-VirtualBox:/home/controller# ls /var/run/netns/net0 net1 net2 ip netns monitor监控虚拟网络namespace行为root@controller-VirtualBox:/home/controller# ip netns monitordelete ddd","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"arp相关命令","slug":"arp相关命令","permalink":"http://vcpu.me/tags/arp相关命令/"},{"name":"路由相关命令","slug":"路由相关命令","permalink":"http://vcpu.me/tags/路由相关命令/"},{"name":"地址配置相关命令","slug":"地址配置相关命令","permalink":"http://vcpu.me/tags/地址配置相关命令/"},{"name":"虚拟命名空间相关命令","slug":"虚拟命名空间相关命令","permalink":"http://vcpu.me/tags/虚拟命名空间相关命令/"}]},{"title":"二三层转发-基本原理","slug":"switchtech","date":"2017-07-07T10:00:00.000Z","updated":"2017-07-07T03:47:23.000Z","comments":true,"path":"switchtech/","link":"","permalink":"http://vcpu.me/switchtech/","excerpt":"交换机的工作原理1. 交换机根据收到数据帧中的源MAC地址建立该地址同交换机端口的映射，并将其写入MAC地址表中。 2. 交换机将数据帧中的目的MAC地址同已建立的MAC地址表进行比较，以决定由哪个端口进行转发。 3. 如数据帧中的目的MAC地址不在MAC地址表中，则向所有端口转发。这一过程称为泛洪（flood）。 4. 广播帧和组播帧向所有的端口转发。交换机的工作原理（二、三、四层交换原理） 交换机的三个主要功能 学习：以太网交换机了解每一端口相连设备的MAC地址，并将地址同相应的端口映射起来存放在交换机缓存中的MAC地址表中。 转发/过滤：当一个数据帧的目的地址在MAC地址表中有映射时，它被转发到连接目的节点的端口而不是所有端口（如该数据帧为广播/组播帧则转发至所有端口）。 消除回路：当交换机包括一个冗余回路时，以太网交换机通过生成树协议避免回路的产生，同时允许存在后备路径。 交换机的工作特性 1. 交换机的每一个端口所连接的网段都是一个独立的冲突域。 2. 交换机所连接的设备仍然在同一个广播域内，也就是说，交换机不隔绝广播（惟一的例外是在配有VLAN的环境中）。 3. 交换机依据帧头的信息进行转发，因此说交换机是工作在数据链路层的网络设备（此处所述交换机仅指传统的二层交换设备）。 交换机的分类 依照交换机处理帧时不同的操作模式，主要可分为两类： 存储转发：交换机在转发之前必须接收整个帧，并进行错误校检，如无错误再将这一帧发往目的地址。帧通过交换机的转发时延随帧长度的不同而变化。 直通式：交换机只要检查到帧头中所包含的目的地址就立即转发该帧，而无需等待帧全部的被接收，也不进行错误校验。由于以太网帧头的长度总是固定的，因此帧通过交换机的转发时延也保持不变。","text":"交换机的工作原理1. 交换机根据收到数据帧中的源MAC地址建立该地址同交换机端口的映射，并将其写入MAC地址表中。 2. 交换机将数据帧中的目的MAC地址同已建立的MAC地址表进行比较，以决定由哪个端口进行转发。 3. 如数据帧中的目的MAC地址不在MAC地址表中，则向所有端口转发。这一过程称为泛洪（flood）。 4. 广播帧和组播帧向所有的端口转发。交换机的工作原理（二、三、四层交换原理） 交换机的三个主要功能 学习：以太网交换机了解每一端口相连设备的MAC地址，并将地址同相应的端口映射起来存放在交换机缓存中的MAC地址表中。 转发/过滤：当一个数据帧的目的地址在MAC地址表中有映射时，它被转发到连接目的节点的端口而不是所有端口（如该数据帧为广播/组播帧则转发至所有端口）。 消除回路：当交换机包括一个冗余回路时，以太网交换机通过生成树协议避免回路的产生，同时允许存在后备路径。 交换机的工作特性 1. 交换机的每一个端口所连接的网段都是一个独立的冲突域。 2. 交换机所连接的设备仍然在同一个广播域内，也就是说，交换机不隔绝广播（惟一的例外是在配有VLAN的环境中）。 3. 交换机依据帧头的信息进行转发，因此说交换机是工作在数据链路层的网络设备（此处所述交换机仅指传统的二层交换设备）。 交换机的分类 依照交换机处理帧时不同的操作模式，主要可分为两类： 存储转发：交换机在转发之前必须接收整个帧，并进行错误校检，如无错误再将这一帧发往目的地址。帧通过交换机的转发时延随帧长度的不同而变化。 直通式：交换机只要检查到帧头中所包含的目的地址就立即转发该帧，而无需等待帧全部的被接收，也不进行错误校验。由于以太网帧头的长度总是固定的，因此帧通过交换机的转发时延也保持不变。 二三四层交换机 多种理解的说法： 理解1 二层交换（也称为桥接）是基于硬件的桥接。基于每个末端站点的唯一MAC地址转发数据包。二层交换的高性能可以产生增加各子网主机数量的网络设计。其仍然有桥接所具有的特性和限制。 三层交换是基于硬件的路由选择。路由器和第三层交换机对数据包交换操作的主要区别在于物理上的实施。 四层交换的简单定义是：不仅基于MAC（第二层桥接）或源/目的地IP地址（第三层路由选择），同时也基于TCP/UDP应用端口来做出转发决定的能力。其使网络在决定路由时能够区分应用。能够基于具体应用对数据流进行优先级划分。它为基于策略的服务质量技术提供了更加细化的解决方案。提供了一种可以区分应用类型的方法。 理解2 二层交换机 基于MAC地址 三层交换机 具有VLAN功能 有交换和路由 ///基于IP，就是网络 四层交换机 基于端口，就是应用 理解3 二层交换技术从网桥发展到VLAN（虚拟局域网），在局域网建设和改造中得到了广泛的应用。第二层交换技术是工作在OSI七层网络模型中的第二层，即数据链路层。它按照所接收到数据包的目的MAC地址来进行转发，对于网络层或者高层协议来说是透明的。它不处理网络层的IP地址，不处理高层协议的诸如TCP、UDP的端口地址，它只需要数据包的物理地址即MAC地址，数据交换是靠硬件来实现的，其速度相当快，这是二层交换的一个显著的优点。但是，它不能处理不同IP子网之间的数据交换。传统的路由器可以处理大量的跨越IP子网的数据包，但是它的转发效率比二层低，因此要想利用二层转发效率高这一优点，又要处理三层IP数据包，三层交换技术就诞生了。 三层交换技术的工作原理 第三层交换工作在OSI七层网络模型中的第三层即网络层，是利用第三层协议中的IP包的包头信息来对后续数据业务流进行标记，具有同一标记的业务流的后续报文被交换到第二层数据链路层，从而打通源IP地址和目的IP地址之间的一条通路。这条通路经过第二层链路层。有了这条通路，三层交换机就没有必要每次将接收到的数据包进行拆包来判断路由，而是直接将数据包进行转发，将数据流进行交换 理解4 二层交换技术 二层交换技术是发展比较成熟，二层交换机属数据链路层设备，可以识别数据包中的MAC地址信息，根据MAC地址进行转发，并将这些MAC地址与对应的端口记录在自己内部的一个地址表中。具体的工作流程如下： （1）当交换机从某个端口收到一个数据包，它先读取包头中的源MAC地址，这样它就知道源MAC地址的机器是连在哪个端口上的； （2） 再去读取包头中的目的MAC地址，并在地址表中查找相应的端口； （3） 如表中有与这目的MAC地址对应的端口，把数据包直接复制到这端口上； （4）如表中找不到相应的端口则把数据包广播到所有端口上，当目的机器对源机器回应时，交换机又可以学习一目的MAC地址与哪个端口对应，在下次传送数据时就不再需要对所有端口进行广播了。 不断的循环这个过程，对于全网的MAC地址信息都可以学习到，二层交换机就是这样建立和维护它自己的地址表。 从二层交换机的工作原理可以推知以下三点： （1）由于交换机对多数端口的数据进行同时交换，这就要求具有很宽的交换总线带宽，如果二层交换机有N个端口，每个端口的带宽是M，交换机总线带宽超过N×M，那么这交换机就可以实现线速交换； （2） 学习端口连接的机器的MAC地址，写入地址表，地址表的大小（一般两种表示方式：一为BEFFER RAM，一为MAC表项数值），地址表大小影响交换机的接入容量； （3） 还有一个就是二层交换机一般都含有专门用于处理数据包转发的ASIC （Application specific Integrated Circuit）芯片，因此转发速度可以做到非常快。由于各个厂家采用ASIC不同，直接影响产品性能。 以上三点也是评判二三层交换机性能优劣的主要技术参数，这一点请大家在考虑设备选型时注意比较。 路由技术 路由器工作在OSI模型的第三层—网络层操作，其工作模式与二层交换相似，但路由器工作在第三层，这个区别决定了路由和交换在传递包时使用不同的控制信息，实现功能的方式就不同。工作原理是在路由器的内部也有一个表，这个表所标示的是如果要去某一个地方，下一步应该向那里走，如果能从路由表中找到数据包下一步往那里走，把链路层信息加上转发出去；如果不能知道下一步走向那里，则将此包丢弃，然后返回一个信息交给源地址。 路由技术实质上来说不过两种功能：决定最优路由和转发数据包。路由表中写入各种信息，由路由算法计算出到达目的地址的最佳路径，然后由相对简单直接的转发机制发送数据包。接受数据的下一台路由器依照相同的工作方式继续转发，依次类推，直到数据包到达目的路由器。 而路由表的维护，也有两种不同的方式。一种是路由信息的更新，将部分或者全部的路由信息公布出去，路由器通过互相学习路由信息，就掌握了全网的拓扑结构，这一类的路由协议称为距离矢量路由协议；另一种是路由器将自己的链路状态信息进行广播，通过互相学习掌握全网的路由信息，进而计算出最佳的转发路径，这类路由协议称为链路状态路由协议。 由于路由器需要做大量的路径计算工作，一般处理器的工作能力直接决定其性能的优劣。当然这一判断还是对中低端路由器而言，因为高端路由器往往采用分布式处理系统体系设计。 三层交换技术 近年来的对三层技术的宣传，耳朵都能起茧子，到处都在喊三层技术，有人说这是个非常新的技术，也有人说，三层交换嘛，不就是路由器和二层交换机的堆叠，也没有什么新的玩意，事实果真如此吗？下面先来通过一个简单的网络来看看三层交换机的工作过程。 组网比较简单 使用IP的设备A————三层交换机———–使用IP的设备B 比如A要给B发送数据，已知目的IP，那么A就用子网掩码取得网络地址，判断目的IP是否与自己在同一网段。 如果在同一网段，但不知道转发数据所需的MAC地址，A就发送一个ARP请求，B返回其MAC地址，A用此MAC封装数据包并发送给交换机，交换机起用二层交换模块，查找MAC地址表，将数据包转发到相应的端口。 如果目的IP地址显示不是同一网段的，那么A要实现和B的通讯，在流缓存条目中没有对应MAC地址条目，就将第一个正常数据包发送向一个缺省网关，这个缺省网关一般在操作系统中已经设好，对应第三层路由模块，所以可见对于不是同一子网的数据，最先在MAC表中放的是缺省网关的MAC地址；然后就由三层模块接收到此数据包，查询路由表以确定到达B的路由，将构造一个新的帧头，其中以缺省网关的MAC地址为源MAC地址，以主机B的MAC地址为目的MAC地址。通过一定的识别触发机制，确立主机A与B的MAC地址及转发端口的对应关系，并记录进流缓存条目表，以后的A到B的数据，就直接交由二层交换模块完成。这就通常所说的一次路由多次转发。 以上就是三层交换机工作过程的简单概括，可以看出三层交换的特点： 由硬件结合实现数据的高速转发。 这就不是简单的二层交换机和路由器的叠加，三层路由模块直接叠加在二层交换的高速背板总线上，突破了传统路由器的接口速率限制，速率可达几十Gbit/s。算上背板带宽，这些是三层交换机性能的两个重要参数。 简洁的路由软件使路由过程简化。 大部分的数据转发，除了必要的路由选择交由路由软件处理，都是又二层模块高速转发，路由软件大多都是经过处理的高效优化软件，并不是简单照搬路由器中的软件。 结论： 二层交换机用于小型的局域网络。这个就不用多言了，在小型局域网中，广播包影响不大，二层交换机的快速交换功能、多个接入端口和低谦价格为小型网络用户提供了很完善的解决方案。 路由器的优点在于接口类型丰富，支持的三层功能强大，路由能力强大，适合用于大型的网络间的路由，它的优势在于选择最佳路由，负荷分担，链路备份及和其他网络进行路由信息的交换等等路由器所具有功能。 三层交换机的最重要的功能是加快大型局域网络内部的数据的快速转发，加入路由功能也是为这个目的服务的。如果把大型网络按照部门，地域等等因素划分成一个个小局域网，这将导致大量的网际互访，单纯的使用二层交换机不能实现网际互访；如单纯的使用路由器，由于接口数量有限和路由转发速度慢，将限制网络的速度和网络规模，采用具有路由功能的快速转发的三层交换机就成为首选。 一般来说，在内网数据流量大，要求快速转发响应的网络中，如全部由三层交换机来做这个工作，会造成三层交换机负担过重，响应速度受影响，将网间的路由交由路由器去完成，充分发挥不同设备的优点，不失为一种好的组网策略，当然，前提是客户的腰包很鼓，不然就退而求其次，让三层交换机也兼为网际互连。 第四层交换技术 第四层交换的一个简单定义是：它是一种功能，它决定传输不仅仅依据MAC地址(第二层网桥)或源/目标IP地址(第三层路由),而且依据TCP/UDP(第四层) 应用端口号。第四层交换功能就象是虚IP，指向物理服务器。它传输的业务服从的协议多种多样，有HTTP、FTP、NFS、Telnet或其他协议。这些业务在物理服务器基础上，需要复杂的载量平衡算法。在IP世界，业务类型由终端TCP或UDP端口地址来决定，在第四层交换中的应用区间则由源端和终端IP地址、TCP和UDP端口共同决定。 在第四层交换中为每个供搜寻使用的服务器组设立虚IP地址（VIP），每组服务器支持某种应用。在域名服务器（DNS）中存储的每个应用服务器地址是VIP，而不是真实的服务器地址。 当某用户申请应用时，一个带有目标服务器组的VIP连接请求（例如一个TCP SYN包）发给服务器交换机。服务器交换机在组中选取最好的服务器，将终端地址中的VIP用实际服务器的IP取代，并将连接请求传给服务器。这样，同一区间所有的包由服务器交换机进行映射，在用户和同一服务器间进行传输。 第四层交换的原理 OSI模型的第四层是传输层。传输层负责端对端通信，即在网络源和目标系统之间协调通信。在IP协议栈中这是TCP（一种传输协议）和UDP（用户数据包协议）所在的协议层。 在第四层中，TCP和UDP标题包含端口号（portnumber），它们可以唯一区分每个数据包包含哪些应用协议（例如HTTP、FTP等）。端点系统利用这种信息来区分包中的数据，尤其是端口号使一个接收端计算机系统能够确定它所收到的IP包类型，并把它交给合适的高层软件。端口号和设备IP地址的组合通常称作“插口（socket）”。 1和255之间的端口号被保留，他们称为“熟知”端口，也就是说，在所有主机TCP/IP协议栈实现中，这些端口号是相同的。除了“熟知”端口外，标准UNIX服务分配在256到1024端口范围，定制的应用一般在1024以上分配端口号. 分配端口号的最近清单可以在RFc1700”Assigned Numbers”上找到。TCP／UDP端口号提供的附加信息可以为网络交换机所利用，这是第4层交换的基础。 熟知的端口号举例: 应用协议 端口号 FTP 20（数据），21（控制） TELNET 23 SMTP 25 HTTP 80 NNTP 119 NNMP 16，162（SNMP traps） TCP/UDP端口号提供的附加信息可以为网络交换机所利用，这是第四层交换的基础。 具有第四层功能的交换机能够起到与服务器相连接的“虚拟IP”(VIP)前端的作用。 每台服务器和支持单一或通用应用的服务器组都配置一个VIP地址。这个VIP地址被发送出去并在域名系统上注册。 在发出一个服务请求时，第四层交换机通过判定TCP开始，来识别一次会话的开始。然后它利用复杂的算法来确定处理这个请求的最佳服务器。一旦做出这种决定，交换机就将会话与一个具体的IP地址联系在一起，并用该服务器真正的IP地址来代替服务器上的VIP地址。 每台第四层交换机都保存一个与被选择的服务器相配的源IP地址以及源TCP 端口相关联的连接表。然后第四层交换机向这台服务器转发连接请求。所有后续包在客户机与服务器之间重新影射和转发，直到交换机发现会话为止。 在使用第四层交换的情况下，接入可以与真正的服务器连接在一起来满足用户制定的规则，诸如使每台服务器上有相等数量的接入或根据不同服务器的容量来分配传输流。 本文章来网络","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"二三层转发","slug":"二三层转发","permalink":"http://vcpu.me/tags/二三层转发/"}]},{"title":"集线器、网桥、交换机、中继器原理","slug":"hub","date":"2017-07-07T09:22:34.000Z","updated":"2017-07-07T07:04:56.000Z","comments":true,"path":"hub/","link":"","permalink":"http://vcpu.me/hub/","excerpt":"","text":"集线器集线器（HUB），它是工作在物理层的设备，由于它只是工作在物理层的设备，所以它并不关心也不可能关心OSI上面几层所涉及的，它的工作机制流程 是：从一个端口接收到数据包时，会在其他端口把这个包转发一次，因为它不知道也不可能知道这个包是发给谁的（物理层设备只关心电压这些物理概念），它也只能对所有人广播(这里和下文提到的广播该词的意思和ARP请求时的广播有些不同。 这里的广播意思是：使用物理层转发设备，如HUB，导致的广播，可以说这个广播是被逼的，因为设备的问题！是设备转发包引起的广播！而ARP请求的 广播是自己要求的，主动的，因为ARP请求包的目标地址IP是255.255.255.255，但ARP请求的广播涉及IP层的知识，不在这篇文章讨论的 范围，所以这里提到的广播，除非特别说明，否则都是第一个意思，也就说是”因设备转发数据包引起的广播” )，让他们自己处理了。 这样一来会有不少问题，你发的数据其他人都收到了，私隐这总东西是不存在的！别入可以随便监听你信息！所以会话劫持在那个年代相当容易（记得俺第一次接触会话劫持这个概念的时候还是高2，那是2001~2002，呵，那时候集线器还是比较普遍的）。 另外一个比较严重的问题是，如果一个大型的局域网，比如有500台机器，全部用HUB连接的，后果会怎么样呢？？相当慢，网络的效率极差！为什么？ 如果500台机器都发一个包，那就是说每台机器，都需要接收差不多499个无用包…并且如果是需要回应的话……无用的数据包会充斥着整个的局 域网，这就是传说中的广播风暴！ 为了减少广播风暴，网桥产生了（注意这里用的时候“减少”，不是“杜绝”，仅仅是减少!如果仅仅用网桥说能杜绝广播风暴，个人觉得还是不太准确，后来交换机的出现才可以说是完全杜绝了广播风暴的发生）！ 在介绍网桥之前，还想简单介绍另一个物理层的设备：“中继器”，这种设备的作用是把物理层传输的信号放大，由于长距离的传输，信号会有一定的损耗的，这种设备主要解决的就是这个问题。它和HUB的区别是：HUB主要是为了在物理层上转发数据的，所以它不关心电压值的大小，也不会放大物理信号；而中继器它的作用就是为了放大信号用的，SO….. 网桥网桥又称桥接器，英文名Network Bridge，数据链路层设备。它也是转发数据包的设备，但和HUB不一样的是，它工作在数据链路层，HUB只能看懂物理层上的东西（比如一段物理信号），网桥却能看懂一些帧的信息（在链路层上，把上面传下来的数据封装后，封装好了的数据就是帧，但这里我用“数据包”这样的泛指去代替“帧”这个专业术语）。 在以太网构造的局域网上，最终的寻址是以数据链路层的MAC地址作为标识的(就是用MAC地址可以在局域网上找到一台唯一的机器)，网桥能从发来的 数据包中提取MAC信息，并且根据MAC信息对数据包进行有目的的转发，而不采用广播的方式，这样就能减少广播风暴的出现，提升整个网络的效率，在详细说网桥这东西之前，我想先介绍一下交换机。 交换机： 这是大家最熟悉的设备之一 交换机Switch，数据链路层设备，作用是转发数据包。和网桥一样它也是通过识别帧中的MAC地址，然后对特定的端口进行转发的。 网络基础之网桥和交换机的工作原理及区别网桥和交换机区别在叙述前，我们先一起看两幅图： 网桥的连接模式：(红点处为HUB) 交换机连接模式：从图中可以看到，网桥只有2个（输入/出）端口，而交换机有8个。嗯，是的，一开始的时候(那时候只有HUB这种设备)，由于硬件水平不是很发达，人们为了提高局域网效率，减少广播风暴的出现，他们生产了网桥（一个只有两个输入/出端口的链路层设备，这时的网桥已经是个比较先进的设备），然后他们把一个局域网一分为2，中间用网桥连接，这样A发给BCD的数据就不会再广播到EFGH了(网桥发现如果数据包不是转发给下面这个子网的，它会自动丢弃此包)，只有从A 发到EFGH的数据包才能通过网桥，到达另外一个子网(网桥发现如果数据包是转发给下面这个子网的，它才会把包转发给这个子网)。 这样一来，非必要的传输减少了，整个网络的效率也随之提高可不少！人们发现网桥真是个好东西呀，随着硬件发展，出现了4个，8个端口的链路层设备，这就是交换机，由于交换机可以使得网络更安全(数据不容易被监听，因为数据不再广播了，注意：只是不容易而已，要搞你其实还是可以的)，网络效率更高(还是因为数据不再广播！)，交换机渐渐替代了HUB，成为组建局域网的重要设备。 所以说，网桥和交换机，基本上是一样的，嗯，只是基本上而已，细看还是有些不一样的，但在说明他们之间有什么不一样之前，我想先简单介绍一下网桥和交换机的工作原理。 网桥的工作原理： 上图是用一个网桥连接的两个网络，网桥的A端口连接A子网，B端口连接B子网，为什么网桥知道哪些数据包该转发，哪些包不该转发呢？那是因为它有两个表A和B，当有数据包进入端口A时，网桥从数据包中提取出源MAC地址和目的MAC地址。 一开始的时候，表A和表B都是空的，没有一条记录，这时，网桥会把数据包转发给B网络，并且在表A中增加一条MAC地址(把源MAC地址记录表中)，说明这个MAC地址的机器是A子网的，同理，当B子网发送数据包到B端口时，网桥也会记录源MAC地址到B表。 当网桥工作一段时候后，表A基本上记录了A子网所有的机器的MAC地址，表B同理，当再有一个数据包从A子网发送给网桥时，网桥会先看看数据包的目的MAC 地址是属于A子网还是B子网的，如果从A表中找到对应则，抛弃该包，如果不是，则转发给B子网，然后检查源MAC地址，是否在表中已经存在，如果不存在，在表A中增加一条记录。 噢，或许你现在会问了，为什么需要两张表呢，一张表不行么？？嗯～刚才把表一分为二是为了便于理解，实际上，真正的网桥里面存的应该是一张表(当然有可能为了提速，或者其他原因，它也可能把信息存为多张表，这个得看它怎么实现了～)，如果是一张信息表，表里记录的应该是：MAC-PortNum。 交换机有些许不同，如图： 交换机也有一张MAC-PORT对应表(这张表的学名为：CAM)，和网桥不一样的是，网桥的表是一对多的(一个端口号对多个MAC地址)，但交换机的CAM表却是一对一的，如果一个端口有新的MAC地址，它不会新增MAC－PORT记录，而是修改原有的记录 例如：现在交换机记录表里已经有一项：MAC1-Port1，如果此刻端口1又来了一个数据包，里面的源MAC地址是MAC2，此时，交换机会刷新交换机记录表：MAC1-Port1记录被修改为MAC2－Port1，因为交换机认为是端口1的计算机MAC地址变了，如果端口1连接的一台物理机器，MAC一般是不会变的，如果连接的是另外一个交换机，那这个端口的记录会变化得比较频繁(如上图的Port12，它是对外的接口，与一个局域网连接)，另外，如果 CAM表中没有找到和数据包的目的MAC地址对应的记录，交换机会对此数据包进行广播，发给本交换机的每一个端口。 网桥和交换机的缓存对比网桥和交换机除了用CAM表指导数据包转发这点和HUB不一样外，网桥和交换机还有一个特点：缓存！对，网桥和交换机都有一定量的缓存，因为网桥和交换机转发数据包需要一些额外的操作，所以可能会占用一些时间，为了避免出现因来不及转发数据，导致大量数据丢失的情况，网桥和交换机就出现了缓存。 当然，缓存不是万能的，当网桥或交换机处理不及并且缓存用完了，以后再来的数据还是会丢失的。还一个网桥需要缓存的原因是：桥接两个传输速率不同的局域网，比如：802.3的传输速率说10mb/s，但实际上并不是真的10Mb/s，而 802.4(几乎)确实为10Mb/s，但两个局域网桥接时候必须是速率一样的，否则会有包丢失，而缓存也正好可以为不同速率网络连接时所使用。 网桥的缓存量是大于交换机的缓存量，因为网桥天生是为了连接两个网络的，两个网络桥接时所要处理的数据量会比一般的交换机需要处理的数据量要多，所以网桥的缓存也一般要比交换机大一点。","categories":[{"name":"网络","slug":"网络","permalink":"http://vcpu.me/categories/网络/"}],"tags":[{"name":"集线器","slug":"集线器","permalink":"http://vcpu.me/tags/集线器/"},{"name":"网桥","slug":"网桥","permalink":"http://vcpu.me/tags/网桥/"},{"name":"中继器","slug":"中继器","permalink":"http://vcpu.me/tags/中继器/"},{"name":"交换机","slug":"交换机","permalink":"http://vcpu.me/tags/交换机/"}]},{"title":"拥塞控制","slug":"TCP拥塞控制","date":"2017-07-04T10:00:00.000Z","updated":"2017-07-05T01:33:00.000Z","comments":true,"path":"TCP拥塞控制/","link":"","permalink":"http://vcpu.me/TCP拥塞控制/","excerpt":"TCP拥塞控制设计概念延迟确认定时器 TCP延时确认定时器是指在一个TCP连接中，当一方收到另一端的数据后，并不是立刻ACK确认，而是等待200ms（2.6内核 40ms），如果这段时间内有新的数据要发往对方，本地奖ACK和数据封装在一个数据包中捎带发送；如果这段时间内没有新的数据要发往对方，200ms后ack确认。 优点：减少传输消耗缺点：增加了延迟 超时重传定时器超时重传是TCP协议栈保障数据可靠性的一个重要机制原理： 发送一个数据后同时开启定时器，在一定时间内如果没有收到对方确认，定时器激活重新发送数据包，直到发送成功或者到达最大重传次数。","text":"TCP拥塞控制设计概念延迟确认定时器 TCP延时确认定时器是指在一个TCP连接中，当一方收到另一端的数据后，并不是立刻ACK确认，而是等待200ms（2.6内核 40ms），如果这段时间内有新的数据要发往对方，本地奖ACK和数据封装在一个数据包中捎带发送；如果这段时间内没有新的数据要发往对方，200ms后ack确认。 优点：减少传输消耗缺点：增加了延迟 超时重传定时器超时重传是TCP协议栈保障数据可靠性的一个重要机制原理： 发送一个数据后同时开启定时器，在一定时间内如果没有收到对方确认，定时器激活重新发送数据包，直到发送成功或者到达最大重传次数。 RTO(重传超时时间) 如果RTO值被设置过大，将会使得发送端经过很长时间等待才能发现报文段丢失，会降低吞吐如果RTO值被设置很小，发送端尽管可以很快的检测出报文段的丢失，但也可能将一些延迟大的报文段误认为是丢失，造成不必要重传，浪费网络资源。 慢启动阈值慢启动阈值是慢启动算法和拥塞避免的分水岭，当拥塞窗口大于慢启动阈值时，就用拥塞避免慢启动阈值门限就用慢启动 SACK机制SACK TCP选项 SACK是TCP一个选项，握手过程中会协商判断是否支持SACK，如果支持会在TCP选项中SACK permitted。SACK 选项格式 TCP不能超过四组SACK边界，因为TCP选项最大支持40个字节 作用描述 SACK通常是由数据接收方产生，收到的SYN包中有SACK-Permitted选项为true，同时自己也支持SACK，可以在接收数据异常时候，产生SACK option。SACK中需要携带接收队列中没有被确认的数据信息，已经确认的数据信息不会通过SACK携带。发送端SACK含义： 第一个block指出是哪个segment触发了SACK 选项，发sack人认为是谁乱序了才导致SACK 尽可能填满SACK SACK需要报告最近接收的不连续的数据块接收端： 数据没有被确认前都会保持在滑动窗口内 每一个数据包都有一个sakced标记，发送数据会忽略被sacked的数据 如果SACK丢失，超时重传后重置所有数据包的SACKed标记SACK分析 适用于多包丢失情况，可以快速退出快速恢复阶段；如果丢包率很低，或者丢包时常常只丢一个包，那么SACK就是多余的。 D-SACKD-SACK主要是使用SACK来告诉发送方有哪些数据被重复接收了，如果是D-SACK；D-SACK option的第一个block代表呗重复发送的序号片段。注意：D_SACK仅仅是接收端的报告重复连续的片段每个重复连续片段只能在第一个block，其它block是接收端已经收到但是还没有ACK的数包汇报重复收到片段 此时数据发送端，可以知道，是对方的ACK丢失了导致此种情况 慢启动过程最初的TCP在建立完成后会向网络中发送大量的数据包，这样很容易导致网络中路由缓存空间耗尽，从而发生拥塞。慢启动就是为解决该问题，其思想是一开始不是发大量的数据包而是根据网络情况逐步增加每次的发送的数量，以避免上述现象的发生。其基本做法为：新建连接时候，CWND初始化1个最大报文段大小，每当一个报文段被确认，拥塞窗口就增价1个mss大小，在发出的报文段均被正常确认情况 下，拥塞窗会随着往返时间指数增长。慢启动并不慢。RTTlogW 时间可占满带宽。 拥塞避免拥塞窗口不能一直增长下去，其受慢启动阈值（一开始为65535）限制；如果超过该阈值，进入拥塞避免阶段。 拥塞避免思想 拥塞窗口的加法增大，拥塞窗口随着RTT开始线性增加，这样可以避免增长过快导致网络拥塞，慢慢调整到网络的最佳值。 如何确定拥塞状态超时重传此时发出去的报文在RTT时间内没有被确认，此时发生拥塞可能性较大，TCP协议栈强烈反应为： 慢启动阈值 降低为当前拥塞窗口的一半拥塞窗口设置为1，从新进入慢启动 快速重传收到3个相同的ACK，TCP在收到乱序包时就会立即发送ACK，TCP利用3个相同的ACK判定数据包失效，发送快速重传，TCP协议栈强烈反应为： 慢启动阈值降低设置为拥塞窗口的一半拥塞窗口设置为慢启动阈值，从新进入拥塞避免阶段 快速恢复当收到三个重复ACK时，随着TCP协议栈改进TCP进入的不是拥塞避免，而是快速恢复。快速恢复思想是： 数据包守恒原则，当老的数据包离开了网络后，才能向网络中发送一个新的数据包。 如果收到一个重复ACK，代表已经有一个数据包离开了网络，于是拥塞窗口加1，此时能向网络中发一个新的数据包。 具体步骤： 当收到3个重复ACK，慢启动阈值会降为拥塞窗口的一半，把拥塞窗口设置为慢启动阈值大小+3（3个数据包离开网络）； 再收到重复ACK时，拥塞窗口+1 当收到新的数据包ACK时候，拥塞窗口设置为慢启动阈值，快速恢复结束，可以回到拥塞避免阶段了 上述算法是reno算法，新版本的reno算法差异为： reno算法发再收到一个新的数据ACK退出快速恢复状态，new reno 需要收等到改窗口所有的数据包确认才会推出快速恢复。 TCP协议栈拥塞窗口改变时机拥塞窗口改变只发生在收到ACK和重传定时器超时。 LINUX协议栈拥塞控制状态划分open状态慢启动和拥塞避免状态都是没有发生拥塞，网络畅通的状态，linux协议栈使用open状态来表示慢启动和拥塞避免。 当TCP会话初次建立连接时，还没有发生拥塞，这时慢启动阈值无法估计，linux协议栈设置为极大值0xffffffff；Reno和Cubic算法在慢启动阶段都是当有n个数据报被确认，拥塞窗口就自增n，但是两者在拥塞避免和慢启动阈值计算上，是不同的。 diorder状态在Linux内核TCP实现中，有一个disorder状态，此状态是open状态向快速重传过度的一个状态，收到3个重复确认才开开始快速重传，从收到第一个重复确认到收到第三个重复确认这段时间，处于disorder状态。 设置disorder状态的必要性在于，当收到重复确认比较少时，我们还没法判断当前是否发生丢包，因为对端收到乱序报文，也会发送选择确认。如果重复确认足够多比如3个重复确认，就进入recovery状态，而如果在收到1-2个重复确认再收到数据确认，则回到open状态。 从disorder状态回到open状态时，拥塞窗口和慢启动阈值不发生变化，会继续进行慢启动或者拥塞避免。 reocvery状态在linux内核实现中快速重传（动作）和快速恢复（阶段）用recovery状态表示。快速恢复阶段，指的是从快速重传开始，到网络上没有丢失的报文，可以回到open状态的这段时间，拥塞窗口和慢启动阈值也调整回去。 LOSS状态重传定时器超时以后所处状态就是Loss状态。 重传定时器超时时，代表网络环境已经极差，此时会拥塞窗口变得很小（一般1），同时调整慢启动阈值为一半，重新开始慢启动算法；认为之前发出去的所有数据包均已经丢失，重新开始慢启动算法。 重传定时器超时之前所有已发的数据包被确认后才退出快速loss状态。 Loss状态和Open状态差别 Loss状态是采用慢启动来重传丢失的报文，知道丢失的报文被确认后才发新的数据包，而Open状态没有丢失数据包，一直再发新的数据包。 Linux协议栈拥塞控制的初始化当TCP从syn sent或者syn recv进入 established状态时，初始话拥塞窗口和慢启动阈值。kernel2.6版本拥塞窗口会根据mss大小来进行初始话，mss值越大窗口就越小。 因为网络最初建立时候，网络情况很难估计，一半慢启动阈值会被设置很大0x7fffffff直到察觉到拥塞发生时才做调整。 Linux在TCP实现中，当TCP连接进入TW和LAST_ACK准备关闭连接时候，会选者性地将拥塞窗口和慢启动阈值保存下来。作为下一次同一条线路TCP会话建立时的拥塞窗口和慢启动阈值初始值。 Linux协议栈拥塞控制相关杂谈发送数据包的限制 发送数据包实际上受三个条件限制：条件1:发送端的拥塞窗口条件2:接收端的接收窗口条件3:发送队列上实际数据包量 慢启动阶段拥塞窗口调整策略慢启动阶段，有多少数据包被确认久增加多少数据包。一般的描述为一个传输轮次，拥塞窗口翻倍。这与有多少数据包被确认，拥塞窗口增加多少是一个意思。后者更适合于写代码。拥塞避免阶段拥塞窗口调整 没收到一定个数的ACK拥塞窗口加1reno算法拥塞避免表现为： 拥塞窗口在维持一段时间内的恒定值后，自增1 disorder阶段拥塞窗口调整 disorder是一个观望状态，拥塞窗口和慢启动阈值保持不变。即此时满足in_flight &lt; 拥塞窗口，对端能接收，并且发送队列中还有数据，则会继续发送数据。 disorder收到数据确认时候，先前引发重复确认报文段已经到达了接收端，这时会重新回到open状态，拥塞窗口和慢启动阈值没有发生任何变化。 disorder接收到足够多的重复确认数据包时（一般3个），会进入revocery状态。 快速恢复阶段拥塞窗口调整进入快速恢复之前，调整慢启动阈值，等退出快速恢复阶段时，拥塞窗口等于慢启动阈值。快速恢复阶段不同恢复算法用不同的拥塞窗口调整策略。 快速恢复阶段突降式调整突降方式调整是指进入快速恢复阶段，就立刻调整拥塞窗口。windows采用的就是这种调整方法。 快速恢复阶段比例式拥塞窗口调整拥塞窗口缓慢过度至慢启动阈值。 快速重传阶段继续收到重复确认处理Linux协议栈采用的方法为： 如果拥塞窗口没有降到慢启动阈值，每收到两个重复ACK，拥塞窗口减1，并发送丢失的数据段。否则拥塞窗口不变，每收到一个重复ACK就重传一次。在此期间不发送新的数据段。 快速重传部分确认处理Linux2.6的处理是： ACK确认了新数据时候，有两种可能性。第一种，重传数据到达对端 第二种，正常数据到达对端为了区分这两种情况，协议栈会比较新数据ack的时间戳和发生重传数据时间戳。如果后发的重传数据，则说明数据包没有丢失，只是晚到达了，慢启动阈值会被恢复到之前的值；此时不重传此ACK的数据包。 如果是重传到达对端，导致的新数据ACK产生；此时重传ACK报文，如果拥塞窗口允许，就发数据。 Loss状态收到ACK如果从ack判断出这个ack报文在我们最后一次重传前就已经发出，这种情况代表重传是没有必要重传，其重传数据段已经到达了对端。把慢启动窗口和阈值恢复到loss状态前的值，就当什么也没有发生过，继续发送数据。 否则： 继续重传队列中剩余的数据，重传过程中不发新数据；重传完成后按照拥塞窗口进行发送。进入Loss时候会记录至少重传最大序列号；在确认这些数据传输完成，协议栈从Loss退出进入open。 拥塞控制的误判发生在进入Recovery和Loss状态时，会发送误判。并不是收到三个重复ACK就一定代表需要快速从传，并不是重传定时器超时就一定是网络拥塞。 误判进入recovery状态导致问题：拥塞窗口和慢启动阈值减小是不必要的，会导致不必要的性能下降。 超时误判：由于网络状态的不稳定性，RTT会经常变化，超时时有可能发生的。单相对恢复误判，超时误判发生可能性小，因为重传定时器RTO比RTT大。 误判的判定：recovery误判：如果重传数据到达之前，ack就确认了这个数据包，则说明为误判，直接回到以前状态。 这种误判恢复linux需要时间戳的支持。 误判发生经常伴随着时间戳和sack出现。 重定序临界值这个临界值就是我们经常说的3个重复ACK的值，在kernel的描述；实际上该值是可能变化的，例如误判发生时，该临界值会变大。这也是linux较为保守的实现。 Linux拥塞控制概述 慢启动阶段涨的很快，拥塞避免阶段探测性增长，收到重复确认先观望，收到足够多的重复确认时开始快速重传；在快速恢复阶段争取把可能丢失的报文重传出去，并适当发送新的数据包以维持协议栈正常运转。当退出快速重传时，拥塞窗口和慢启动阈值按预期的值变小。并会到拥塞避免阶段。同时挂一个定时器来处理长时间未收到报文的情况。当重传定时器超时时，批量重传，重新开始慢启动。 慢启动大家基本默认翻倍增长，在一次TCP的生命周期中，只要不断网，超时重传可能性很小，所以拥塞控制算法的主要集中在拥塞避免阶段，偶尔处在recovery状态；即 拥塞避免-recovery-拥塞避免-recovery拥塞控制性能差异，主要来源于： 1 拥塞避免阶段初始窗口大小，即推出快速恢复状态时的慢启动阈值 2.拥塞避免阶段的拥塞窗口调整策略 3.快速恢复状态的快慢（指不支持sack） 拥塞控制状态图 慢启动阶段1.发出去数据均能被ack 拥塞窗口持续指数增加，另外一种描述发出去的数据包被ACK多少就额外增加多少数据包。这个情况下拥塞窗口会持续增加。 2.收到重复ACK但是还没有达到3个 拥塞窗口和慢启动阈值不会变化，此时发包行为取决于当前和拥塞窗口接收端的窗口，以及当前发送队列上的数据量。如果其后续收到了新的数据ack，会继续进行慢启动，就好像什么也没有发生一样。 如果没有收到新数据ack，会引发快速重传。 3.收到足够重复ACK 足够的重复ack引发快速重传，慢启动阈值降低为当前拥塞窗口的一半，拥塞窗口设置为慢启动阈值+3（已经收到3个重复ack，代表3个数据包已经离开网络到达对端）；此时此刻会重传丢失的数据包，具体要不要发新数据包完全取决于当前的拥塞窗口。如果丢失的数据包统统确认已经到达对端，会退出快速恢复从新进入open状态；进入open状态后因为拥塞窗口大于慢启动阈值，所以 会进入拥塞避免阶段。 4.发送数据包超时 慢启动阈值变为当前拥塞窗口一半，拥塞窗口减少到1，会按照慢启动发包行为发送认为已经丢失的报文；待这些报文被确认后退出超时阶段从新回到open。按照当前的拥塞窗口和慢启动阈值进行慢启动或者拥塞避免发包。 拥塞避免阶段1.发出去数据均被ack 拥塞窗口会继续线性增长，慢启动阈值保持不变。 2.收到重复ack但是还没有达到3个 此状态属于TCP协议栈的disorder状态，慢启动阈值保持不变，拥塞窗口保持不变，协议栈发包行为收协议栈拥塞窗口的限制。如果后续收到新数据的ACK，将从disorder状态回到拥塞避免阶段，从新线性增长方式发包。 3.收到足够重复ack 足够的重复ack引发快速重传，慢启动阈值降低为当前拥塞窗口的一半，拥塞窗口设置为慢启动阈值+3；此时此刻会重传丢失的数据包，具体要不要发新数据取决于当前的拥塞窗口。如果丢失数据包通通被确认已经到达对端，会退出快速恢复阶段进入拥塞避免阶段。 4.发送数据包超时 慢启动阈值变成当前拥塞窗口一半，拥塞窗口减少到1，会按照慢启动发包行为完成已经发送数据包的重传。待这些报文被确认后退出loss阶段，从新回到open状态，根据慢启动阈值和拥塞窗口大小进行慢启动或者拥塞避免发包。","categories":[{"name":"TCP","slug":"TCP","permalink":"http://vcpu.me/categories/TCP/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"http://vcpu.me/tags/拥塞控制/"},{"name":"Linux拥塞控制基本概念","slug":"Linux拥塞控制基本概念","permalink":"http://vcpu.me/tags/Linux拥塞控制基本概念/"}]},{"title":"软中断和硬中断","slug":"interrupt","date":"2017-07-04T04:00:00.000Z","updated":"2017-07-04T03:28:38.000Z","comments":true,"path":"interrupt/","link":"","permalink":"http://vcpu.me/interrupt/","excerpt":"中断概述Linux内核需要对连接到计算机上所有硬件设备进行管理，毫无疑问这是它分内的事情。其通过中断机制让管理的硬件设备主动通知，而不是其主动轮询。中断是一种电信号，由硬件设备产生送入中断控制器的输入引脚，然后中断控制器会想处理器发出信号；处理器收到该信号后，停下当前正在处理的事情，跳到中断处理程序的入口点，进行中断处理。当然处理器会通知操作系统已经产生中断；操作系统也可能会进行适当的处理。 处理器通过中断向量识别产生的中断，linux系统下Intel X86支持256中断向量,中断编号0-255 0-31 异常 非屏蔽 固定不变32-47 屏蔽中断（IO设备）48-25 软中断 硬件中断硬中断是外部设备对CPU的中断，硬中断可抢占软中断，优先级高执行较快。硬中断的本质是接收到中断信号后，跳转到公共段代码执行do_IRQ，并切换到硬中断请求栈，执行中断回调函数。","text":"中断概述Linux内核需要对连接到计算机上所有硬件设备进行管理，毫无疑问这是它分内的事情。其通过中断机制让管理的硬件设备主动通知，而不是其主动轮询。中断是一种电信号，由硬件设备产生送入中断控制器的输入引脚，然后中断控制器会想处理器发出信号；处理器收到该信号后，停下当前正在处理的事情，跳到中断处理程序的入口点，进行中断处理。当然处理器会通知操作系统已经产生中断；操作系统也可能会进行适当的处理。 处理器通过中断向量识别产生的中断，linux系统下Intel X86支持256中断向量,中断编号0-255 0-31 异常 非屏蔽 固定不变32-47 屏蔽中断（IO设备）48-25 软中断 硬件中断硬中断是外部设备对CPU的中断，硬中断可抢占软中断，优先级高执行较快。硬中断的本质是接收到中断信号后，跳转到公共段代码执行do_IRQ，并切换到硬中断请求栈，执行中断回调函数。 硬件中断流程硬中断的汇编处理-&gt;do_IRQ-&gt;handle_irq-&gt;handle_edge_irq(handle_level_irq)-&gt;handle_irq_event-&gt;具体设备的硬中断处理 嵌套linux下硬件中断可以嵌套，且无优先级别；除同种中断外，一个中断可打断另一个中断。此种机制短时间内可以接受更多的中断，可以有大的设备控制吞吐量；无优先级可以简化内核。同种中断处理机制可以描述为，中断数据结构会设置IRQD_IRQ_INPROGRESS中断不处理标识，本地CPU或者其它CPU如果检查到此种中断的该标记，会直接退出，置上IRQS_PENDING后续处理标记。 软中断软中断是硬中断服务程序对内核的中断，软中断时一种推后执行的机制，软中断是bottom half，上半部在屏蔽中断的上下文中运行，软中断相对来讲不是非常紧急，通常还比较耗时，不会在中断上下文中执行系统会自行安排运行时机。软中断不会抢占另一个软中断。 原理概述1.软中断通过open_softirq注册一个软中断处理函数，在软中断向量表softirq_vec数组中添加新的action函数1234567//定时器init_timers调用初始化软中断调用函数open_softirq(TIMER_SOFTIRQ, run_timer_softirq);...void open_softirq(int nr, void (*action)(struct softirq_action *))&#123; softirq_vec[nr].action = action;&#125; 2.调用raise_softirq软中断触发函数，即软中断标记为挂起状态12345678910111213141516171819202122232425262728293031/* * This function must run with irqs disabled! */inline void raise_softirq_irqoff(unsigned int nr)&#123; //设置 __raise_softirq_irqoff(nr); /* * If we're in an interrupt or softirq, we're done * (this also catches softirq-disabled code). We will * actually run the softirq once we return from * the irq or softirq. * * Otherwise we wake up ksoftirqd to make sure we * schedule the softirq soon. */ //不能在硬中断，必须要硬中断处理完 //不能在软中断里，软中断不能嵌套 if (!in_interrupt()) wakeup_softirqd();&#125;void raise_softirq(unsigned int nr)&#123; unsigned long flags; //关闭本地CPU中断 local_irq_save(flags); raise_softirq_irqoff(nr); local_irq_restore(flags);&#125; 内核会在一些位置检查是否有挂起状态的软中断，如果有的话调用do_softirq执行软中断处理action函数3.do_softirq完成两件事情（1）切换到软件请求栈，让其处于软中断上下文（2）执行do_softirq4.do_softirq（1）执行软中断处理函数（2）如果软中处理函数超过10个，唤醒内核线程让其处理本地CPU软中断。 软中断本质就是内核在某些位置检查是否有挂起的软中断（local_software_pending()不为0指有挂起软中断），若有则调用do_softirq切换到软中断请求栈，调用__do_softirq。 进程角度看软中断执行过程步骤1:将返回四值和CPU状态寄存器压栈步骤2:修改特权级别（系统程序需要核心态特权才能运行，用户态函数只能通过软中断调用系统API）,设置中断事务标记步骤3:唤醒守护线程，检测中断状态寄存器，发现软中断事务步骤4:根据中断号通过查找中断向量表，找到ISR中断服务历程地址，跳转执行步骤5:中断服务程序执行完成后，返回压栈的函数执行点 嵌套软中断不打断软中断，相同软中断可在所有CPU上同时执行 软中断触发时机（1）调用do_IRQ完成I/O中断时调用irq_exit irq_exit-&gt;invoke_softirq-&gt;do_softirq（2）如果系统使用I/O APIC，在处理完本地时钟中断时（3）local_bh_enable-&gt;do_softirq1234void local_bh_enable(void)&#123; _local_bh_enable_ip(_RET_IP_);&#125; （4）在SMP中，当CPU处理完被CALL_FUNCTION_VECTOR处理器间中断所触发的函数时：12345678void smp_trace_call_function_interrupt(struct pt_regs *regs)&#123; smp_entering_irq(); trace_call_function_entry(CALL_FUNCTION_VECTOR); __smp_call_function_interrupt(); trace_call_function_exit(CALL_FUNCTION_VECTOR); exiting_irq();&#125; exiting_irq-&gt;irq_exit __do_softirq 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485asmlinkage void __do_softirq(void)&#123; struct softirq_action *h; __u32 pending; //软中断结束时间 unsigned long end = jiffies + MAX_SOFTIRQ_TIME; int cpu; unsigned long old_flags = current-&gt;flags; //软中断执行次数10次 int max_restart = MAX_SOFTIRQ_RESTART; /* * Mask out PF_MEMALLOC s current task context is borrowed for the * softirq. A softirq handled such as network RX might set PF_MEMALLOC * again if the socket is related to swap */ current-&gt;flags &amp;= ~PF_MEMALLOC; //获得CPU的软中断掩码，这时候仍然是关中断，可安全获得掩码 pending = local_softirq_pending(); //统计信息：进程被中断使用时间 account_irq_enter_time(current); //执行完该函数后，关闭软中断，后续即使硬件再次触发新的软中断，也不会重新进入__do_softirq __local_bh_disable(_RET_IP_, SOFTIRQ_OFFSET); lockdep_softirq_enter();//just for debugging cpu = smp_processor_id(); restart: /* Reset the pending bitmask before enabling irqs */ //中断掩码清0，当然局部变量pending已经存储下来了，开启硬件中断后，可设置上新的软中断了 set_softirq_pending(0); //开硬件中断，由于软中断执行时间一般较长，这里将中断打开避免长时间关中断，这段处理时间硬件中断就不会丢失了 local_irq_enable(); h = softirq_vec; do &#123; if (pending &amp; 1) &#123;//中断挂起 unsigned int vec_nr = h - softirq_vec;//获取中断号 //保存抢占计数，后续无法破坏该计数了 int prev_count = preempt_count(); //软中断在每个核上执行计数 kstat_incr_softirqs_this_cpu(vec_nr); trace_softirq_entry(vec_nr); //执行回调函数 h-&gt;action(h); trace_softirq_exit(vec_nr); //软中断回调函数破坏了抢占计数，打印高级别警告信息，并恢复抢占计数 if (unlikely(prev_count != preempt_count())) &#123; printk(KERN_ERR \"huh, entered softirq %u %s %p\" \"with preempt_count %08x,\" \" exited with %08x?\\n\", vec_nr, softirq_to_name[vec_nr], h-&gt;action, prev_count, preempt_count()); preempt_count() = prev_count; &#125; rcu_bh_qs(cpu); &#125; //处理下一个软中断 h++; pending &gt;&gt;= 1; &#125; while (pending);//无软中断循环结束//处理完一轮软中断后，因为处理时候中断是开启的，可能发生了硬件中断重新触发了软中断//我们就关中断保障中断掩码再被修改 local_irq_disable(); //如果没有超过10次，且处理时间也在合法范围内，继续处理,否则唤醒ksoftirqd守护线程处理软中断 pending = local_softirq_pending(); if (pending) &#123; if (time_before(jiffies, end) &amp;&amp; !need_resched() &amp;&amp; --max_restart) goto restart; //调用线程处理剩下的中断 wakeup_softirqd(); &#125; lockdep_softirq_exit(); account_irq_exit_time(current); __local_bh_enable(SOFTIRQ_OFFSET); tsk_restore_flags(current, old_flags, PF_MEMALLOC);&#125; 防止软中断嵌套的流程：关软中断中肯定有一句原子地加1的关键语句，如果当前内核路径A在该原子操作之前被另一个内核路径B打断，则B执行完硬中断和软中断后，返回到A的此处，A接着执行该原子操作，之后的软中断处理应该是空转，因为肯定已经被B处理完了。如果在该原子操作之后被B打断，则B执行完硬中断，不会执行自己的软中断而是会直接退出（因为软中断嵌套了），返回到A的此处，A接着执行，这次A除了处理自己软中断，还会额外地处理B的软中断。对于preempt_count中的软中断位，由上述可以知道，它的作用有两个：防止软中断在单cpu上嵌套；保证了在执行软中断期间不被抢占。 ksoftirqd进程run_ksoftirqd是ksoftirqd线程的核心处理函数123456789101112131415static void run_ksoftirqd(unsigned int cpu)&#123; //1.把当前CPU中断中断关掉 local_irq_disable(); //2.当前CPU是否有软中断 if (local_softirq_pending()) &#123; //3.处理软中断 __do_softirq(); rcu_note_context_switch(cpu); local_irq_enable(); cond_resched(); return; &#125; local_irq_enable();&#125; 该内核线程的优先级较低，且采用关闭中断保护方式，而不是关闭抢占保护方式，让更多的软中断被其它人调用执行。达到ksoftirqd进程的辅助作用。一旦开始执行中断就不允许抢占了，软中断和硬中断都是这个做法，在执行期间不允许调度。","categories":[{"name":"linux","slug":"linux","permalink":"http://vcpu.me/categories/linux/"}],"tags":[{"name":"软中断","slug":"软中断","permalink":"http://vcpu.me/tags/软中断/"},{"name":"硬中断","slug":"硬中断","permalink":"http://vcpu.me/tags/硬中断/"}]},{"title":"惊群探究","slug":"惊群","date":"2017-06-29T04:00:00.000Z","updated":"2017-06-29T03:06:20.000Z","comments":true,"path":"惊群/","link":"","permalink":"http://vcpu.me/惊群/","excerpt":"惊群发生在多进程或者多线程，等待同一个socket事件，当该事件发生，这些进程或者线程都被唤醒 发生位置2.6版本内核accept已经解决该问题了，但是select／poll或者epool_wait仍然存在该问题 产生影响一个连接来临时，多个子进程同时被唤醒，却只有一个子进程accept成功，其余都失败，重新休眠；产生了没有必要的唤醒和上下文切换，造成性能浪费。 惊群实例分析总体结论：accept linux内核已经解决惊群问题3.10.0-514.16.1.el7.x86_64，具体唤醒方式实现参见socket信号处理博文select/pool和epool内核并没有支持解决惊群问题。之前一直觉得epool和select使用非常平凡，为什么操作系统不直接自己解决惊群问题？应该是epool和select不仅仅使用在socket监听中，会存在需要唤醒多个进程的使用场景。","text":"惊群发生在多进程或者多线程，等待同一个socket事件，当该事件发生，这些进程或者线程都被唤醒 发生位置2.6版本内核accept已经解决该问题了，但是select／poll或者epool_wait仍然存在该问题 产生影响一个连接来临时，多个子进程同时被唤醒，却只有一个子进程accept成功，其余都失败，重新休眠；产生了没有必要的唤醒和上下文切换，造成性能浪费。 惊群实例分析总体结论：accept linux内核已经解决惊群问题3.10.0-514.16.1.el7.x86_64，具体唤醒方式实现参见socket信号处理博文select/pool和epool内核并没有支持解决惊群问题。之前一直觉得epool和select使用非常平凡，为什么操作系统不直接自己解决惊群问题？应该是epool和select不仅仅使用在socket监听中，会存在需要唤醒多个进程的使用场景。 accept实例背景：3.10.0-514.16.1.el7.x86_64内核下，在创建、绑定和监听后，创建多进程accept 建立好的fd，当一个客户端访问该服务端时候，观察进程的连接获取情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 9999int main(int argc,char **argv)&#123; int listenfd,connfd; pid_t childpid,childpid2; socklen_t clilen; struct sockaddr_in cliaddr,servaddr; listenfd = socket(AF_INET,SOCK_STREAM,0); bzero(&amp;servaddr,sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl (INADDR_ANY); servaddr.sin_port = htons (SERV_PORT); bind(listenfd, (struct sockaddr *) &amp;servaddr, sizeof(servaddr)); listen(listenfd,1000); clilen = sizeof(cliaddr); if( (childpid = fork()) == 0) &#123; while(1) &#123; connfd = accept(listenfd,(struct sockaddr *) &amp;cliaddr,&amp;clilen); printf(\"fork 1 is [%d],error is %m\\n\",connfd); &#125; &#125; if( (childpid2 = fork()) == 0) &#123; while(1)&#123; connfd = accept(listenfd,(struct sockaddr *) &amp;cliaddr,&amp;clilen); printf(\"fork 2 is [%d]，error is %m\\n\",connfd); &#125; &#125; sleep(100); return 1;&#125; 窗口1:编译运行例子 123[root@localhost demo]# gcc jq.c -o jq[root@localhost demo]# ./jqfork 1 is [4],error is Success 窗口2：访问9999端口1[root@localhost ~]# curl http://127.0.0.1:9999 结果：在创建、绑定和监听后，创建多进程监听后，只有一个进程被唤醒接收处理fd，其它进程均在休眠阶段，在linux内核3.10.0-514.16.1.el7.x86_64版本下，多进程accept连接时候不存在惊群现象。 select实例背景：3.10.0-514.16.1.el7.x86_64内核下，在创建、绑定和监听后，创建多进程select监听 建立好的fd，当一个客户端访问该服务端时候，多进程活动情况 jingqunselect.c1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 8888int main(int argc,char **argv)&#123; int listenfd,connfd; pid_t childpid,childpid2; socklen_t clilen; struct sockaddr_in cliaddr,servaddr; listenfd = socket(AF_INET,SOCK_STREAM,0); bzero(&amp;servaddr,sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl (INADDR_ANY); servaddr.sin_port = htons (SERV_PORT); bind(listenfd, (struct sockaddr *) &amp;servaddr, sizeof(servaddr)); listen(listenfd,1000); clilen = sizeof(cliaddr); if( (childpid = fork()) == 0) &#123; int maxsock = listenfd+1; fd_set fdsr; FD_ZERO(&amp;fdsr); FD_SET(listenfd, &amp;fdsr); struct timeval tv; tv.tv_sec = 30; tv.tv_usec = 0; int ret = select(maxsock, &amp;fdsr,NULL,NULL,&amp;tv); if(ret &lt; 0) &#123; printf(\"[%d]child err 1 \\n\",getpid()); &#125; else if(ret == 0) &#123; printf(\"time out\\n\"); &#125; else &#123; printf(\"[%d] rcv singal \\n\",getpid()); while(1) &#123; connfd = accept(listenfd,(struct sockaddr *) &amp;cliaddr,&amp;clilen); printf(\"[%d] [%d],error is %m\\n\",getpid(),connfd); &#125; &#125; &#125; if( (childpid2 = fork()) == 0) &#123; int maxsock = listenfd+1; fd_set fdsr; FD_ZERO(&amp;fdsr); FD_SET(listenfd, &amp;fdsr); struct timeval tv; tv.tv_sec = 30; tv.tv_usec = 0; int ret = select(maxsock, &amp;fdsr,NULL,NULL,&amp;tv); if(ret &lt; 0) &#123; printf(\"[%d]child 2 err\\n\",getpid()); &#125; else if(ret == 0) &#123; printf(\"time out\\n\"); &#125; else &#123; printf(\"[%d] rcv singal \\n\",getpid()); while(1) &#123; connfd = accept(listenfd,(struct sockaddr *) &amp;cliaddr,&amp;clilen); printf(\"[%d] [%d],error is %m\\n\",getpid(),connfd); &#125; &#125; &#125; sleep(100); return 1;&#125; 窗口1:编译运行上述代码123456[root@localhost demo]# gcc jingqunselect.c -o jqselect[root@localhost demo]#[root@localhost demo]# ./jqselect[23954] rcv singal[23955]rcv singal[23954] [4],error is Success 窗口2:[root@localhost ~]# curl http://127.0.0.1:8888结论：有连接建立信号时候两个监听进程均被唤醒，也就是存在惊群问题。 epool实例背景：3.10.0-514.16.1.el7.x86_64内核下，在创建、绑定和监听后，创建10个子进程epool监听 建立好的fd，当一个客户端访问该服务端时候，多进程活动情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149[root@localhost demo]# cat epjq.c#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;netdb.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;sys/wait.h&gt;#define PROCESS_NUM 10static intcreate_and_bind (char *port)&#123; int fd = socket(PF_INET, SOCK_STREAM, 0); struct sockaddr_in serveraddr; serveraddr.sin_family = AF_INET; serveraddr.sin_addr.s_addr = htonl(INADDR_ANY); serveraddr.sin_port = htons(atoi(port)); bind(fd, (struct sockaddr*)&amp;serveraddr, sizeof(serveraddr)); return fd;&#125; static intmake_socket_non_blocking (int sfd)&#123; int flags, s; flags = fcntl (sfd, F_GETFL, 0); if (flags == -1) &#123; perror (\"fcntl\"); return -1; &#125; flags |= O_NONBLOCK; s = fcntl (sfd, F_SETFL, flags); if (s == -1) &#123; perror (\"fcntl\"); return -1; &#125; return 0;&#125;#define MAXEVENTS 64intmain (int argc, char *argv[])&#123; int sfd, s; int efd; struct epoll_event event; struct epoll_event *events; sfd = create_and_bind(\"8888\"); if (sfd == -1) abort (); s = make_socket_non_blocking (sfd); if (s == -1) abort (); s = listen(sfd, SOMAXCONN); if (s == -1) &#123; perror (\"listen\"); abort (); &#125; efd = epoll_create(MAXEVENTS); if (efd == -1) &#123; perror(\"epoll_create\"); abort(); &#125; event.data.fd = sfd; //event.events = EPOLLIN | EPOLLET; event.events = EPOLLIN; s = epoll_ctl(efd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) &#123; perror(\"epoll_ctl\"); abort(); &#125; /* Buffer where events are returned */ events = calloc(MAXEVENTS, sizeof event); int k; for(k = 0; k &lt; PROCESS_NUM; k++) &#123; int pid = fork(); if(pid == 0) &#123; /* The event loop */ while (1) &#123; int n, i; n = epoll_wait(efd, events, MAXEVENTS, -1); printf(\"process %d return from epoll_wait!\\n\", getpid()); /* sleep here is very important!*/ sleep(2); for (i = 0; i &lt; n; i++) &#123; if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) &#123; /* An error has occured on this fd, or the socket is not ready for reading (why were we notified then?) */ fprintf (stderr, \"epoll error\\n\"); close (events[i].data.fd); continue; &#125; else if (sfd == events[i].data.fd) &#123; /* We have a notification on the listening socket, which means one or more incoming connections. */ struct sockaddr in_addr; socklen_t in_len; int infd; char hbuf[NI_MAXHOST], sbuf[NI_MAXSERV]; in_len = sizeof in_addr; infd = accept(sfd, &amp;in_addr, &amp;in_len); if (infd == -1) &#123; printf(\"process %d accept failed!\\n\", getpid()); break; &#125; printf(\"process %d accept successed!\\n\", getpid()); /* Make the incoming socket non-blocking and add it to the list of fds to monitor. */ close(infd); &#125; &#125; &#125; &#125; &#125; int status; wait(&amp;status); free (events); close (sfd); return EXIT_SUCCESS;&#125; 窗口1:编译运行epooldemo[root@localhost demo]# ./epjqprocess 24197 return from epoll_wait!process 24198 return from epoll_wait!process 24196 return from epoll_wait!process 24195 return from epoll_wait!process 24194 return from epoll_wait!process 24193 return from epoll_wait!process 24192 return from epoll_wait!process 24191 return from epoll_wait!process 24190 return from epoll_wait!process 24189 return from epoll_wait!process 24193 accept successed!process 24194 accept failed!process 24197 accept failed!process 24195 accept failed!process 24192 accept failed!process 24191 accept failed!process 24196 accept failed!process 24198 accept failed!process 24189 accept failed!process 24190 accept failed! 窗口2:[root@localhost ~]# curl http://127.0.0.1:8888 结论:epoll_wait监听事件时候没有解决惊群问题；所有监听进程均会被打扰惊醒，进行上下文切换后然后进入睡眠。 how to slove ?多进程需要从某一个端口获取连接，为了高性能我们摒弃直接accept而根据具体使用场景选用epool／poll／select等多socket管控机制，这种管控机制会带来惊群问题（具体内容上述实验已经表达）。目前标准解决方案有两种： 1.锁机制（见后文详细说明）2.复用端口 kernel 3.9增加了SO_REUSEPORT socket option，该选项允许服务端socket复用端口，通过hash机制将连接分配客户端到具体的进程；而这一切都是内核在处理。 实例背景：3.10.0-514.16.1.el7.x86_64内核，fork多个进程，通过设置SO_REUSEPORT标记，多进程一起监听端口8888，具体实验demo程序如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;netdb.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;sys/wait.h&gt;#define PROCESS_NUM 10static intcreate_and_bind (char *port)&#123; int fd = socket(PF_INET, SOCK_STREAM, 0); int optval = 1; setsockopt(fd,SOL_SOCKET,SO_REUSEPORT,&amp;optval,sizeof(optval)); struct sockaddr_in serveraddr; serveraddr.sin_family = AF_INET; serveraddr.sin_addr.s_addr = htonl(INADDR_ANY); serveraddr.sin_port = htons(atoi(port)); bind(fd, (struct sockaddr*)&amp;serveraddr, sizeof(serveraddr)); return fd;&#125; static intmake_socket_non_blocking (int sfd)&#123; int flags, s; flags = fcntl (sfd, F_GETFL, 0); if (flags == -1) &#123; perror (\"fcntl\"); return -1; &#125; flags |= O_NONBLOCK; s = fcntl (sfd, F_SETFL, flags); if (s == -1) &#123; perror (\"fcntl\"); return -1; &#125; return 0;&#125;#define MAXEVENTS 64static void socket_proc()&#123; int sfd, s; int efd; struct epoll_event event; struct epoll_event *events; sfd = create_and_bind(\"8888\"); if (sfd == -1) abort (); s = make_socket_non_blocking (sfd); if (s == -1) abort (); s = listen(sfd, SOMAXCONN); if (s == -1) &#123; perror (\"listen\"); abort (); &#125; efd = epoll_create(MAXEVENTS); if (efd == -1) &#123; perror(\"epoll_create\"); abort(); &#125; event.data.fd = sfd; //event.events = EPOLLIN | EPOLLET; event.events = EPOLLIN; s = epoll_ctl(efd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) &#123; perror(\"epoll_ctl\"); abort(); &#125; /* Buffer where events are returned */ events = calloc(MAXEVENTS, sizeof event); /* The event loop */ while (1) &#123; int n, i; n = epoll_wait(efd, events, MAXEVENTS, -1); printf(\"process %d return from epoll_wait!\\n\", getpid()); /* sleep here is very important!*/ // sleep(2); for (i = 0; i &lt; n; i++) &#123; if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) &#123; /* An error has occured on this fd, or the socket is not ready for reading (why were we notified then?) */ fprintf (stderr, \"epoll error\\n\"); close (events[i].data.fd); continue; &#125; else if (sfd == events[i].data.fd) &#123; /* We have a notification on the listening socket, which means one or more incoming connections. */ struct sockaddr in_addr; socklen_t in_len; int infd; char hbuf[NI_MAXHOST], sbuf[NI_MAXSERV]; in_len = sizeof in_addr; infd = accept(sfd, &amp;in_addr, &amp;in_len); if (infd == -1) &#123; printf(\"process %d accept failed!\\n\", getpid()); break; &#125; printf(\"process %d accept successed!\\n\", getpid()); /* Make the incoming socket non-blocking and add it to the list of fds to monitor. */ close(infd); &#125; &#125; &#125; free (events); close (sfd);&#125;intmain (int argc, char *argv[])&#123; int k; for(k = 0; k &lt; PROCESS_NUM; k++) &#123; int pid = fork(); if(pid == 0) &#123; socket_proc(); &#125; &#125; int status; wait(&amp;status); return EXIT_SUCCESS;&#125; 编译运行：1234567[root@localhost demo]#[root@localhost demo]# gcc epjqreuseport.c -o e.out[root@localhost demo]# ./e.outprocess 31071 return from epoll_wait!process 31071 accept successed!process 31075 return from epoll_wait!process 31075 accept successed! 总结：SO_REUSEPORT允许多进程共同bind同一个端口，内核会按照一定机制分配访问连接到不同的进程 nginx采用epool模型，怎么解决惊群？ 如果进程并没有处于过载状态，那么就会去争用锁，当然，实际上是争用监听套接口的监控权. 争锁成功就会把所有监听套接口加入到自身的事件监控机制里（如果原本不在） 争锁失败就会把监听套接口从自身的事件监控机制里删除（如果原本在） 争抢成功的进程，可以调用epoll_ctl把所有要监听的端口加入该进程的epool事件中，然后epool_wait阻塞及时获取客户端的新建tcp事件，如果获取到相应事件，该进程调用accept正式建立建立连接；然后释放锁。当锁被释放后所有进程可以共同争抢锁了。 也就是说，因为锁的原因，同一时间只能有一个进程拥有监听端口的监控权利（将监听端口放入自己epool中中控制并且调用epoolwait监控新建事件）。这种机制保障了不会有多进程共同拥有套接口的监控权，从而避免了惊群问题。","categories":[{"name":"linux","slug":"linux","permalink":"http://vcpu.me/categories/linux/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"nginx","slug":"nginx","permalink":"http://vcpu.me/tags/nginx/"},{"name":"惊群","slug":"惊群","permalink":"http://vcpu.me/tags/惊群/"}]},{"title":"epool基本用法","slug":"epool","date":"2017-06-28T04:00:00.000Z","updated":"2017-07-04T03:23:55.000Z","comments":true,"path":"epool/","link":"","permalink":"http://vcpu.me/epool/","excerpt":"epool基本概念是什么？改进的pool，一种I/O多路复用技术，可管理大批量文件描述符。 工作原理？ 内核中，一切皆文件，epoll向内核注册一个文件系统，用于存储要管理的文件描述符号。调用epoll_create时，会在虚拟文件系统中创建一个file节点服务epool同时也会创建就绪事件list链表。操作系统启动后，会开辟出自己的高速cache，socket问价描述符会以红黑树存入cache，方便查找、插入、删除。 epool_ctl，把socket放到epool文件系统里file对应的红黑树，也会注册一个回调函数，文件描述符有信号后，会调用该组册函数，内核把网卡数据copy到内核中把socket插入就绪列表中。 epoll_wait调用时候，看一眼就绪列表，所以效率很高。监控百万描述符，但是准备就绪fd却很少。","text":"epool基本概念是什么？改进的pool，一种I/O多路复用技术，可管理大批量文件描述符。 工作原理？ 内核中，一切皆文件，epoll向内核注册一个文件系统，用于存储要管理的文件描述符号。调用epoll_create时，会在虚拟文件系统中创建一个file节点服务epool同时也会创建就绪事件list链表。操作系统启动后，会开辟出自己的高速cache，socket问价描述符会以红黑树存入cache，方便查找、插入、删除。 epool_ctl，把socket放到epool文件系统里file对应的红黑树，也会注册一个回调函数，文件描述符有信号后，会调用该组册函数，内核把网卡数据copy到内核中把socket插入就绪列表中。 epoll_wait调用时候，看一眼就绪列表，所以效率很高。监控百万描述符，但是准备就绪fd却很少。 适用场景？非常适用大量并发连接中只有少量活跃连接情况，且在该情况下CPU适用率很低。 可能缺点？所有socket基本都是活跃的，比如在一个高速的LAN环境，使用epool可能会比select／pool效率低 分为LT和ETLT和ET作用在epool_wait过程中，LT模式下，只要一个文件描述符没有处理完，后续再次调用epool_wait时也会返回。实现过程为，内核会把socket事件插入就绪链表，epool_wait调用会被把就绪的文件描述符拷入用户态，清空就绪链表，如果是ET则额外检测如果存在没有处理文件描述符，则将再次放入就绪列表中。 epool例子epoll_create函数用途：创建一个epool事件管理并返回描述符号12#include &lt;sys/epoll.h&gt;int epoll_create(int size); 参数：size 最大fd数返回值：epool使用的文件描述符 -1 失败 >= 0 成功 epoll_ctl函数用途：控制epoll事件，添加修改删除事件12#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 参数： epfd:epoll_create的返回值op:要进行的操作例如注册事件，可能的取值EPOLL_CTL_ADD 注册、EPOLL_CTL_MOD 修 改、EPOLL_CTL_DEL 删除fd:要在epool事件管理上加入删除或者修改的文件描述符event:event.data.fd 要处理的文件描述符event:event.events = EPOLLIN|EPOLLET;EPOLLIN ：表示对应的文件描述符可以读；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET：边缘触发； 返回值：成功返回0，失败返回-1 When successful, epoll_ctl() returns zero. When an error occurs, epoll_ctl() returns -1 and errno is set appropriately epool_wait函数用途：返回IO事件就绪的fd12int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 参数：epfd：epoll_create的返回值events：取出内核结果的事件数组maxevents：要处理的事件数timeout：等待IO发生超时值 -1 阻塞直到有事件 0 非阻塞 &gt;0： 阻塞时间，单位毫秒 epoool函数实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt; /* basic system data types */#include &lt;sys/socket.h&gt; /* basic socket definitions */#include &lt;netinet/in.h&gt; /* sockaddr_in&#123;&#125; and other Internet defns */#include &lt;arpa/inet.h&gt; /* inet(3) functions */#include &lt;sys/epoll.h&gt; /* epoll function */#include &lt;fcntl.h&gt; /* nonblocking */#include &lt;sys/resource.h&gt; /*setrlimit */#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define MAXEPOLLSIZE 10000#define MAXLINE 10240int handle(int connfd);int setnonblocking(int sockfd)&#123; if (fcntl(sockfd, F_SETFL, fcntl(sockfd, F_GETFD, 0)|O_NONBLOCK) == -1) &#123; return -1; &#125; return 0;&#125;int main(int argc, char **argv)&#123; int servPort = 8080; int listenq = 1024; int listenfd, connfd, kdpfd, nfds, n, nread, curfds,acceptCount = 0; struct sockaddr_in servaddr, cliaddr; socklen_t socklen = sizeof(struct sockaddr_in); struct epoll_event ev; struct epoll_event events[MAXEPOLLSIZE]; struct rlimit rt; char buf[MAXLINE]; /* 设置每个进程允许打开的最大文件数 */ rt.rlim_max = rt.rlim_cur = MAXEPOLLSIZE; if (setrlimit(RLIMIT_NOFILE, &amp;rt) == -1) &#123; perror(\"setrlimit error\"); return -1; &#125; bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl (INADDR_ANY); servaddr.sin_port = htons (servPort); listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) &#123; perror(\"can't create socket file\"); return -1; &#125; int opt = 1; setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;opt, sizeof(opt)); if (setnonblocking(listenfd) &lt; 0) &#123; perror(\"setnonblock error\"); &#125; if (bind(listenfd, (struct sockaddr *) &amp;servaddr, sizeof(struct sockaddr)) == -1) &#123; perror(\"bind error\"); return -1; &#125; if (listen(listenfd, listenq) == -1) &#123; perror(\"listen error\"); return -1; &#125; /* 创建 epoll 句柄，把监听 socket 加入到 epoll 集合里 */ kdpfd = epoll_create(MAXEPOLLSIZE); ev.events = EPOLLIN | EPOLLET; ev.data.fd = listenfd; if (epoll_ctl(kdpfd, EPOLL_CTL_ADD, listenfd, &amp;ev) &lt; 0) &#123; fprintf(stderr, \"epoll set insertion error: fd=%d\\n\", listenfd); return -1; &#125; curfds = 1; printf(\"epollserver startup,port %d, max connection is %d, backlog is %d\\n\", servPort, MAXEPOLLSIZE, listenq); for (;;) &#123; /* 等待有事件发生 */ nfds = epoll_wait(kdpfd, events, curfds, -1); if (nfds == -1) &#123; perror(\"epoll_wait\"); continue; &#125; /* 处理所有事件 */ for (n = 0; n &lt; nfds; ++n) &#123; if (events[n].data.fd == listenfd) &#123; connfd = accept(listenfd, (struct sockaddr *)&amp;cliaddr,&amp;socklen); if (connfd &lt; 0) &#123; perror(\"accept error\"); continue; &#125; sprintf(buf, \"accept form %s:%d\\n\", inet_ntoa(cliaddr.sin_addr), cliaddr.sin_port); printf(\"%d:%s\", ++acceptCount, buf); if (curfds &gt;= MAXEPOLLSIZE) &#123; fprintf(stderr, \"too many connection, more than %d\\n\", MAXEPOLLSIZE); close(connfd); continue; &#125; if (setnonblocking(connfd) &lt; 0) &#123; perror(\"setnonblocking error\"); &#125; ev.events = EPOLLIN | EPOLLET; ev.data.fd = connfd; if (epoll_ctl(kdpfd, EPOLL_CTL_ADD, connfd, &amp;ev) &lt; 0) &#123; fprintf(stderr, \"add socket '%d' to epoll failed: %s\\n\", connfd, strerror(errno)); return -1; &#125; curfds++; continue; &#125; // 处理客户端请求 if (handle(events[n].data.fd) &lt; 0) &#123; epoll_ctl(kdpfd, EPOLL_CTL_DEL, events[n].data.fd,&amp;ev); curfds--; &#125; &#125; &#125; close(listenfd); return 0;&#125;int handle(int connfd) &#123; int nread; char buf[MAXLINE]; nread = read(connfd, buf, MAXLINE);//读取客户端socket流 if (nread == 0) &#123; printf(\"client close the connection\\n\"); close(connfd); return -1; &#125; if (nread &lt; 0) &#123; perror(\"read error\"); close(connfd); return -1; &#125; printf(\"recv:%s\\n\",buf); write(connfd, buf, nread);//响应客户端 printf(\"send:%s\\n\",buf); return 0;&#125;","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"epoll","slug":"epoll","permalink":"http://vcpu.me/tags/epoll/"}]},{"title":"socket信号处理","slug":"socket信号处理","date":"2017-06-26T11:10:34.000Z","updated":"2017-06-26T09:01:27.000Z","comments":true,"path":"socket信号处理/","link":"","permalink":"http://vcpu.me/socket信号处理/","excerpt":"socket I/O事件处理以TCP socket为例kernel: 3.10.0-514.16.1.el7.x86_64 socket IO处理函数1234567891011struct sock &#123; ... struct socket_wq __rcu *sk_wq; /*等待队列和异步队列*/ ... void (*sk_state_change)(struct sock *sk); void (*sk_data_ready)(struct sock *sk, int bytes); void (*sk_write_space)(struct sock *sk); void (*sk_error_report)(struct sock *sk); int (*sk_backlog_rcv)(struct sock *sk); ...&#125;; sk_wq 含有等待队列用来睡眠唤醒程序使用，异步队列异步socket使用 sk_state_change 从SYN_SEND或者SYN_RECV到ES状态，从ES到CLOSE_WAIT状态，当协议栈遇到这些事件时候会调用 sk_data_ready sk_write_space sock有数据可读和可写时候调用 sk_error_report sock上存在错误时调用，比如收到RST包","text":"socket I/O事件处理以TCP socket为例kernel: 3.10.0-514.16.1.el7.x86_64 socket IO处理函数1234567891011struct sock &#123; ... struct socket_wq __rcu *sk_wq; /*等待队列和异步队列*/ ... void (*sk_state_change)(struct sock *sk); void (*sk_data_ready)(struct sock *sk, int bytes); void (*sk_write_space)(struct sock *sk); void (*sk_error_report)(struct sock *sk); int (*sk_backlog_rcv)(struct sock *sk); ...&#125;; sk_wq 含有等待队列用来睡眠唤醒程序使用，异步队列异步socket使用 sk_state_change 从SYN_SEND或者SYN_RECV到ES状态，从ES到CLOSE_WAIT状态，当协议栈遇到这些事件时候会调用 sk_data_ready sk_write_space sock有数据可读和可写时候调用 sk_error_report sock上存在错误时调用，比如收到RST包 处理函数初始化1234567----------------START------------------------- 0xffffffff81557ed0 : sock_init_data+0x0/0x220 [kernel] 0xffffffff815ec9f4 : inet_create+0x154/0x360 [kernel] 0xffffffff81555200 : __sock_create+0x110/0x260 [kernel] 0xffffffff81556521 : SyS_socket+0x61/0xf0 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel]----------------END------------------------- 步骤1:通用初始化socket-&gt;SyS_socket-&gt;__sock_create-&gt;inet_create-&gt;sock_init_data 1234567sock_init_data：sk-&gt;sk_state_change = sock_def_wakeup;sk-&gt;sk_data_ready = sock_def_readable;sk-&gt;sk_write_space = sock_def_write_space;sk-&gt;sk_error_report = sock_def_error_report;sk-&gt;sk_destruct = sock_def_destruct; 步骤2:对于TCP socket，特有更新1234560xffffffff815be170 : tcp_init_sock+0x0/0x200 [kernel]0xffffffff815d4212 : tcp_v4_init_sock+0x12/0x30 [kernel]0xffffffff815eca71 : inet_create+0x1d1/0x360 [kernel]0xffffffff81555200 : __sock_create+0x110/0x260 [kernel]0xffffffff81556521 : SyS_socket+0x61/0xf0 [kernel]0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel] inet_create-&gt;tcp_v4_init_sock-&gt;tcp_init_sock12tcp_init_sock:sk-&gt;sk_write_space = sk_stream_write_space; sock_def_wakeup信号触发时机 无论是作为客户端还是服务端socket TCP协议栈进入到ES或者CLOSE_WAIT时候，会触发sock_def_wakeup通知用户态进程TCP状态变更具体来讲：sock_def_wakeup可以唤醒connect或者accept，或者因收到结束喜欢fin而正常结束send/recv返回值为0 作为客户端主动连接对方获取资源访问方式：curl -v http://180.97.33.107 123456789101112131415161718192021[root@localhost socketdemo]# curl -v http://180.97.33.107* About to connect() to 180.97.33.107 port 80 (#0)* Trying 180.97.33.107...* Connected to 180.97.33.107 (180.97.33.107) port 80 (#0)&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: 180.97.33.107&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Server: bfe/1.0.8.18&lt; Date: Fri, 23 Jun 2017 10:02:40 GMT&lt; Content-Type: text/html&lt; Content-Length: 2381&lt; Last-Modified: Mon, 23 Jan 2017 13:28:20 GMT&lt; Connection: Keep-Alive&lt; ETag: \"588604f4-94d\"&lt; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform&lt; Pragma: no-cache&lt; Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/&lt; Accept-Ranges: bytes systemtap探测sock_def_wakeup被调用情况，并打出调用栈，如下： 1234567891011121314151617181920212223242526272829303132333435调用情况：收到对端回复synack后，发出ack时候客户端connect结束，从SYN_SEND跳转到ES状态唤醒用户态进程，此时连接已经成功，可以发送数据了sock_def_wakeup:[2017/6/23,18:00:59]local=10.0.2.15:60162,remote=180.97.33.107:80 state:ESTABLISHED 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815cbc09 : tcp_finish_connect+0xc9/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000]WARNING: Missing unwind data for a module, rerun with 'stap -d e1000'情况2:访问的服务端主动关闭连接，则客户端从ES进入CLOSE_WAIT,通知用户态进程sock_def_wakeup:[2017/6/23,18:00:59]local=10.0.2.15:60162,remote=180.97.33.107:80 state:CLOSE_WAIT 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815c5ca9 : tcp_fin+0x169/0x1e0 [kernel] 0xffffffff815c84f8 : tcp_data_queue+0x7f8/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000] 作为服务端结论作为服务端堵塞在accept时，收到客户端请求，三次握手建立完成后，服务端状态进入ES状态，会调用sock_def_wakeup通知用户态进程123456789101112131415sock_def_wakeup:[2017/6/26,10:47:00]local=192.168.55.178:8080,remote=192.168.55.165:50536 state:ESTABLISHED 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815cc3bf : tcp_rcv_state_process+0x75f/0xf20 [kernel] 0xffffffff815d7dde : tcp_child_process+0x3e/0x130 [kernel] 0xffffffff815d60d5 : tcp_v4_do_rcv+0x255/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000] 服务端测试程序如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768 //#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;//#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;fcntl.h&gt;#define SERVPORT 8080#define BACKLOG 10#define MAX_CONNECTED_NO 10#define MAXDATASIZE 100int main()&#123; struct sockaddr_in server_sockaddr,client_sockaddr; int sin_size,recvbytes,flags; int sockfd,client_fd; char buf[MAXDATASIZE]; if((sockfd = socket(AF_INET,SOCK_STREAM,0))==-1)&#123; perror(\"socket\"); return 0; &#125; printf(\"socket success!,sockfd=%d\\n\",sockfd); server_sockaddr.sin_family=AF_INET; server_sockaddr.sin_port=htons(SERVPORT); server_sockaddr.sin_addr.s_addr=INADDR_ANY; bzero(&amp;(server_sockaddr.sin_zero),8); if(bind(sockfd,(struct sockaddr *)&amp;server_sockaddr,sizeof(struct sockaddr))==-1)&#123; perror(\"bind\"); return 0; &#125; printf(\"bind success!\\n\"); if(listen(sockfd,BACKLOG)==-1)&#123; perror(\"listen\"); return 0; &#125; printf(\"listening....\\n\"); if((flags=fcntl( sockfd, F_SETFL, 0))&lt;0) perror(\"fcntl F_SETFL\"); flags |= O_ASYNC; if(fcntl( sockfd, F_SETFL,flags)&lt;0) perror(\"fcntl\"); while(1)&#123; sin_size=sizeof(struct sockaddr_in); if((client_fd=accept(sockfd,(struct sockaddr*)&amp;client_sockaddr,&amp;sin_size))==-1)&#123; perror(\"accept\"); return 0; &#125; printf(\"%d\\n\",client_sockaddr.sin_port); if((recvbytes=recv(client_fd,buf,MAXDATASIZE,0))==-1)&#123; perror(\"recv\"); return 0; &#125; printf(\"recvbytes: %d %s \\n \",recvbytes,buf); close(client_fd); return; &#125;&#125; 运行过程：123456[root@localhost socketdemo]# gcc server.c -o server[root@localhost socketdemo]#[root@localhost socketdemo]# ./serversocket success!,sockfd=3bind success!listening.... 服务端stap探测脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123[root@localhost stp]# cat socketsingtal.stp%&#123; #include &lt;linux/tcp.h&gt; #include&lt;linux/rtc.h&gt; #include &lt;net/tcp.h&gt; static const char tcp_state_array[][16] = &#123; \"NULL\", \"ESTABLISHED\", \"SYN_SENT\", \"SYN_RECV\", \"FIN_WAIT1\", \"FIN_WAIT2\", \"TIME_WAIT\", \"CLOSE\", \"CLOSE_WAIT\", \"LAST_ACK\", \"LISTEN\", \"CLOSING\" &#125;;%&#125;function get_short_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%02d:%02d:%02d\", tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_full_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%d/%d/%d,%02d:%02d:%02d\", tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday, tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_conn_lifetime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = jiffies_to_msecs(tcp_time_stamp - info-&gt;estab_t);%&#125;function get_conn_data:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct tcp_sock *tp = tcp_sk(sk); struct stap_info *info = sk-&gt;sk_protinfo; u32 len = tp-&gt;snd_nxt - info-&gt;isn; STAP_RETVALUE = len ? len - 1 : len;%&#125;function filter_http_transtime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = info-&gt;http_filter;%&#125;function get_socket_addr:string (sk:long)&#123; laddr = tcpmib_local_addr(sk) lport = tcpmib_local_port(sk) raddr = tcpmib_remote_addr(sk) rport = tcpmib_remote_port(sk) local_addr = sprintf(\"%s:%d\", ip_ntop(htonl(laddr)), lport) remote_addr = sprintf(\"%s:%d\", ip_ntop(htonl(raddr)), rport) return sprintf(\"local=%s,remote=%s\", local_addr, remote_addr)&#125;function get_socket_state:string (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[sk-&gt;sk_state]);%&#125;function get_socket_sk_sndbuf:string(sk:long)%&#123; struct sock *sk=(struct sock*)STAP_ARG_sk; sprintf(STAP_RETVALUE,\"%d:%d\", sk-&gt;sk_wmem_queued, sk-&gt;sk_sndbuf);%&#125;function socket_state_num2str:string (state:long)%&#123; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[STAP_ARG_state]);%&#125;function sshfilter:long(sk:long)&#123; lport = tcpmib_local_port(sk) if(lport == 22) return 1 return 0&#125;probe kernel.function(\"sock_def_wakeup\").call&#123; if(sshfilter($sk)) next printf(\"sock_def_wakeup:[%s]%s state:%s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk)) print_backtrace()&#125; 运行过程：1[root@localhost stp]# stap -g socketsingtal.stp sock_def_wakeup状态改变事件实现分析12345678910111213141516171819//判断等待队列释放存在进程static inline bool wq_has_sleeper(struct socket_wq *wq)&#123; //同步使用，具体实现未分析 smp_mb(); return wq &amp;&amp; waitqueue_active(&amp;wq-&gt;wait);&#125;static void sock_def_wakeup(struct sock *sk)&#123; struct socket_wq *wq; rcu_read_lock(); wq = rcu_dereference(sk-&gt;sk_wq); //如果等待队列有进程，全部唤醒 if (wq_has_sleeper(wq)) wake_up_interruptible_all(&amp;wq-&gt;wait); rcu_read_unlock();&#125; 唤醒进程实现如下wake_up_interruptible_all -&gt;wake_up-&gt;wake_up_common比较特殊点是，__wake_up的nr_exclusive为0时候唤醒所有进程。其它说明是：nr_exclusive为1时候，是为了惊鸿设置的，只唤醒一个进程123456789101112#define wake_up_interruptible_all(x) __wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, void *key)&#123; unsigned long flags; spin_lock_irqsave(&amp;q-&gt;lock, flags); __wake_up_common(q, mode, nr_exclusive, 0, key); spin_unlock_irqrestore(&amp;q-&gt;lock, flags);&#125;EXPORT_SYMBOL(__wake_up); __wake_up_common 参数nr_exclusive为0时候，break不可能被执行12345678910111213static void __wake_up_common(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, int wake_flags, void *key)&#123; wait_queue_t *curr, *next; list_for_each_entry_safe(curr, next, &amp;q-&gt;task_list, task_list) &#123; unsigned flags = curr-&gt;flags; if (curr-&gt;func(curr, mode, wake_flags, key) &amp;&amp; (flags &amp; WQ_FLAG_EXCLUSIVE) &amp;&amp; !--nr_exclusive) break; &#125;&#125; __wake_up_common 中curr-&gt;func是什么呢？ 是autoremove_wake_function，将socket睡眠时候，会调用DEFINE_WAIT将autoremove_wake_function设置12345678#define DEFINE_WAIT_FUNC(name, function)\\ wait_queue_t name = &#123; \\ .private = current, \\ .func = function, \\ .task_list = LIST_HEAD_INIT((name).task_list),x\\ &#125;#define DEFINE_WAIT(name) DEFINE_WAIT_FUNC(name, autoremove_wake_function) autoremove_wake_function 干了什么？1:default_wake_function -&gt;try_to_wake_up 把进程状态设置为TASK_RUNNING，并把其插入CPU运行队列，从而唤醒睡眠进程2:待进程状态唤醒后，把等待事件从等待队列中删除 1234567891011121314151617int autoremove_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)&#123; int ret = default_wake_function(wait, mode, sync, key); if (ret) list_del_init(&amp;wait-&gt;task_list);//等待队列删除 return ret;&#125;EXPORT_SYMBOL(autoremove_wake_function);int default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags, void *key)&#123; //把进程状态设置为TASK_RUNNING，并把其插入CPU运行队列，从而唤醒睡眠进程 return try_to_wake_up(curr-&gt;private, mode, wake_flags);&#125;EXPORT_SYMBOL(default_wake_function); sock_def_readablesock_def_readable调用时机，sock数据可读会调用此函数唤醒进程 作为服务端123456789101112131415161718192021222324252627282930313233//收到syn包后sock_def_wakeup:[2017/6/26,11:52:07]local=0.0.0.0:8080,remote=0.0.0.0:0 state:LISTEN 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815d7eb8 : tcp_child_process+0x118/0x130 [kernel] 0xffffffff815d60d5 : tcp_v4_do_rcv+0x255/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000]//收到数据包后sock_def_wakeup:[2017/6/26,11:52:07]local=192.168.55.178:8080,remote=192.168.55.165:50843 state:ESTABLISHED 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815c8197 : tcp_data_queue+0x497/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000][root@localhost stp]# 作为客户端 收到ES状态服务端回复的数据在tcp_data_queue／tcp_rcv_established中调用 具体实现后续分析。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960sock_def_wakeup:[2017/6/26,13:44:35]local=10.0.2.15:43188,remote=180.97.33.107:80 state:ESTABLISHED 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815cb6c3 : tcp_rcv_established+0x433/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000]sock_def_wakeup:[2017/6/26,13:44:35]local=10.0.2.15:43188,remote=180.97.33.107:80 state:ESTABLISHED 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815c8197 : tcp_data_queue+0x497/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000]sock_def_wakeup:[2017/6/26,13:44:35]local=10.0.2.15:43188,remote=180.97.33.107:80 state:ESTABLISHED 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815cb6c3 : tcp_rcv_established+0x433/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000] 服务端主动关闭连接，作为客户端TCP状态机处于CLOSE_WAITsock_def_wakeup:[2017/6/26,13:44:35]local=10.0.2.15:43188,remote=180.97.33.107:80 state:CLOSE_WAIT 0xffffffff81558220 : sock_def_readable+0x0/0x70 [kernel] 0xffffffff815c8197 : tcp_data_queue+0x497/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00a72fc [e1000] sock_def_readable123456789101112131415161718192021222324252627282930313233343536static void sock_def_readable(struct sock *sk, int len)&#123; struct socket_wq *wq; rcu_read_lock(); wq = rcu_dereference(sk-&gt;sk_wq); if (wq_has_sleeper(wq)) //阻塞队列通知 wake_up_interruptible_sync_poll(&amp;wq-&gt;wait, POLLIN | POLLPRI | POLLRDNORM | POLLRDBAND); //异步队列通知 sk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN); rcu_read_unlock();&#125;#define wake_up_interruptible_sync_poll(x, m)\\ __wake_up_sync_key((x), TASK_INTERRUPTIBLE, 1, (void *) (m)) void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, void *key)&#123; unsigned long flags; int wake_flags = WF_SYNC; if (unlikely(!q)) return; if (unlikely(!nr_exclusive)) wake_flags = 0; spin_lock_irqsave(&amp;q-&gt;lock, flags); __wake_up_common(q, mode, nr_exclusive, wake_flags, key); spin_unlock_irqrestore(&amp;q-&gt;lock, flags);&#125;EXPORT_SYMBOL_GPL(__wake_up_sync_key); wake_up_interruptible_sync_poll传递的nr_exclusive为1，表示只允许唤醒一个等待进程。 sk_stream_write_spacesk-&gt;sk_write_space的实例为sock_def_write_space()。如果socket是SOCK_STREAM类型的，那么函数指针的值会更新为sk_stream_write_space()。sk_stream_write_space()在TCP中的调用路径为：tcp_rcv_established / tcp_rcv_state_process tcp_data_snd_check tcp_check_space tcp_new_space 12345678910111213141516171819202122232425void sk_stream_write_space(struct sock *sk)&#123; struct socket *sock = sk-&gt;sk_socket; struct socket_wq *wq; //发送缓存大小，当要发送数据没有到达发送缓存的2／3 下限 //尚未发送数据缓冲区大小数据，没有大于用户设定值sysctl_tcp_notsent_lowat时候才能触发写数据 //针对于下限，要发送数据还不多 //针对于上限，要发送数据已经很多的话，不变要再发了，否则会使用过多内存 if (sk_stream_is_writeable(sk) &amp;&amp; sock) &#123; //经过sk_stream_is_writeable判别说明缓冲是足够的SOCK_NOSPACE标记清除 clear_bit(SOCK_NOSPACE, &amp;sock-&gt;flags); rcu_read_lock(); wq = rcu_dereference(sk-&gt;sk_wq); if (wq_has_sleeper(wq)) //唤醒等待队列一个进程 wake_up_interruptible_poll(&amp;wq-&gt;wait, POLLOUT | POLLWRNORM | POLLWRBAND); //异步队列允许发送数据，通知异步队列 if (wq &amp;&amp; wq-&gt;fasync_list &amp;&amp; !(sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) sock_wake_async(sock, SOCK_WAKE_SPACE, POLL_OUT); rcu_read_unlock(); &#125;&#125;","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"socket信号处理","slug":"socket信号处理","permalink":"http://vcpu.me/tags/socket信号处理/"}]},{"title":"socket读写条件","slug":"socket_rw_proc","date":"2017-06-26T11:10:34.000Z","updated":"2017-06-26T08:55:50.000Z","comments":true,"path":"socket_rw_proc/","link":"","permalink":"http://vcpu.me/socket_rw_proc/","excerpt":"","text":"概念说明1.接收缓存区低水位标记（用于读）和发送缓存区低水位标记（用于写）：每个套接字有一个接收低水位和一个发送低水位。他们由select函数使用。接收低水位标记是让select返回”可读”时套接字接收缓冲区中所需的数据量。对于TCP,其默认值为1。发送低水位标记是让select返回”可写”时套接字发送缓冲区中所需的可用空间。对于TCP，其默认值常为2048 通俗的解释一下，缓存区我们当成一个大小为 n bytes的空间，那么：接收区缓存的作用就是，接收对面的数据放在缓存区，供应用程序读。当然了，只有当缓存区可读的数据量(接收低水位标记)到达一定程度（eg:1）的时候，我们才能读到数据，不然不就读不到数据了吗。发送区缓存的作用就是，发送应用程序的数据到缓存区，然后一起发给对面。当然了，只有当缓存区剩余一定空间(发送低水位标记)（eg:2048）,你才能写数据进去，不然可能导致空间不够。 2.FIN: (结束标志,Finish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据. 特别纠正说明： 经过测试在3.10.0-514.16.1.el7.x86_64 内核情况下SO_SNDLOWAT/SO_RCVLOWAT默认发送和接收最低水位均为1，也就是说默认情况下低水位值可以不考虑，系统缓冲区有数据就读有空闲就写 1234567int value =0;int v_len = sizeof(value);getsockopt(sockfd, SOL_SOCKET, SO_SNDLOWAT, (void*)&amp;value, &amp;v_len);printf(\"snd low at value: %d\\n\",value);value = 0;getsockopt(sockfd, SOL_SOCKET, SO_RCVLOWAT, (void*)&amp;value, &amp;v_len);printf(\"rcv low at value: %d\\n\",value); socket可读的条件下列四个条件中的任何一个满足时,socket准备好读:1.socket的接收缓冲区中的数据字节大于等于该socket的接收缓冲区低水位标记的当前大小。对这样的socket的读操作将不阻塞并返回一个大于0的值 {也就是返回准备好读入的数据}。我们可以用SO_RCVLOWATsocket选项来设置该socket的低水位标记。对于TCP和UDP .socket而言，其缺省值为1 2.该连接的读这一半关闭{也就是接收了FIN的TCP连接}。对这样的socket的读操作将不阻塞并返回0 3.socket是一个用于监听的socket,并且已经完成的连接数为非0.这样的soocket处于可读状态,是因为socket收到了对方的connect请求,执行了三次握手的第一步:对方发送SYN请求过来,使监听socket处于可读状态;正常情况下,这样的socket上的accept操作不会阻塞; 4.有一个socket有异常错误条件待处理。对于这样的socket的读操作将不会阻塞,并且返回一个错误-1,errno则设置成明确的错误条件。这些待处理的错误也可通过指定socket选项SO_ERROR调用getsockopt来取得并清除 socket可写的条件1.socket发送缓冲区中数据字节大于等于该socket发送缓冲区低水位大小。写操作不会被阻塞，会直接返回写入的数据大小，SO_SNDLOWAT socket选项设置socket可写低水位大小。经过测试默认大小为1。2.如果你已经关闭连接，或者主动fin半关闭。这种情况下socket再些将要产生SIGPIPE信号，你没有对这个信号处理的话，你的进程将会被关闭。3.有socket异常错误待处理，这种情况下写操作并不会被阻塞会直接返回一个错误-1，想知道错误原因erron可以帮助你。 参考地址http://blog.csdn.net/szcarewell/article/details/51227540","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"}]},{"title":"select","slug":"select","date":"2017-06-26T09:00:57.000Z","updated":"2017-06-26T09:00:57.000Z","comments":true,"path":"select/","link":"","permalink":"http://vcpu.me/select/","excerpt":"1234567891011#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);void FD_CLR(int fd, fd_set *set);int FD_ISSET(int fd, fd_set *set);void FD_SET(int fd, fd_set *set);void FD_ZERO(fd_set *set); nfds 是最大文件描述符号 +1 怎么可能这么简单，它限制的是最大值而不是个数 readfds 用来记录可读fd集合 writefds 用来记录可写fd集合 exceptfds 用来检查带外数据 timeout 决定select等待I/O时间 1.timeout该值为NULL，会阻塞一定等到监控的文件描述符集合中产生状态变化（可读，可写等）2.timeout值为0分0毫秒，非阻塞，不关注文件描述符是否变化立刻返回3.timeout正常值，timeout这段时间内阻塞，如果监控集合中有信号来临，select将返回，否则超时返回","text":"1234567891011#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);void FD_CLR(int fd, fd_set *set);int FD_ISSET(int fd, fd_set *set);void FD_SET(int fd, fd_set *set);void FD_ZERO(fd_set *set); nfds 是最大文件描述符号 +1 怎么可能这么简单，它限制的是最大值而不是个数 readfds 用来记录可读fd集合 writefds 用来记录可写fd集合 exceptfds 用来检查带外数据 timeout 决定select等待I/O时间 1.timeout该值为NULL，会阻塞一定等到监控的文件描述符集合中产生状态变化（可读，可写等）2.timeout值为0分0毫秒，非阻塞，不关注文件描述符是否变化立刻返回3.timeout正常值，timeout这段时间内阻塞，如果监控集合中有信号来临，select将返回，否则超时返回 返回值： 0 出发信号的fd数目 =0 超时 -1 出错 作用： 用来管理fd集合，实现多fd集合监听操作 select用户态用法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#define MYPORT 1234 // the port users will be connecting to#define BACKLOG 5 // how many pending connections queue will hold#define BUF_SIZE 200int fd_A[BACKLOG]; // accepted connection fdint conn_amount; // current connection amountvoid showclient()&#123; int i; printf(\"client amount: %d\\n\", conn_amount); for (i = 0; i &lt; BACKLOG; i++) &#123; printf(\"[%d]:%d \", i, fd_A[i]); &#125; printf(\"\\n\\n\");&#125;int main(void)&#123; int sock_fd, new_fd; // listen on sock_fd, new connection on new_fd struct sockaddr_in server_addr; // server address information struct sockaddr_in client_addr; // connector's address information socklen_t sin_size; int yes = 1; char buf[BUF_SIZE]; int ret; int i; if ((sock_fd = socket(AF_INET, SOCK_STREAM, 0)) == -1) &#123; perror(\"socket\"); exit(1); &#125; if (setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;yes, sizeof(int)) == -1) &#123; perror(\"setsockopt\"); exit(1); &#125; server_addr.sin_family = AF_INET; server_addr.sin_port = htons(MYPORT); server_addr.sin_addr.s_addr = INADDR_ANY; memset(server_addr.sin_zero, '\\0', sizeof(server_addr.sin_zero)); if (bind(sock_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)) == -1) &#123; perror(\"bind\"); exit(1); &#125; if (listen(sock_fd, BACKLOG) == -1) &#123; perror(\"listen\"); exit(1); &#125; printf(\"listen port %d\\n\", MYPORT); fd_set fdsr; int maxsock; struct timeval tv; conn_amount = 0; sin_size = sizeof(client_addr); maxsock = sock_fd; while (1) &#123; // initialize file descriptor set FD_ZERO(&amp;fdsr); FD_SET(sock_fd, &amp;fdsr); // timeout setting tv.tv_sec = 30; tv.tv_usec = 0; // add active connection to fd set for (i = 0; i &lt; BACKLOG; i++) &#123; if (fd_A[i] != 0) &#123; FD_SET(fd_A[i], &amp;fdsr); &#125; &#125; ret = select(maxsock + 1, &amp;fdsr, NULL, NULL, &amp;tv); if (ret &lt; 0) &#123; perror(\"select\"); break; &#125; else if (ret == 0) &#123; printf(\"timeout\\n\"); continue; &#125; // check every fd in the set for (i = 0; i &lt; conn_amount; i++) &#123; if (FD_ISSET(fd_A[i], &amp;fdsr)) &#123; ret = recv(fd_A[i], buf, sizeof(buf), 0); if (ret &lt;= 0) &#123; // client close printf(\"client[%d] close\\n\", i); close(fd_A[i]); FD_CLR(fd_A[i], &amp;fdsr); fd_A[i] = 0; &#125; else &#123; // receive data if (ret &lt; BUF_SIZE) memset(&amp;buf[ret], '\\0', 1); printf(\"client[%d] send:%s\\n\", i, buf); &#125; &#125; &#125; // check whether a new connection comes if (FD_ISSET(sock_fd, &amp;fdsr)) &#123; new_fd = accept(sock_fd, (struct sockaddr *)&amp;client_addr, &amp;sin_size); if (new_fd &lt;= 0) &#123; perror(\"accept\"); continue; &#125; // add to fd queue if (conn_amount &lt; BACKLOG) &#123; fd_A[conn_amount++] = new_fd; printf(\"new connection client[%d] %s:%d\\n\", conn_amount, inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port)); if (new_fd &gt; maxsock) maxsock = new_fd; &#125; else &#123; printf(\"max connections arrive, exit\\n\"); send(new_fd, \"bye\", 4, 0); close(new_fd); break; &#125; &#125; showclient(); &#125; // close other connections for (i = 0; i &lt; BACKLOG; i++) &#123; if (fd_A[i] != 0) &#123; close(fd_A[i]); &#125; &#125; exit(0);&#125; 代码实现采用select用法描述： 1.select作为服务端使用2.select监听服务的fd，如果有客户端连接此服务端时候，服务端fd会被触发，然后调用accept完成连接3.select监听服务端和客户端建立好连接的fd，如果客户端发送数据过来，select可监听到读信号，然后recv读出数据。 select实现分析用户态select 系统调用 sys_select 调用栈如下： 0xffffffff81213f80 : sys_select+0x0/0x110 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel] 实现代码位于：fs/select.c SYSCALL_DEFINE5(select,… select功能概述sys_select实现分析分析结论： sys_select1234567891011121314151617181920212223242526SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp, fd_set __user *, exp, struct timeval __user *, tvp)&#123; struct timespec end_time, *to = NULL; struct timeval tv; int ret; //用户态时间处理，将用户态时间拷入内核态并将参数规整为struct timespec以供调用 if (tvp) &#123; if (copy_from_user(&amp;tv, tvp, sizeof(tv))) return -EFAULT; to = &amp;end_time; if (poll_select_set_timeout(to, tv.tv_sec + (tv.tv_usec / USEC_PER_SEC), (tv.tv_usec % USEC_PER_SEC) * NSEC_PER_USEC)) return -EINVAL; &#125; //select的核心实现 ret = core_sys_select(n, inp, outp, exp, to); //该函数会将剩余的时间拷入到用户态的tvp 中 ret = poll_select_copy_remaining(&amp;end_time, tvp, 1, ret); return ret;&#125; 1.将用户态select时间参数拷入内核2.调用core_sys_select3.将select退出后剩余时间结果拷入用户态时间参数中 core_sys_select1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, struct timespec *end_time)&#123; fd_set_bits fds; void *bits; int ret, max_fds; unsigned int size; struct fdtable *fdt; /* Allocate small arguments on the stack to save memory and be faster */ long stack_fds[SELECT_STACK_ALLOC/sizeof(long)]; //用户态给予参数nfds &lt; 0 ,直接返并报告参数非法 -EINVAL ret = -EINVAL; if (n &lt; 0) goto out_nofds; /* max_fds can increase, so grab it once to avoid race */ rcu_read_lock(); fdt = files_fdtable(current-&gt;files); max_fds = fdt-&gt;max_fds; rcu_read_unlock(); if (n &gt; max_fds) n = max_fds; /* * We need 6 bitmaps (in/out/ex for both incoming and outgoing), * since we used fdset we need to allocate memory in units of * long-words. */ //以一个文件描述符占1bit，传递进来的这么多fd共占多数字 size = FDS_BYTES(n); bits = stack_fds; //检查默认静态数据资源是否够用 if (size &gt; sizeof(stack_fds) / 6) &#123; /* Not enough space in on-stack array; must use kmalloc */ ret = -ENOMEM; bits = kmalloc(6 * size, GFP_KERNEL); if (!bits) goto out_nofds; &#125; //fds用来指向具体的存储空间 fds.in = bits; fds.out = bits + size; fds.ex = bits + 2*size; fds.res_in = bits + 3*size; fds.res_out = bits + 4*size; fds.res_ex = bits + 5*size;//将用户空间的inp outp exp 拷入内核空间 if ((ret = get_fd_set(n, inp, fds.in)) || (ret = get_fd_set(n, outp, fds.out)) || (ret = get_fd_set(n, exp, fds.ex))) goto out; //存放返回状态的字段清零，后续可用作返回结果使用 zero_fd_set(n, fds.res_in); zero_fd_set(n, fds.res_out); zero_fd_set(n, fds.res_ex); //select核心逻辑处理函数 ret = do_select(n, &amp;fds, end_time); //存在错误 if (ret &lt; 0) goto out; //超时情况 if (!ret) &#123; ret = -ERESTARTNOHAND; if (signal_pending(current)) goto out; ret = 0; &#125; //把结果集拷入用户空间 if (set_fd_set(n, inp, fds.res_in) || set_fd_set(n, outp, fds.res_out) || set_fd_set(n, exp, fds.res_ex)) ret = -EFAULT;out: //释放辅助内存 if (bits != stack_fds) kfree(bits);out_nofds: return ret;&#125; 1.检验nfds，如果其小于0，参数异常返回；并规整nfds（最大不能超过当前进程的max_fds）2.将用户态fd集合拷入内核态3.运行do_select4.将do_select检测结果拷入用户空间5.释放select运算中辅助内存 do_select123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161int do_select(int n, fd_set_bits *fds, struct timespec *end_time)&#123; ktime_t expire, *to = NULL; struct poll_wqueues table; poll_table *wait; int retval, i, timed_out = 0; unsigned long slack = 0; unsigned int busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0; unsigned long busy_end = 0; rcu_read_lock(); //检查fd对应file状态，且找出最大fd retval = max_select_fd(n, fds); rcu_read_unlock(); if (retval &lt; 0) return retval; n = retval; poll_initwait(&amp;table); wait = &amp;table.pt; //传入的时间为0s 0ms time_out标记为1 这种情况不阻塞直接返回 if (end_time &amp;&amp; !end_time-&gt;tv_sec &amp;&amp; !end_time-&gt;tv_nsec) &#123; wait-&gt;_qproc = NULL; timed_out = 1; &#125; //正常情况处理。 超时时间转换 if (end_time &amp;&amp; !timed_out) slack = select_estimate_accuracy(end_time); retval = 0; for (;;) &#123; unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp; bool can_busy_loop = false; inp = fds-&gt;in; outp = fds-&gt;out; exp = fds-&gt;ex; rinp = fds-&gt;res_in; routp = fds-&gt;res_out; rexp = fds-&gt;res_ex; //所有监听的fd大循环 for (i = 0; i &lt; n; ++rinp, ++routp, ++rexp) &#123; unsigned long in, out, ex, all_bits, bit = 1, mask, j; unsigned long res_in = 0, res_out = 0, res_ex = 0; //32个文件描述符号，没有任何状态被检测，进入下一轮32个 in = *inp++; out = *outp++; ex = *exp++; all_bits = in | out | ex; if (all_bits == 0) &#123; i += BITS_PER_LONG; continue; &#125; // 这一轮32个fd存在需要检测的状态 for (j = 0; j &lt; BITS_PER_LONG; ++j, ++i, bit &lt;&lt;= 1) &#123; struct fd f; //超过最大待检测fd n直接退出循环 if (i &gt;= n) break; //跳过没有状态检测的fd if (!(bit &amp; all_bits)) continue; f = fdget(i); if (f.file) &#123; const struct file_operations *f_op; f_op = f.file-&gt;f_op; //设置fd检测事件掩码，poll相关情况处理 mask = DEFAULT_POLLMASK; if (f_op &amp;&amp; f_op-&gt;poll) &#123; //设置用户需要探查的标记 wait_key_set(wait, in, out, bit, busy_flag); //获取fd当前对应的信号掩码 mask = (*f_op-&gt;poll)(f.file, wait); &#125; fdput(f); //可读 if ((mask &amp; POLLIN_SET) &amp;&amp; (in &amp; bit)) &#123; res_in |= bit; retval++; wait-&gt;_qproc = NULL; &#125; //可写 if ((mask &amp; POLLOUT_SET) &amp;&amp; (out &amp; bit)) &#123; res_out |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLEX_SET) &amp;&amp; (ex &amp; bit)) &#123; res_ex |= bit; retval++; wait-&gt;_qproc = NULL; &#125; /* got something, stop busy polling */ if (retval) &#123; can_busy_loop = false; busy_flag = 0; /* * only remember a returned * POLL_BUSY_LOOP if we asked for it */ &#125; else if (busy_flag &amp; mask) can_busy_loop = true; &#125; &#125; //将检测结果存下来 if (res_in) *rinp = res_in; if (res_out) *routp = res_out; if (res_ex) *rexp = res_ex; //增加抢占点 该抢占点可达到效果是：判断是否有进程需要抢占当前进程，如果是将立即发生调度 //已经检查过的fd如果此时被唤醒，则会在此产生调度 cond_resched(); &#125; wait-&gt;_qproc = NULL; if (retval || timed_out || signal_pending(current)) break; //设备就绪异常超时终止灯信号触发，直接break，可跳出大循环结束程序 if (table.error) &#123; retval = table.error; break; &#125; /* only if found POLL_BUSY_LOOP sockets &amp;&amp; not out of time */ if (can_busy_loop &amp;&amp; !need_resched()) &#123; if (!busy_end) &#123; busy_end = busy_loop_end_time(); continue; &#125; if (!busy_loop_timeout(busy_end)) continue; &#125; busy_flag = 0; /* * If this is the first loop and we have a timeout * given, then we convert to ktime_t and set the to * pointer to the expiry value. */ if (end_time &amp;&amp; !to) &#123; expire = timespec_to_ktime(*end_time); to = &amp;expire; &#125; //当前用户进程从这里进入睡眠，超时后timed_out 置1 直接退出 if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) timed_out = 1; &#125; poll_freewait(&amp;table); return retval; &#125; do_select为select的核心实现，其处理过程如下： 1.调用poll_initwait初始化poll_wqueues对象table，包括其成员poll_table； 2.如果用户传入的timeout不为NULL，但是设定的时间为0，那么设置poll_table指针wait(即 &amp;table.pt）为NULL；当&amp;table.pt为NULL，它并不会被加到等到队列中。 3.将in,out和exception进行或运算，得到all_bits，然后遍历all_bits中bit为1的fd，根据进程的fd_table查找到file指针filp，然后设置wait的key值（POLLEX_SET, POLLIN_SET,POLLIN_SET三者的或运算，取决于用户输入），并调用filp-&gt;poll(filp, wait)，获得返回值mask。 再根据mask值检查该文件是否立即满足条件，如果满足，设置res_in/res_out/res_exception的值，执行retval++, 并设置wait为NULL。 4.在每遍历32（取决于long型整数的位数）个文件后，调用1次cond_resched()，主动寻求调度，可以等待已经遍历过的文件是否有唤醒的； 5.在遍历完所有文件之后，设置wait为NULL，并检查是否有满足条件的文件（retval值是否为0），或者是否超时，或者是否有未决信号，如果有那么直接跳出循环，进入步骤7； 6.否则调用poll_schedule_timeout，使进程进入睡眠，直到超时（如果未设置超时，那么是直接调用的schedule()）。如果是超时后进程继续执行，那么设置pwq-&gt;triggered为0；如果是被文件对应的驱动程序唤醒的，那么pwq-&gt;triggered被设置为1. 7.最终，函数调用poll_freewait，将本进程从所有文件的等待队列中删掉，并删除分配的poll_table_page对象，回收内存，并返回retval值。 8.拷贝res_in, res_out和res_exception到传入的in, out, exception，并返回ret。 select睡眠过程do_select … 步骤1 poll_initwait(&amp;table); wait = &amp;table.pt;… 步骤2 if (f_op &amp;&amp; f_op-&gt;poll) { wait_key_set(wait, in, out, bit, busy_flag); //如果是socket此处调用的是sock_poll mask = (*f_op-&gt;poll)(f.file, wait);} 步骤3… if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) 步骤1:初始化table struct poll_wqueues table; 12345678910void poll_initwait(struct poll_wqueues *pwq)&#123; init_poll_funcptr(&amp;pwq-&gt;pt, __pollwait); pwq-&gt;polling_task = current; pwq-&gt;triggered = 0; pwq-&gt;error = 0; pwq-&gt;table = NULL; pwq-&gt;inline_index = 0;&#125;EXPORT_SYMBOL(poll_initwait); 将当前进程标志current给table让其记录下来 将__pollwait给table-&gt;pt-&gt;_qproc让其记录下来 步骤2:调用sock_poll最终调用tcp_pool12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* No kernel lock held - perfect */static unsigned int sock_poll(struct file *file, poll_table *wait)&#123; unsigned int busy_flag = 0; struct socket *sock; /* * We can't return errors to poll, so it's either yes or no. */ sock = file-&gt;private_data; if (sk_can_busy_loop(sock-&gt;sk)) &#123; /* this socket can poll_ll so tell the system call */ busy_flag = POLL_BUSY_LOOP; /* once, only if requested by syscall */ if (wait &amp;&amp; (wait-&gt;_key &amp; POLL_BUSY_LOOP)) sk_busy_loop(sock-&gt;sk, 1); &#125; //针对于tcpsocket来讲此处调用tcp_pool return busy_flag | sock-&gt;ops-&gt;poll(file, sock, wait);&#125;/* * Wait for a TCP event. * * Note that we don't need to lock the socket, as the upper poll layers * take care of normal races (between the test and the event) and we don't * go look at any of the socket buffers directly. */unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)&#123; unsigned int mask; struct sock *sk = sock-&gt;sk; const struct tcp_sock *tp = tcp_sk(sk); sock_rps_record_flow(sk); sock_poll_wait(file, sk_sleep(sk), wait); if (sk-&gt;sk_state == TCP_LISTEN) return inet_csk_listen_poll(sk); /* Socket is not locked. We are protected from async events * by poll logic and correct handling of state changes * made by other threads is impossible in any case. */ mask = 0; if (sk-&gt;sk_shutdown == SHUTDOWN_MASK || sk-&gt;sk_state == TCP_CLOSE) mask |= POLLHUP; if (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) mask |= POLLIN | POLLRDNORM | POLLRDHUP; /* Connected or passive Fast Open socket? */ if (sk-&gt;sk_state != TCP_SYN_SENT &amp;&amp; (sk-&gt;sk_state != TCP_SYN_RECV || tp-&gt;fastopen_rsk != NULL)) &#123; int target = sock_rcvlowat(sk, 0, INT_MAX); if (tp-&gt;urg_seq == tp-&gt;copied_seq &amp;&amp; !sock_flag(sk, SOCK_URGINLINE) &amp;&amp; tp-&gt;urg_data) target++; /* Potential race condition. If read of tp below will * escape above sk-&gt;sk_state, we can be illegally awaken * in SYN_* states. */ if (tp-&gt;rcv_nxt - tp-&gt;copied_seq &gt;= target) mask |= POLLIN | POLLRDNORM; if (!(sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) &#123; if (sk_stream_is_writeable(sk)) &#123; mask |= POLLOUT | POLLWRNORM; &#125; else &#123; /* send SIGIO later */ set_bit(SOCK_ASYNC_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); set_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /* Race breaker. If space is freed after * wspace test but before the flags are set, * IO signal will be lost. */ if (sk_stream_is_writeable(sk)) mask |= POLLOUT | POLLWRNORM; &#125; &#125; else mask |= POLLOUT | POLLWRNORM; if (tp-&gt;urg_data &amp; TCP_URG_VALID) mask |= POLLPRI; &#125; /* This barrier is coupled with smp_wmb() in tcp_reset() */ smp_rmb(); if (sk-&gt;sk_err) mask |= POLLERR; return mask;&#125; 收集信号状态以mask方式返回 调用sock_poll_wait然后poll_wait最终调用_qproc也就是__pollwait __pollwait 123456789101112131415/* Add a new entry */static void __pollwait(struct file *filp, wait_queue_head_t *wait_address, poll_table *p)&#123; struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt); struct poll_table_entry *entry = poll_get_entry(pwq); if (!entry) return; entry-&gt;filp = get_file(filp); entry-&gt;wait_address = wait_address; entry-&gt;key = p-&gt;_key; init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake); entry-&gt;wait.private = pwq; add_wait_queue(wait_address, &amp;entry-&gt;wait);&#125; 为每个fd对应文件分配 poll_table_entry 将fd对应poll_table_entry加入到等待队列中 步骤3: poll_schedule_timeout，作用是使进程进入睡眠，直到超时或者被唤醒 如果超时后进程继续执行设置pwq-&gt;triggered为0 如果是被文件对应的驱动程序唤醒pwq-&gt;triggered为1 1234567891011121314151617181920212223242526int poll_schedule_timeout(struct poll_wqueues *pwq, int state, ktime_t *expires, unsigned long slack)&#123; int rc = -EINTR; set_current_state(state); if (!pwq-&gt;triggered) rc = freezable_schedule_hrtimeout_range(expires, slack, HRTIMER_MODE_ABS); __set_current_state(TASK_RUNNING); /* * Prepare for the next iteration. * * The following set_mb() serves two purposes. First, it's * the counterpart rmb of the wmb in pollwake() such that data * written before wake up is always visible after wake up. * Second, the full barrier guarantees that triggered clearing * doesn't pass event check of the next iteration. Note that * this problem doesn't exist for the first iteration as * add_wait_queue() has full barrier semantics. */ set_mb(pwq-&gt;triggered, 0); return rc;&#125; select唤醒过程 0xffffffff81213130 : pollwake+0x0/0x90 [kernel] 0xffffffff810ba628 : wake_up_common+0x58/0x90 [kernel] 0xffffffff810bc4a4 : wake_up_sync_key+0x44/0x60 [kernel] 0xffffffff8155825a : sock_def_readable+0x3a/0x70 [kernel] 0xffffffff815c8197 : tcp_data_queue+0x497/0xdd0 [kernel] 0xffffffff815cb4a7 : tcp_rcv_established+0x217/0x760 [kernel] 0xffffffff815d5f8a : tcp_v4_do_rcv+0x10a/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000] pollwake -&gt;__pollwake-&gt;default_wake_function-&gt;try_to_wake_up try_to_wake_up会把进程的状态设置为TASK_RUNNING，并把进程插入CPU运行队列，来唤醒睡眠的进程 linux select 1024限制魔咒__FD_SETSIZE 默认最大为1024，一个int占用4个byte，也就是32个bit，所以使用了一个int数组大小为32位来表示了我们要操作的fd的数值，每个bit代表了一个handle数值 需要注意的问题是，这里的最大为1024，如果handle数值为1025是不能处理的（而且很容易导致破坏堆栈），不是说可以容纳1024个网络客户端句柄，而是最大的handle数值为1024，再算上系统本身使用的stdout,stdin, stderr默认的3个，因此最多也就是1021个，再算上程序打开的文件句柄等等，实际上使用可能要比1024少上好多。 另外，ulimit对每个进程打开的句柄也有限制。 why 1024 ?内核参数适用结构体是fd_set 123SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp, fd_set __user *, exp, struct timeval __user *, tvp)&#123; fd_set是 __kernel_fd_set 1typedef __kernel_fd_set fd_set; __kernel_fd_set 中fds_bits 最大只能1024 12345#define __FD_SETSIZE 1024typedef struct &#123; unsigned long fds_bits[__FD_SETSIZE / (8 * sizeof(long))];&#125; __kernel_fd_set; 我该怎么办才能突破1024限制？修改掉此宏重新编译吧，当然还有其他办法，但是没必要这么复杂，直接用pool或者epool解决吧当然你也可以多进程或者多线程，每个进程／线程 分别select select缺点总结 select效率低下，用户空间和内核空间来回拷贝，select内部吧存进程上下文切换，大型项目不适用可同时监听的文件数量有限，linux平台1024个每次调用select都要遍历完成所有的fd，每隔32fd需要调度一次多个fd情况下，如果小的fs一直可读，会导致大的fd信号不会被收集到需要在用户态和内核态来回拷贝fd_set，睡眠唤醒机制需要为fd分配poll_table_entry","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"select","slug":"select","permalink":"http://vcpu.me/tags/select/"}]},{"title":"TIME_WAIT状态分析","slug":"TIME_WAIT状态分析","date":"2017-06-23T03:16:59.000Z","updated":"2017-06-23T03:16:59.000Z","comments":true,"path":"TIME_WAIT状态分析/","link":"","permalink":"http://vcpu.me/TIME_WAIT状态分析/","excerpt":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat","text":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat 我猜你一定被巨大无比的TIME_WAIT网络连接总数吓到了！以我个人的经验，对于一台繁忙的Web服务器来说，如果主要以短连接为主，那么其TIME_WAIT网络连接总数很可能会达到几万，甚至十几万。虽然一个TIME_WAIT网络连接耗费的资源无非就是一个端口、一点内存，但是架不住基数大，所以这始终是一个需要面对的问题。 TIMEWAIT是什么因为TCP连接是双向的，所以在关闭连接的时候，两个方向各自都需要关闭。先发FIN包的一方执行的是主动关闭；后发FIN包的一方执行的是被动关闭。主动关闭的一方会进入TIME_WAIT状态，并且在此状态停留两倍的MSL时长。穿插一点MSL的知识：MSL指的是报文段的最大生存时间，如果报文段在网络活动了MSL时间，还没有被接收，那么会被丢弃。关于MSL的大小，RFC 793协议中给出的建议是两分钟，不过实际上不同的操作系统可能有不同的设置，以Linux为例，通常是半分钟，两倍的MSL就是一分钟，也就是60秒，并且这个数值是硬编码在内核中的，也就是说除非你重新编译内核，否则没法修改它： #define TCP_TIMEWAIT_LEN (60*HZ) 如果每秒的连接数是一千的话，那么一分钟就可能会产生六万个TIME_WAIT。为什么主动关闭的一方不直接进入CLOSED状态，而是进入TIME_WAIT状态，并且停留两倍的MSL时长呢？这是因为TCP是建立在不可靠网络上的可靠的协议。例子：主动关闭的一方收到被动关闭的一方发出的FIN包后，回应ACK包，同时进入TIME_WAIT状态，但是因为网络原因，主动关闭的一方发送的这个ACK包很可能延迟，从而触发被动连接一方重传FIN包。极端情况下，这一去一回，就是两倍的MSL时长。如果主动关闭的一方跳过TIME_WAIT直接进入CLOSED，或者在TIME_WAIT停留的时长不足两倍的MSL，那么当被动关闭的一方早先发出的延迟包到达后，就可能出现类似下面的问题： ▪ 旧的TCP连接已经不存在了，系统此时只能返回RST包 ▪ 新的TCP连接被建立起来了，延迟包可能干扰新的连接不管是哪种情况都会让TCP不再可靠，所以TIME_WAIT状态有存在的必要性。 如何控制TIME_WAIT的数量？从前面的描述我们可以得出这样的结论：TIME_WAIT这东西没有的话不行，不过太多可能也是个麻烦事。下面让我们看看有哪些方法可以控制TIME_WAIT数量，这里只说一些常规方法，另外一些诸如SO_LINGER之类的方法太过偏门，略过不谈。ip_conntrack：顾名思义就是跟踪连接。一旦激活了此模块，就能在系统参数里发现很多用来控制网络连接状态超时的设置，其中自然也包括TIME_WAIT：shell&gt; modprobe ip_conntrackshell&gt; sysctl net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait我们可以尝试缩小它的设置，比如十秒，甚至一秒，具体设置成多少合适取决于网络情况而定，当然也可以参考相关的案例。不过就我的个人意见来说，ip_conntrack引入的问题比解决的还多，比如性能会大幅下降，所以不建议使用。 tcp_tw_recycle：顾名思义就是回收TIME_WAIT连接。可以说这个内核参数已经变成了大众处理TIME_WAIT的万金油，如果你在网络上搜索TIME_WAIT的解决方案，十有八九会推荐设置它，不过这里隐藏着一个不易察觉的陷阱：当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。（tcp_tw_recycle和tcp_timestamps导致connect失败问题。同时开启情况下，60s内同一源ip主机socket 请求中timestamp必须是递增的） tcp_tw_reuse：顾名思义就是复用TIME_WAIT连接。当创建新连接的时候，如果可能的话会考虑复用相应的TIME_WAIT连接。通常认为「tcp_tw_reuse」比「tcp_tw_recycle」安全一些，这是因为一来TIME_WAIT创建时间必须超过一秒才可能会被复用；二来只有连接的时间戳是递增的时候才会被复用。官方文档里是这样说的：如果从协议视角看它是安全的，那么就可以使用。这简直就是外交辞令啊！按我的看法，如果网络比较稳定，比如都是内网连接，那么就可以尝试使用。不过需要注意的是在哪里使用，既然我们要复用连接，那么当然应该在连接的发起方使用，而不能在被连接方使用。举例来说：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端，此类情况使用「tcp_tw_reuse」是无效的，因为服务端是被连接方，所以不存在复用连接一说。让我们延伸一点来看，比如说服务端是PHP，它查询另一个MySQL服务端，然后主动断开连接，于是TIME_WAIT就落在了PHP一侧，此类情况下使用「tcp_tw_reuse」是有效的，因为此时PHP相对于MySQL而言是客户端，它是连接的发起方，所以可以复用连接。说明：如果使用tcp_tw_reuse，请激活tcp_timestamps，否则无效。 tcp_max_tw_buckets：顾名思义就是控制TIME_WAIT总数。官网文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为的降低它。如果缩小了它，那么系统会将多余的TIME_WAIT删除掉，日志里会显示：「TCP: time wait bucket table overflow」。需要提醒大家的是物极必反，曾经看到有人把「tcp_max_tw_buckets」设置成0，也就是说完全抛弃TIME_WAIT，这就有些冒险了，用一句围棋谚语来说：入界宜缓。…有时候，如果我们换个角度去看问题，往往能得到四两拨千斤的效果。前面提到的例子：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端。这里的关键在于主动关闭连接的是服务端！在关闭TCP连接的时候，先出手的一方注定逃不开TIME_WAIT的宿命，套用一句歌词：把我的悲伤留给自己，你的美丽让你带走。如果客户端可控的话，那么在服务端打开KeepAlive，尽可能不让服务端主动关闭连接，而让客户端主动关闭连接，如此一来问题便迎刃而解了。 原文连接于https://huoding.com/2013/12/31/316","categories":[{"name":"TCP","slug":"TCP","permalink":"http://vcpu.me/categories/TCP/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"}]},{"title":"Linux系统调用","slug":"Linux系统调用","date":"2017-06-22T10:10:14.000Z","updated":"2017-06-22T10:10:14.000Z","comments":true,"path":"Linux系统调用/","link":"","permalink":"http://vcpu.me/Linux系统调用/","excerpt":"","text":"什么事系统调用 linux虚拟地址空间分为用户空间和内核空间 用户空间不可直接访问内核空间，帝王班的内核空间可直接访问用户空间 用户空间只能通过系统调用访问内核空间 系统调用时内核提供的一组函数接口，使得用户空间上进程可以和内核空间交互 系统调用过程 执行用户程序 根据glibc中实现，取得系统调用号，将其存入EAX并执行int $0x80（128号中断） 用户态可以传递变量、参数值给内核，内核态运行时候会保存用户进程的一些寄存器值等（上下文环境） 触发中断后内核根据系统调用号执行对应的中断处理函数 系统调用结束将访问址存入EAX，返回中断处理函数 中断处理函数根据存储用户态进程上下文环境恢复用户态，同时用户态就获取了内核态函数执行的返回值 系统调用汇编123456// pid = fork();asm volatile( &quot;mov $0x2, %%eax\\n\\t&quot; // 将fork的系统调用号2存到eax寄存器 &quot;int $0x80\\n\\t&quot; // 产生int 0x80中断 &quot;mov %%eax,%0\\n\\t&quot; // 将结果存入pid中 : &quot;=m&quot; (pid) 系统调用实现分析待续 添加系统调用待续","categories":[{"name":"linux","slug":"linux","permalink":"http://vcpu.me/categories/linux/"}],"tags":[{"name":"系统调用","slug":"系统调用","permalink":"http://vcpu.me/tags/系统调用/"}]},{"title":"tcp socket发送缓冲区","slug":"tcp_sndbuf","date":"2017-06-20T10:31:48.000Z","updated":"2017-06-20T10:31:48.000Z","comments":true,"path":"tcp_sndbuf/","link":"","permalink":"http://vcpu.me/tcp_sndbuf/","excerpt":"tcp socket发送缓冲区探究结论 1: 未设置SO_SNDBUF时，sk-&gt;sk_sndbuf值由tcp_finish_connect-&gt;tcp_init_buffer_space-&gt;tcp_sndbuf_expand决定，TCP协议栈会自己计算一个值出来46080，sk_sndbuf是46080和net.ipv4.tcp_wmem[2]（4194304）的较小值 2: 设置SO_SNDBUF后，tcp_sndbuf_expand将不会再被调用，其值情况完全由sock_setsockopt决定 2-1: 设置值较小 value &lt; 2304 { SOCK_MIN_SNDBUF（4608）/2 } sk_sndbuf = 4608 2-2: 设置值适中 { SOCK_MIN_SNDBUF（4608）/2 } &lt; value &lt; net.core.wmem_max sk_sndbuf = value*2 2-3: 设置值较大 value &gt; net.core.wmem_max sk_sndbuf = net.core.wmem_max* 2","text":"tcp socket发送缓冲区探究结论 1: 未设置SO_SNDBUF时，sk-&gt;sk_sndbuf值由tcp_finish_connect-&gt;tcp_init_buffer_space-&gt;tcp_sndbuf_expand决定，TCP协议栈会自己计算一个值出来46080，sk_sndbuf是46080和net.ipv4.tcp_wmem[2]（4194304）的较小值 2: 设置SO_SNDBUF后，tcp_sndbuf_expand将不会再被调用，其值情况完全由sock_setsockopt决定 2-1: 设置值较小 value &lt; 2304 { SOCK_MIN_SNDBUF（4608）/2 } sk_sndbuf = 4608 2-2: 设置值适中 { SOCK_MIN_SNDBUF（4608）/2 } &lt; value &lt; net.core.wmem_max sk_sndbuf = value*2 2-3: 设置值较大 value &gt; net.core.wmem_max sk_sndbuf = net.core.wmem_max* 2 默认情况下（未设置SO_SNDBUF）net.core.wmem_default = 212992net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 TCPsocket未connect之前 sendbuf:16384 sk-&gt;sk_sndbuf是sysctl_tcp_wmem[1]的值 connect之后，sendbuf:46080 通过调试机制可知，sendbuf默认大小为sysctl_tcp_wmem[1] 为16384connect连接连接到服务端后，sendbuf变为46080，该值不是尚书配置中任何一个值 原因探究阶段1:tcp_init_sock初始化，sk-&gt;sk_sndbuf = sysctl_tcp_wmem[1] 阶段2:主动连接进入ES状态时候，状态切换时候调用tcp_sndbuf_expand调整sk_sndbuf stp脚本探测结果如下：123456789101112131415161718tcp_v4_connect[2017/6/20,10:57:56]local=0.0.0.0:3000,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:16384tcp_v4_connect return [2017/6/20,10:57:56]local=192.168.55.178:3000,remote=180.97.33.108:80 state:SYN_SENT,sndbubf 1280:16384tcp_input:302 return [2017/6/20,10:57:56]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:16384 sndmem : 46080 permss 2304 0xffffffff815c3527 : tcp_sndbuf_expand+0x67/0x90 [kernel] 0xffffffff815c7ba8 : tcp_init_buffer_space+0x178/0x190 [kernel] 0xffffffff815cbbae : tcp_finish_connect+0x6e/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : __netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : __netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000] 12345678910111213141516171819202122232425262728static void tcp_sndbuf_expand(struct sock *sk)&#123; const struct tcp_sock *tp = tcp_sk(sk); int sndmem, per_mss; u32 nr_segs; /* Worst case is non GSO/TSO : each frame consumes one skb * and skb-&gt;head is kmalloced using power of two area of memory */ per_mss = max_t(u32, tp-&gt;rx_opt.mss_clamp, tp-&gt;mss_cache) + MAX_TCP_HEADER + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)); per_mss = roundup_pow_of_two(per_mss) + SKB_DATA_ALIGN(sizeof(struct sk_buff)); nr_segs = max_t(u32, TCP_INIT_CWND, tp-&gt;snd_cwnd); nr_segs = max_t(u32, nr_segs, tp-&gt;reordering + 1); /* Fast Recovery (RFC 5681 3.2) : * Cubic needs 1.7 factor, rounded to 2 to include * extra cushion (application might react slowly to POLLOUT) */ sndmem = 2 * nr_segs * per_mss; if (sk-&gt;sk_sndbuf &lt; sndmem) sk-&gt;sk_sndbuf = min(sndmem, sysctl_tcp_wmem[2]);&#125; 设置发送缓冲区大小为较小值123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 100;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 输出信息如下：default,sendbuf:16384now,sendbuf:4608 输出信息总结：设置sendbubf为100时，没有生效，反而设置出来一个较大的值4608 原因探究12345678910111213141516171819202122232425262728293031323334/* * This is meant for all protocols to use and covers goings on * at the socket level. Everything here is generic. */int sock_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)&#123; ··· case SO_SNDBUF: /* Don't error on this BSD doesn't and if you think * about it this is right. Otherwise apps have to * play 'guess the biggest size' games. RCVBUF/SNDBUF * are treated in BSD as hints */ val = min_t(u32, val, sysctl_wmem_max);set_sndbuf: sk-&gt;sk_userlocks |= SOCK_SNDBUF_LOCK; sk-&gt;sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF); /* Wake up sending tasks if we upped the value. */ sk-&gt;sk_write_space(sk); break; ··· default: ret = -ENOPROTOOPT; break; &#125; release_sock(sk); return ret;&#125;#define TCP_SKB_MIN_TRUESIZE (2048 + SKB_DATA_ALIGN(sizeof(struct sk_buff)))#define SOCK_MIN_SNDBUF (TCP_SKB_MIN_TRUESIZE * 2) 设置socket选项SO_SNDBUF会触发系统调用最终调用sock_setsockopt函数，其处理设置选项过程如上：其会将用户设置的缓冲区大小乘以2，然后和SOCK_MIN_SNDBUF（4608）比较，取较大值因此最终较小的缓冲区设置值200没有生效，生效的是4608 设置发送缓冲区大小为中间值缓冲区系统设置值大小：net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 实验动作将缓冲区大小设置为3000123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 3000;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 实验程序输出：default,sendbuf:16384now,sendbuf:6000输出信息总结：设置大小3000生效，sndbuf大小会被设置成为3000*2 设置发送缓冲区大小威较大值缓冲区系统设置值大小：net.core.wmem_max = 212992net.ipv4.tcp_wmem = 4096 16384 4194304 实验动作将缓冲区大小设置为230000123456789socklen_t sendbuflen = 0;socklen_t len = sizeof(sendbuflen);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"default,sendbuf:%d\\n\", sendbuflen);socklen_t sendbuflen = 230000;setsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len);getsockopt(fd, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len);printf(\"now,sendbuf:%d\\n\", sendbuflen); 实现程序输出：default,sendbuf:16384now,sendbuf:425984实验结果分析：设置大小23000（大于系统212992），sendbuf最终结果为212992*2 原因探究12345678case SO_SNDBUF: val = min_t(u32, val, sysctl_wmem_max);set_sndbuf: sk-&gt;sk_userlocks |= SOCK_SNDBUF_LOCK; sk-&gt;sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF); /* Wake up sending tasks if we upped the value. */ sk-&gt;sk_write_space(sk); break; val为用户set的值，其在选择时候会同sysctl_wmem_max比较，选取一个较小的值，如果设置值大于sysctl_wmem_max值的话，val就取系统wmem的最大值。 如上可知：230000 &gt; net.core.wmem_max ,所以用户设置SO_SNDBUF选项最大只能取net.core.wmem_max，所以最终sk_sndbubf值为net.core.wmem_max*2 即425984 其它说明tcp socket记录当前发送队列的占用缓冲区大小的变量为sk_wmem_queued和发送缓冲区判断函数如下：12345678static inline bool sk_stream_memory_free(const struct sock *sk)&#123; if (sk-&gt;sk_wmem_queued &gt;= sk-&gt;sk_sndbuf) return false; return sk-&gt;sk_prot-&gt;stream_memory_free ? sk-&gt;sk_prot-&gt;stream_memory_free(sk) : true;&#125; 从上述判别中我们可以知道，发送缓冲区记录和比对单位均是字节","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"http://vcpu.me/tags/socket/"}]},{"title":"PF_INET和AF_INET区别","slug":"pf_inet","date":"2017-06-20T10:12:51.000Z","updated":"2017-06-20T10:12:51.000Z","comments":true,"path":"pf_inet/","link":"","permalink":"http://vcpu.me/pf_inet/","excerpt":"","text":"PF_INET和AF_INET区别 在初始化socket时候socket(PF_INET,SOCK_SRTEAM,0) 用PF_INET，表示ip协议 指定地址协议族时候用AF_INET，表示地址为IP协议 Linux AF_INET和PF_INET值相同均为2 123456socket.h#define AF_INET 2 /* Internet IP Protocol *...#define PF_INET AF_INET socket通信协议类型 SOCKET_STREAM: 面向连接TCP SOCK_DGRAM: 无保障UDP","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"PF_INET","slug":"PF-INET","permalink":"http://vcpu.me/tags/PF-INET/"},{"name":"AF_INET","slug":"AF-INET","permalink":"http://vcpu.me/tags/AF-INET/"}]},{"title":"sockaddr_in和sockaddr的区别","slug":"sockaddr_in","date":"2017-06-20T07:18:11.000Z","updated":"2017-06-20T07:18:11.000Z","comments":true,"path":"sockaddr_in/","link":"","permalink":"http://vcpu.me/sockaddr_in/","excerpt":"sockaddr_in在头文件/usr/include/netinet/in.h123456789101112131415161718192021222324252627282930/* Structure describing an Internet socket address. */struct sockaddr_in &#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr&apos;. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)]; &#125;; or struct sockaddr_in &#123; short int sin_family; /* Address family */ unsigned short int sin_port; /* Port number */ struct in_addr sin_addr; /* Internet address */ unsigned char sin_zero[8]; /* Same size as struct sockaddr */&#125;; struct in_addr &#123; union &#123; struct &#123; u_char s_b1,s_b2,s_b3,s_b4; &#125; S_un_b; struct &#123; u_short s_w1,s_w2; &#125; S_un_w; u_long S_addr; &#125; S_un; #define s_addr S_un.S_addr &#125;; 组成包含协议家族、端口、地址、填充 端口和地址，需要是网络字节序号 inet_addr(“127.0.0.1”)把字符串点分十进制地址按照网络字节序转换为4字节的地址","text":"sockaddr_in在头文件/usr/include/netinet/in.h123456789101112131415161718192021222324252627282930/* Structure describing an Internet socket address. */struct sockaddr_in &#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr&apos;. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)]; &#125;; or struct sockaddr_in &#123; short int sin_family; /* Address family */ unsigned short int sin_port; /* Port number */ struct in_addr sin_addr; /* Internet address */ unsigned char sin_zero[8]; /* Same size as struct sockaddr */&#125;; struct in_addr &#123; union &#123; struct &#123; u_char s_b1,s_b2,s_b3,s_b4; &#125; S_un_b; struct &#123; u_short s_w1,s_w2; &#125; S_un_w; u_long S_addr; &#125; S_un; #define s_addr S_un.S_addr &#125;; 组成包含协议家族、端口、地址、填充 端口和地址，需要是网络字节序号 inet_addr(“127.0.0.1”)把字符串点分十进制地址按照网络字节序转换为4字节的地址 sockaddr为通用的socket地址1234struct sockaddr &#123; unsigned short sa_family; // address family, AF_INET char sa_data[14]; // 14 bytes of protocol address &#125;; bind、connect、recv、send等socket参数使用的就是这个结构体","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"sockaddr_in","slug":"sockaddr-in","permalink":"http://vcpu.me/tags/sockaddr-in/"},{"name":"sockaddr","slug":"sockaddr","permalink":"http://vcpu.me/tags/sockaddr/"}]},{"title":"systemtap使用调试记录（二）","slug":"socket_stp","date":"2017-06-20T06:52:23.000Z","updated":"2017-06-20T06:52:23.000Z","comments":true,"path":"socket_stp/","link":"","permalink":"http://vcpu.me/socket_stp/","excerpt":"socket sendbubf探究stp脚本 centos7 3.10.0-514.16.1.el7.x86_64 该systap脚本是在调用协议栈sk-&gt;sk_sndbuf可能改变的位置增加探测点，探究snd_buf变更规律使用","text":"socket sendbubf探究stp脚本 centos7 3.10.0-514.16.1.el7.x86_64 该systap脚本是在调用协议栈sk-&gt;sk_sndbuf可能改变的位置增加探测点，探究snd_buf变更规律使用 脚本socket.stp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217%&#123; #include &lt;linux/tcp.h&gt; #include&lt;linux/rtc.h&gt; #include &lt;net/tcp.h&gt; static const char tcp_state_array[][16] = &#123; \"NULL\", \"ESTABLISHED\", \"SYN_SENT\", \"SYN_RECV\", \"FIN_WAIT1\", \"FIN_WAIT2\", \"TIME_WAIT\", \"CLOSE\", \"CLOSE_WAIT\", \"LAST_ACK\", \"LISTEN\", \"CLOSING\" &#125;;%&#125;function get_short_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%02d:%02d:%02d\", tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_full_time:string()%&#123; struct timeval tv; struct rtc_time tm; unsigned long time; do_gettimeofday(&amp;tv); time = tv.tv_sec + 8 * 3600; rtc_time_to_tm(time, &amp;tm); sprintf(STAP_RETVALUE, \"%d/%d/%d,%02d:%02d:%02d\", tm.tm_year+1900, tm.tm_mon+1, tm.tm_mday, tm.tm_hour, tm.tm_min, tm.tm_sec);%&#125;function get_conn_lifetime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = jiffies_to_msecs(tcp_time_stamp - info-&gt;estab_t);%&#125;function get_conn_data:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct tcp_sock *tp = tcp_sk(sk); struct stap_info *info = sk-&gt;sk_protinfo; u32 len = tp-&gt;snd_nxt - info-&gt;isn; STAP_RETVALUE = len ? len - 1 : len;%&#125;function filter_http_transtime:long (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; struct stap_info *info = sk-&gt;sk_protinfo; STAP_RETVALUE = info-&gt;http_filter;%&#125;function get_socket_addr:string (sk:long)&#123; laddr = tcpmib_local_addr(sk) lport = tcpmib_local_port(sk) raddr = tcpmib_remote_addr(sk) rport = tcpmib_remote_port(sk) local_addr = sprintf(\"%s:%d\", ip_ntop(htonl(laddr)), lport) remote_addr = sprintf(\"%s:%d\", ip_ntop(htonl(raddr)), rport) return sprintf(\"local=%s,remote=%s\", local_addr, remote_addr)&#125;function get_socket_state:string (sk:long)%&#123; struct sock *sk = (struct sock *)STAP_ARG_sk; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[sk-&gt;sk_state]);%&#125;function get_socket_sk_sndbuf:string(sk:long)%&#123; struct sock *sk=(struct sock*)STAP_ARG_sk; sprintf(STAP_RETVALUE,\"%d:%d\", sk-&gt;sk_wmem_queued, sk-&gt;sk_sndbuf);%&#125;function socket_state_num2str:string (state:long)%&#123; sprintf(STAP_RETVALUE, \"%s\", tcp_state_array[STAP_ARG_state]);%&#125;function sshfilter:long(sk:long)&#123; lport = tcpmib_local_port(sk) if(lport == 22) return 1 return 0&#125;probe kernel.function(\"tcp_send_ack\").call&#123; if(sshfilter($sk)) next printf(\"tcp_send_ack[%s]%s state:%s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk));&#125;probe kernel.function(\"tcp_sendmsg\").call&#123; if(sshfilter($sk)) next printf(\"tcp_sendmsg[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_sendmsg\").return&#123; if(sshfilter($sk)) next printf(\"tcp_sendmsg return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/core/sock.c:711\")&#123; if(sshfilter($sk)) next printf(\"sock:711 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/core/sock.c:715\")&#123; if(sshfilter($sk)) next printf(\"sock:715 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/ip_output.c:1581\")&#123; if(sshfilter($sk)) next printf(\"ip_output:1581 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/ip_output.c:1583\")&#123; if(sshfilter($sk)) next printf(\"ip_output:1583 return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.statement(\"*@net/ipv4/tcp_input.c:304\")&#123; if(sshfilter($sk)) next printf(\"tcp_input:304 return [%s]%s state:%s,sndbubf %s sndmem : %d permss %d\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk),$sndmem,$per_mss); print_backtrace()&#125;probe kernel.function(\"__sk_mem_schedule\").call&#123; if(sshfilter($sk)) next printf(\"__sk_mem_schedule[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"__sk_mem_schedule\").return&#123; if(sshfilter($sk)) next printf(\"__sk_mem_schedule return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_page_frag_refill\").call&#123; if(sshfilter($sk)) next printf(\"sk_page_frag_refill[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_page_frag_refill\").return&#123; if(sshfilter($sk)) next printf(\"sk_page_frag_refill return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_stream_alloc_skb\").call&#123; if(sshfilter($sk)) next printf(\"sk_stream_alloc_skb[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"sk_stream_alloc_skb\").return&#123; if(sshfilter($sk)) next printf(\"sk_stream_alloc_skb return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_v4_connect\").call&#123; if(sshfilter($sk)) next printf(\"tcp_v4_connect[%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125;probe kernel.function(\"tcp_v4_connect\").return&#123; if(sshfilter($sk)) next printf(\"tcp_v4_connect return [%s]%s state:%s,sndbubf %s\\n\",get_full_time(),get_socket_addr($sk),get_socket_state($sk),get_socket_sk_sndbuf($sk));&#125; 执行步骤stap -g socket.stp 执行结果123456789101112131415161718192021222324252627282930313233[root@localhost stp]# stap -g socket.stpWARNING: Eliding unused function 'filter_http_transtime': identifier 'filter_http_transtime' at socket.stp:68:10 source: function filter_http_transtime:long (sk:long) ^WARNING: Eliding unused function 'get_conn_data': identifier 'get_conn_data' at :58:10 source: function get_conn_data:long (sk:long) ^WARNING: Eliding unused function 'get_conn_lifetime': identifier 'get_conn_lifetime' at :51:10 source: function get_conn_lifetime:long (sk:long) ^WARNING: Eliding unused function 'get_short_time': identifier 'get_short_time' at :22:10 source: function get_short_time:string() ^WARNING: Eliding unused function 'socket_state_num2str': identifier 'socket_state_num2str' at :104:10 source: function socket_state_num2str:string (state:long) ^sock:711 return [2017/6/20,14:42:35]local=0.0.0.0:0,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:16384sock:715 return [2017/6/20,14:42:35]local=0.0.0.0:0,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:32768tcp_v4_connect[2017/6/20,14:42:35]local=0.0.0.0:3000,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:32768tcp_v4_connect return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:SYN_SENT,sndbubf 1280:32768tcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDtcp_sendmsg[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768sk_stream_alloc_skb[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768sk_stream_alloc_skb return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 0:32768tcp_sendmsg return [2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHED,sndbubf 2304:32768tcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDtcp_send_ack[2017/6/20,14:42:35]local=192.168.55.178:3000,remote=180.97.33.108:80 state:ESTABLISHEDip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1583 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992ip_output:1581 return [2017/6/20,14:42:35]local=0.0.0.0:6,remote=0.0.0.0:0 state:CLOSE,sndbubf 0:212992","categories":[{"name":"linux kernel","slug":"linux-kernel","permalink":"http://vcpu.me/categories/linux-kernel/"}],"tags":[{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"http://vcpu.me/tags/socket/"},{"name":"systemtap","slug":"systemtap","permalink":"http://vcpu.me/tags/systemtap/"}]},{"title":"socket send","slug":"socketsend1","date":"2017-06-19T04:43:31.000Z","updated":"2017-06-19T04:43:31.000Z","comments":true,"path":"socketsend1/","link":"","permalink":"http://vcpu.me/socketsend1/","excerpt":"用户态发送函数列表1234567891011ssize_t send(int sockfd, const void *buf, size_t len, int flags);ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);int sendmmsg(int sockfd, struct mmsghdr *msgvec, unsigned int vlen, unsigned int flags); ssize_t write(int fd, const void *buf, size_t count);","text":"用户态发送函数列表1234567891011ssize_t send(int sockfd, const void *buf, size_t len, int flags);ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);int sendmmsg(int sockfd, struct mmsghdr *msgvec, unsigned int vlen, unsigned int flags); ssize_t write(int fd, const void *buf, size_t count); 发送函数之间差别 send 有连接协议发送数据使用，send第四个参数flags为0时候，等价于write send(sockfd, buf, len, 0) 等价 write（sockfd, buf, len） send是sendto一部分,send可被sendto替换 send(sockfd, buf, len, flags) 等价于 sendto(sockfd, buf, len, flags, NULL, 0) sendto 无连接和有连接发包都可以使用 sendmsg 可替换上树所有的发包函数 123456789struct msghdr &#123; void *msg_name; /* optional address */ socklen_t msg_namelen; /* size of address */ struct iovec *msg_iov; /* scatter/gather array */ size_t msg_iovlen; /* # elements in msg_iov */ void *msg_control; /* ancillary data, see below */ size_t msg_controllen; /* ancillary data buffer len */ int msg_flags; /* flags (unused) */ &#125;; /proc/sys/net/core/optmem_max可控制每个socket的msg_control大小 sendmsg不使用msg_flags参数 send发包过程概述 阻塞模式下 调用send函数时候，比较要发送数据和套接字发送缓冲区长度（net.ipv4.tcp_wmem）；如果发送缓冲区较小，函数直接返回SOCKET_ERR; 1234567891011121314151617181920if send_len &lt;= tcp_wmem&#123; if is sending&#123; wait if network err return SCOKET_ERR &#125; else&#123; if len &gt; tcp_wmem left&#123; wait if network err return SCOKET_ERR &#125; else&#123; copy data to tcp buf if copy err return SCOKET_ERR return copy data size &#125; &#125;&#125; 剩余缓冲区能容纳发送数据，则直接将数据拷贝到缓冲区中，send直接返回。如果剩余缓冲区不足，发送端阻塞等待，对端在协议栈层接收到数据后会发送ack确认，发送方接收到ack后释放缓冲区空间；如果此时剩余缓冲区大小可放置要发送数据，则直接将数据拷入缓冲区，返回。 Tips：阻塞模式下，数据发送正常，其返回的数据长度一定是发送数据的长度。 非阻塞模式下 send函数将数据拷入协议栈缓冲区，如果缓冲区不足，则send尽力拷贝，并返回拷贝大小；如果缓冲区满则返回-1，同时errno为EAGAIN，让发送端再次尝试发送数据。 发送缓冲区设置socklen_t sendbuflen = 0; socklen_t len = sizeof(sendbuflen); getsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len); printf(&quot;default,sendbuf:%d\\n&quot;, sendbuflen); sendbuflen = 10240; setsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, len); getsockopt(clientSocket, SOL_SOCKET, SO_SNDBUF, (void*)&amp;sendbuflen, &amp;len); printf(&quot;now,sendbuf:%d\\n&quot;, sendbuflen); send发包实例解析实际socket使用过程中，常用的是非阻塞模式，我们就以非阻塞模式为例进行分析，预设多种场景如下： 场景1：发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理数据，此时发送端因为10k数据发送完毕，关闭了socket。 场景分析： 发送端关闭socket，主动fin告诉对端发送端数据发送完毕想关闭TCP连接，发送完fin后发送端处于fin wait1状态等待接收端ack确认；发送端协议栈剩余8k数据依然在独立发送，待数据发送完成后，协议栈才会把fin发给接收端；接收端在接收ack完10k数据后，且收到fin信号后，接收端回复ack确认fin信号，两者协商关闭socket。 场景2：发送端预期发送10k数据，已将2k数据拷入缓冲区并实际发出拷入的2k数据（收到对端ack），接收端正在处理数据，此时发送端又发送了8k新数据；（缓冲区充足(8k新数据会被拷入缓冲区)情况我们不讨论）缓冲区不足时候会发生什么？ 场景分析 新发送的10k数据会尽力拷入缓冲区，send返回拷入缓冲区数据长度2k，如果此时缓冲区剩余空间为0时候，客户端强制send数据，会收到EAGAIN信号；其实这种情况客户端正确处理方式是读出缓冲区可写信号再发送数据，而不是自己进行发送尝试。 场景3:发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理接收到1k数据，处理完成后数据接收端关闭了socket，会发发生什么？ 场景分析 数据发送端有监听机制，数据发送端用户态会得到接收端端关闭信号（socket可读信号），这时候用户正确打开方式是调用close关闭socket 如果数据发送端未处理该关闭信号，且数据接收端没有rst强制关闭连接，数据发送端仍然可正常发送数据 如果数据发送端未处理该关闭信号，但是数据接收端已经rst强制关闭连接，数据发送端仍然在send发送数据，send将返回-1 如果是阻塞情况，但是因缓冲区满正在阻塞，如果接收端发送rst，阻塞发送端会退出阻塞返回，发送成功字节数，如果在此调用send，将返回-1 场景4：发送端10k数据已经安全放入缓冲区，已实际发出2k（收到对端ack），接收端正在处理接收到1k数据，此时网络出现异常 场景分析 接收应用程序在处理完已收到的1k数据后,会继续从缓存区读取余下的1k数据,然后就表现为无数据可读的现象,这种情况需要应用程序来处理超时.一般做法是设定一个select等待的最大时间,如果超出这个时间依然没有数据可读,则认为socket已不可用.发送应用程序会不断的将余下的数据发送到网络上,但始终得不到确认,所以缓存区的可用空间持续为0,这种情况也需要应用程序来处理.如果不由应用程序来处理这种情况超时的情况,也可以通过tcp协议本身来处理,具体可以查看sysctl项中的:net.ipv4.tcp_keepalive_intvlnet.ipv4.tcp_keepalive_probesnet.ipv4.tcp_keepalive_time send特点 send只是将数据放入缓冲区中，并不是真正已经发给对方 非阻塞发送字节可以是1-n，其发送多少完全依赖于剩余的发送缓冲区 socket发送函数解析发送流程图 sendsendtosendmmsgsendmsg 上述流程调用过程如下：-&gt;socketcall -&gt;sock_sendmsg -&gt; __sock_sendmsg -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg)-&gt;[tcp_prot]tcp_sendmsg 内核系统调用send 、sendto、sendmsg、sendmmsg发送函数由glibc提供，声明于/usr/include/sys/socket.h用户态在调用后会进入到sys_socketcall系统调用中，下面代码部分就是其入口1234567891011121314151617181920212223242526SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)&#123;... switch (call) &#123; ... case SYS_SEND: err = sys_send(a0, (void __user *)a1, a[2], a[3]); break; case SYS_SENDTO: err = sys_sendto(a0, (void __user *)a1, a[2], a[3], (struct sockaddr __user *)a[4], a[5]); break; ... case SYS_SENDMSG: err = sys_sendmsg(a0, (struct msghdr __user *)a1, a[2]); break; case SYS_SENDMMSG: err = sys_sendmmsg(a0, (struct mmsghdr __user *)a1, a[2], a[3]); break; ... default: err = -EINVAL; break; &#125; return err;&#125; send 是sendto的一种特殊情况,(sendto发送地址为NULL发送地址长度为0) 12345SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len, unsigned int, flags)&#123; return sys_sendto(fd, buff, len, flags, NULL, 0);&#125; sendto -&gt; sock_sendmsg -&gt; __sock_sendmsg -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg) 123456789101112SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len)&#123; ... err = sock_sendmsg(sock, &amp;msg, len);out_put: fput_light(sock-&gt;file, fput_needed);out: return err;&#125; sendmsg 和sendmmsg 完成用户态数据拷贝到内核态后，最终也是调用inet_sendmsg处理，在此就拿sendto情况详细分析 sendto源码实现分析sendto -&gt; sock_sendmsg -&gt; “sock_sendmsg” -&gt;”sock_sendmsg_nosec” -&gt; sock-&gt;ops-&gt;sendmsg(inet_sendmsg) 首先分析sock_sendmsg实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253int sock_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)&#123; struct kiocb iocb; struct sock_iocb siocb; int ret; /*异步IO控制块初始化*/ init_sync_kiocb(&amp;iocb, NULL); iocb.private = &amp;siocb; /*异步控制块调用完毕后，可调用__sock_sendmsg发送数据*/ ret = __sock_sendmsg(&amp;iocb, sock, msg, size); if (-EIOCBQUEUED == ret) ret = wait_on_sync_kiocb(&amp;iocb); return ret;&#125;static inline int __sock_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; int err = security_socket_sendmsg(sock, msg, size); /*调用__sock_sendmsg_nosec*/ return err ?: __sock_sendmsg_nosec(iocb, sock, msg, size);&#125;static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock_iocb *si = kiocb_to_siocb(iocb); si-&gt;sock = sock; si-&gt;scm = NULL; si-&gt;msg = msg; si-&gt;size = size; /*调用inet_sendnsg*/ return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);&#125;int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock *sk = sock-&gt;sk; sock_rps_record_flow(sk); /*如果连接没有分配本地端口且允许分配本地端口，我们就给连接绑定一个本地端口 */ /* We may need to bind the socket. */ if (!inet_sk(sk)-&gt;inet_num &amp;&amp; !sk-&gt;sk_prot-&gt;no_autobind &amp;&amp; inet_autobind(sk)) return -EAGAIN; /*传输层是TCP情况下，调用tcp_sendmsg()*/ return sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size);&#125; 其次分析inet_autobind ，获取可用端口并给，获取后的端口会赋值给inet-&gt;inet_sport/inet_num 1234567891011121314151617181920static int inet_autobind(struct sock *sk)&#123; struct inet_sock *inet; /* We may need to bind the socket. */ lock_sock(sk); inet = inet_sk(sk); if (!inet-&gt;inet_num) &#123; /*针对于TCP情况sk-&gt;sk_prot-&gt;get_port调用的是inet_csk_get_port * inet_csk_get_port工作获取端口，并将其赋值给inet-&gt;inet_num */ if (sk-&gt;sk_prot-&gt;get_port(sk, 0)) &#123; release_sock(sk); return -EAGAIN; &#125; /*获取inet-&gt;inet_num赋值给inet-&gt;inet_sport*/ inet-&gt;inet_sport = htons(inet-&gt;inet_num); &#125; release_sock(sk); return 0;&#125; 最后分析tcp_sendmsg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t size)&#123; struct iovec *iov; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; int iovlen, flags, err, copied = 0; int mss_now = 0, size_goal, copied_syn = 0, offset = 0; bool sg; long timeo; lock_sock(sk); flags = msg-&gt;msg_flags; if (flags &amp; MSG_FASTOPEN) &#123; err = tcp_sendmsg_fastopen(sk, msg, &amp;copied_syn, size); if (err == -EINPROGRESS &amp;&amp; copied_syn &gt; 0) goto out; else if (err) goto out_err; offset = copied_syn; &#125; /* * 获取数据发送超时时间 */ timeo = sock_sndtimeo(sk, flags &amp; MSG_DONTWAIT); /* Wait for a connection to finish. One exception is TCP Fast Open * (passive side) where data is allowed to be sent before a connection * is fully established. */ /* * TCP状态检查，ES和CLOSE_WAIT状态才能发送数据，其它状态都要等待连接建立起来 * 否则直接返回错误 * * 随着协议栈进步，增加一种情况tcp_passive_fastopen即tcp被动快速打开时候，不区分当前TCP处于状态 */ if (((1 &lt;&lt; sk-&gt;sk_state) &amp; ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &amp;&amp; !tcp_passive_fastopen(sk)) &#123; /*等待连接建立，连接建立成功则返回0*/ if ((err = sk_stream_wait_connect(sk, &amp;timeo)) != 0) goto do_error; &#125; /*开启repair功能处理*/ if (unlikely(tp-&gt;repair)) &#123; if (tp-&gt;repair_queue == TCP_RECV_QUEUE) &#123; copied = tcp_send_rcvq(sk, msg, size); goto out_nopush; &#125; err = -EINVAL; if (tp-&gt;repair_queue == TCP_NO_QUEUE) goto out_err; /* 'common' sending to sendq */ &#125; /**/ /* This should be in poll */ clear_bit(SOCK_ASYNC_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /*获取发送mss*/ mss_now = tcp_send_mss(sk, &amp;size_goal, flags); /* Ok commence sending. */ iovlen = msg-&gt;msg_iovlen;//应用层要发送数据块个数 iov = msg-&gt;msg_iov;//要发送数据地址 copied = 0;//已经放到缓冲区的数据长度 err = -EPIPE; /*在发送数据前，如果sk已经关闭或者出现err，直接返回 -EPIPE*/ if (sk-&gt;sk_err || (sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) goto out_err; /*网卡分散聚合*/ sg = !!(sk-&gt;sk_route_caps &amp; NETIF_F_SG); while (--iovlen &gt;= 0) &#123; /*获取用户态数据长度和数据指针并指向下一个用户态要发送数据块*/ size_t seglen = iov-&gt;iov_len; unsigned char __user *from = iov-&gt;iov_base; iov++; /*TCP fast open涉及*/ if (unlikely(offset &gt; 0)) &#123; /* Skip bytes copied in SYN */ if (offset &gt;= seglen) &#123; offset -= seglen; continue; &#125; seglen -= offset; from += offset; offset = 0; &#125; while (seglen &gt; 0) &#123; int copy = 0; int max = size_goal; /*从发送队列尾部取skb，尝试将用户态数据放入skb-&gt;data剩余空间*/ skb = tcp_write_queue_tail(sk); if (tcp_send_head(sk)) &#123; /*另一种mss情况，GSO*/ if (skb-&gt;ip_summed == CHECKSUM_NONE) max = mss_now; copy = max - skb-&gt;len; &#125; if (copy &lt;= 0) &#123;/*skb已经装满数据，后续会申请新的skb来发送数据*/new_segment: /* Allocate new segment. If the interface is SG, * allocate skb fitting to single page. */ if (!sk_stream_memory_free(sk)) goto wait_for_sndbuf; /*申请内存大小为select_size（线性数据区+协议头），申请失败或者不合法，睡眠等待*/ skb = sk_stream_alloc_skb(sk, select_size(sk, sg), sk-&gt;sk_allocation); if (!skb) goto wait_for_memory; /* * Check whether we can use HW checksum. * 检查释放网卡硬件释放可以计算校验和 */ if (sk-&gt;sk_route_caps &amp; NETIF_F_CSUM_MASK) skb-&gt;ip_summed = CHECKSUM_PARTIAL; /*将新分配的skb入sk_write_queue数据发送队列*/ skb_entail(sk, skb); copy = size_goal; max = size_goal; /* All packets are restored as if they have * already been sent. skb_mstamp isn't set to * avoid wrong rtt estimation. * TCP repair */ if (tp-&gt;repair) TCP_SKB_CB(skb)-&gt;sacked |= TCPCB_REPAIRED; &#125; /* Try to append data to the end of skb. */ if (copy &gt; seglen) copy = seglen; /* Where to copy to? */ /*如果数据还有线性区间，直接将数据拷入冰计算校验和*/ if (skb_availroom(skb) &gt; 0) &#123; /* We have some space in skb head. Superb! */ copy = min_t(int, copy, skb_availroom(skb)); err = skb_add_data_nocache(sk, skb, from, copy); if (err) goto do_fault; &#125; else &#123;/*如果没有了线性空间*/ /* * 数据会被复制到分页中 * */ bool merge = true; /*取得当前SKB的分片段数*/ int i = skb_shinfo(skb)-&gt;nr_frags; struct page_frag *pfrag = sk_page_frag(sk); /*检查分也可用空间，如果没有就申请新的页，如果系统内存不足就睡眠等待*/ if (!sk_page_frag_refill(sk, pfrag)) goto wait_for_memory; /*如果不能将数据最佳到最后一个分片*/ if (!skb_can_coalesce(skb, i, pfrag-&gt;page, pfrag-&gt;offset)) &#123; /*分页已经达到最大规格，将当前数据发出去，跳到new_segment重新申请skb*/ if (i == MAX_SKB_FRAGS || !sg) &#123; tcp_mark_push(tp, skb); goto new_segment; &#125; merge = false; &#125; copy = min_t(int, copy, pfrag-&gt;size - pfrag-&gt;offset); /*系统对发送缓冲区申请合法性判断*/ if (!sk_wmem_schedule(sk, copy)) goto wait_for_memory; /*拷贝用户空间数据，同时计算校验和，更新数据skb长度和缓存*/ err = skb_copy_to_page_nocache(sk, from, skb, pfrag-&gt;page, pfrag-&gt;offset, copy); if (err) goto do_error; /* Update the skb. */ /*最后一个分页可以放数据数据页被放入了，就更新分也大小记录*/ if (merge) &#123; skb_frag_size_add(&amp;skb_shinfo(skb)-&gt;frags[i - 1], copy); &#125; else &#123; /*如果不能分页就新增页，并初始化*/ skb_fill_page_desc(skb, i, pfrag-&gt;page, pfrag-&gt;offset, copy); get_page(pfrag-&gt;page); &#125; pfrag-&gt;offset += copy; &#125; /*如果复制数据长度为0，不用加PSH标记*/ if (!copied) TCP_SKB_CB(skb)-&gt;tcp_flags &amp;= ~TCPHDR_PSH; /*更新发送队列中最后一个序号，数据包的最后一个序号*/ tp-&gt;write_seq += copy; TCP_SKB_CB(skb)-&gt;end_seq += copy; skb_shinfo(skb)-&gt;gso_segs = 0; /*已经拷入了copy大小数据，用户态指针后移且更新已经拷贝数据增加*/ from += copy; copied += copy; /*所有数据处理完毕，直接退出*/ if ((seglen -= copy) == 0 &amp;&amp; iovlen == 0) goto out; /*如果skbb还可以继续填充数据或者是带外数据或者是有REPAIR选项，继续使用skb*/ if (skb-&gt;len &lt; max || (flags &amp; MSG_OOB) || unlikely(tp-&gt;repair)) continue; /*检查释放必须立即发送，即检查自上次发送后产生的数据是否已经超过对方通告过的最大接收窗口的一半。如果必须发送则设置紧急数据标示，然后将数据发出去*/ if (forced_push(tp)) &#123; tcp_mark_push(tp, skb); __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH); &#125; else if (skb == tcp_send_head(sk)) /*数据不必立即发送，且数据上只存在这段数据，则将这段数据发出*/ tcp_push_one(sk, mss_now); continue;wait_for_sndbuf:/*套接口缓冲区大小超过限制，此时无法再申请skb放数据，我们设置socket满标志*/ set_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags); /*系统内存不足处理*/wait_for_memory: /*skb分配失败了，已经拷入发送队列数据，直接调用tcp_push发出去 ～MSG_MORE表示无更多数据 TCP_NAGLE_PUSH 选项调用NAGLE，尽量减少小字节发送数据 */ if (copied) tcp_push(sk, flags &amp; ~MSG_MORE, mss_now, TCP_NAGLE_PUSH, size_goal); /*等待内存空闲，超过timeo时间后返回错误*/ if ((err = sk_stream_wait_memory(sk, &amp;timeo)) != 0) goto do_error; /*啊，内存来了，重新获取MSS和TSO，继续将用户态数据拷入缓冲区*/ mss_now = tcp_send_mss(sk, &amp;size_goal, flags); &#125; &#125;out: /*如果数据已经拷入发送队列，则立即发送*/ if (copied) tcp_push(sk, flags, mss_now, tp-&gt;nonagle, size_goal);out_nopush: release_sock(sk); return copied + copied_syn;do_fault: /*复制数据异常时才进入这里 * skb无负载数据，从发送队列上去除，并更新发送队列等参数*/ if (!skb-&gt;len) &#123; tcp_unlink_write_queue(skb, sk); /* It is the one place in all of TCP, except connection * reset, where we can be unlinking the send_head. */ tcp_check_send_head(sk, skb); sk_wmem_free_skb(sk, skb); &#125;do_error: /*如果已经复制了部分数据，即使发生了错误也可以发送，跳到out就是去发送数据去了*/ if (copied + copied_syn) goto out;out_err: err = sk_stream_error(sk, flags, err); release_sock(sk); return err;&#125; tcp_sendmsg()做了以下事情： 如果使用了TCP Fast Open，则会在发送SYN包的同时携带上数据。 如果连接尚未建立好，不处于ESTABLISHED或者CLOSE_WAIT状态， 那么进程进行睡眠，等待三次握手的完成。 获取当前的MSS、网络设备支持的最大数据长度size_goal。 如果支持GSO，size_goal会是MSS的整数倍。 遍历用户层的数据块数组： 4.1 获取发送队列的最后一个skb，如果是尚未发送的，且长度尚未达到size_goal，那么可以往此skb继续追加数据。 4.2 否则需要申请一个新的skb来装载数据。 4.2.1 如果发送队列的总大小sk_wmem_queued大于等于发送缓存的上限sk_sndbuf，或者发送缓存中尚未发送的数据量超过了用户的设置值： 设置同步发送时发送缓存不够的标志。 如果此时已有数据复制到发送队列了，就尝试立即发送。 等待发送缓存，直到sock有发送缓存可写事件唤醒进程，或者等待超时。 4.2.2 申请一个skb，其线性数据区的大小为：通过select_size()得到的线性数据区中TCP负荷的大小 + 最大的协议头长度。 如果申请skb失败了，或者虽然申请skb成功，但是从系统层面判断此次申请不合法， 等待可用内存，等待时间为2~202ms之间的一个随机数。 4.2.3 如果以上两步成功了，就更新skb的TCP控制块字段，把skb加入到sock发送队列的尾部，增加发送队列的大小，减小预分配缓存的大小。 4.3 接下来就是拷贝消息头中的数据到skb中了。如果skb的线性数据区还有剩余空间，就复制数据到线性数据区中，同时计算校验和。 4.4 如果skb的线性数据区已经用完了，那么就使用分页区： 4.4.1 检查分页是否有可用空间，如果没有就申请新的page。如果申请失败，说明系统内存不足。之后会设置TCP内存压力标志，减小发送缓冲区的上限，睡眠等待内存。 4.4.2 判断能否往最后一个分页追加数据。不能追加时，检查分页数是否达到了上限、或网卡不支持分散聚合。如果是的话，就为此skb设置PSH标志。 然后跳转到4.2处申请新的skb，来继续填装数据。 4.4.3 从系统层面判断此次分页发送缓存的申请是否合法。 4.4.4 拷贝用户空间的数据到skb的分页中，同时计算校验和。更新skb的长度字段，更新sock的发送队列大小和预分配缓存。 4.4.5 如果把数据追加到最后一个分页了，更新最后一个分页的数据大小。否则初始化新的分页。 4.5 拷贝成功后更新：发送队列的最后一个序号、skb的结束序号、已经拷贝到发送队列的数据量。 4.6 尽可能的将发送队列中的skb发送出去。 参考 http://blog.csdn.net/zhangskd/article/details/48207553","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"send","slug":"send","permalink":"http://vcpu.me/tags/send/"},{"name":"sendto","slug":"sendto","permalink":"http://vcpu.me/tags/sendto/"},{"name":"sendmsg","slug":"sendmsg","permalink":"http://vcpu.me/tags/sendmsg/"},{"name":"sendmmsg","slug":"sendmmsg","permalink":"http://vcpu.me/tags/sendmmsg/"}]},{"title":"centos环境下脚本执行顺序探究","slug":"centos脚本执行顺序","date":"2017-06-15T07:42:10.000Z","updated":"2017-06-15T07:42:10.000Z","comments":true,"path":"centos脚本执行顺序/","link":"","permalink":"http://vcpu.me/centos脚本执行顺序/","excerpt":"centos脚本执行顺序通用角度分析，centos 7 系统中存在如下以下5种常用的脚本路径/etc/rc.d/rc.local/etc/profile/etc/bashrc~/.bash_profile~/.bashrc 通过在除rc外的脚本中加入echo信息，reboot虚拟机并ssh登陆用户，打印出顺序如下I am etc profilei am etc bashrci am ~ bash rci am ~ bash profile","text":"centos脚本执行顺序通用角度分析，centos 7 系统中存在如下以下5种常用的脚本路径/etc/rc.d/rc.local/etc/profile/etc/bashrc~/.bash_profile~/.bashrc 通过在除rc外的脚本中加入echo信息，reboot虚拟机并ssh登陆用户，打印出顺序如下I am etc profilei am etc bashrci am ~ bash rci am ~ bash profile 脚本执行顺序和执行时机 脚本路径 执行顺序 执行时机 /etc/rc.d/rc.local 1 系统起机执行一次，后续均不执行 /etc/profile 2 ssh/su/界面登陆时执行 /etc/bashrc 3 ssh/su/界面登陆时执行 ~/.bash_profile 4 ssh/su/界面登陆以当前用户身份登陆 ~/.bashrc 5 ssh/su/界面登陆以当前用户身份登陆 脚本含义rc.local脚本centos启动时候执行脚本，可以用作默认启动/etc/profile和/etc/bashrc 属于系统的全局变量设置~/profile和~/bashrc 属于给予某一个用户的变量设置 profile和bashrc区别 profile 是用户唯一用来设置环境变量的地方，因为用户可能有多种shell（bash、sh、zsh），环境变量没有必要在每种shell都初始化，只需要统一初始化就行，很显然，profile就是这样的地方 bashrc 是专门给bash做初始化设置的，相对应来讲，其它shell会有专门的shrc、zshrc文件存放 开机启动脚本其它说明centos7 默认是没有执行权限的，想在此处加执行脚本，执行脚本不会执行到，需要增添执行权限 123456[root@localhost rc.d]# ls -alt rc.local-rw-r--r--. 1 root root 491 Jun 13 22:24 rc.local[root@localhost rc.d]# chmod +x rc.local[root@localhost qinlong]# ls -alt /etc/rc.d/rc.local-rwxr-xr-x. 1 root root 535 Jun 13 22:48 /etc/rc.d/rc.local","categories":[{"name":"centos","slug":"centos","permalink":"http://vcpu.me/categories/centos/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://vcpu.me/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://vcpu.me/tags/centos/"}]},{"title":"f-stack安装运行","slug":"f-stack安装","date":"2017-06-12T09:50:55.000Z","updated":"2017-07-04T06:38:48.000Z","comments":true,"path":"f-stack安装/","link":"","permalink":"http://vcpu.me/f-stack安装/","excerpt":"mac os virtual box跑f-stack环境搭建基本环境kernel版本3.10.0-514.el7.x86_64CentOS-7-x86_64-Minimal-1611.isoMac osxOreacle Virtual Box5.1.22kernel-devel 操作步骤在Oreacle Virtual Box上安装centos 7虚拟机配置开启NAT网卡和桥接网卡 网卡配置芯片类型选择Add two more virtual network adapters with “Intel PRO/1000 MT Server (82545EM)” type in order to provide virtual network hardware to the virtual machine that is supported by Intel DPDK. 上述说明来自于一篇国外文档说明，使用的Inetl 82545EM，但是经过尝试Intel PRO/1000MT桌面(82540EN)也是可以的。所以不要太迷信啦。","text":"mac os virtual box跑f-stack环境搭建基本环境kernel版本3.10.0-514.el7.x86_64CentOS-7-x86_64-Minimal-1611.isoMac osxOreacle Virtual Box5.1.22kernel-devel 操作步骤在Oreacle Virtual Box上安装centos 7虚拟机配置开启NAT网卡和桥接网卡 网卡配置芯片类型选择Add two more virtual network adapters with “Intel PRO/1000 MT Server (82545EM)” type in order to provide virtual network hardware to the virtual machine that is supported by Intel DPDK. 上述说明来自于一篇国外文档说明，使用的Inetl 82545EM，但是经过尝试Intel PRO/1000MT桌面(82540EN)也是可以的。所以不要太迷信啦。 桥接网卡用来ssh登陆管理串口，NAT网卡用来运行DPDK驱动，跑nginx 在Virtual Box上制作地址映射 centos7虚拟机上网口配置信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s17TYPE=\"Ethernet\"BOOTPROTO=\"dhcp\"DEFROUTE=\"yes\"PEERDNS=\"yes\"PEERROUTES=\"yes\"IPV4_FAILURE_FATAL=\"no\"IPV6INIT=\"yes\"IPV6_AUTOCONF=\"yes\"IPV6_DEFROUTE=\"yes\"IPV6_PEERDNS=\"yes\"IPV6_PEERROUTES=\"yes\"IPV6_FAILURE_FATAL=\"no\"IPV6_ADDR_GEN_MODE=\"stable-privacy\"NAME=\"enp0s17\"UUID=\"2ea1ed66-7bcd-4153-a495-39c25d5f0ff9\"DEVICE=\"enp0s17\"ONBOOT=\"yes\"[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp0s8TYPE=EthernetBOOTPROTO=dhcpDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8UUID=6c930d05-bc17-4316-998e-f01a7233cbd3DEVICE=enp0s8ONBOOT=yes[root@localhost ~]# ifconfigenp0s8: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.55.183 netmask 255.255.255.0 broadcast 192.168.55.255 inet6 fe80::f97d:539:4010:eaff prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:d5:ee:00 txqueuelen 1000 (Ethernet) RX packets 521 bytes 58437 (57.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 155 bytes 23680 (23.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0enp0s17: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255 inet6 fe80::a15d:3b87:fec0:f3c1 prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:28:39:6c txqueuelen 1000 (Ethernet) RX packets 2 bytes 1180 (1.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 10 bytes 1308 (1.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0``` #### 开启CPU flags(SSE 4.1/SSE 4.2)```shellVBoxManage setextradata \"VM name\" VBoxInternal/CPUM/SSE4.1 1VBoxManage setextradata \"VM name\" VBoxInternal/CPUM/SSE4.2 1 Note: 上述CPU flags默认是不开启的，启动f-stack上ngx会err日志如下 如果不开启sse cpu选项，在启动ngx会报如下问题/usr/local/nginx_fstack/sbin/nginx /data/f-stack/config.ini -c 1 –proc-type=primary –num-procs=1 –proc-id=0ERROR: This system does not support “SSE4_1”.Please check that RTE_MACHINE is set correctly. 做NAT网卡10.0.2.15需要在vbox上做主机地址和端口映射才能访问 安装详细步骤1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253yum install -y git gcc openssl-devel bccd /datayum install kernel-devel-`uname -r` -ymkdir /data/f-stackgit clone https://github.com/F-Stack/f-stack.git /data/f-stack# Compile DPDKcd /data/f-stack/dpdkmake config T=x86_64-native-linuxapp-gccmake# set hugepage echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesmkdir /mnt/hugemount -t hugetlbfs nodev /mnt/huge# insmod komodprobe uioinsmod build/kmod/igb_uio.koinsmod build/kmod/rte_kni.ko# set ip addressexport myaddr=`ifconfig enp0s17 | grep \"inet\" | grep -v \":\" | awk -F ' ' '&#123;print $2&#125;'`export mymask=`ifconfig enp0s17 | grep \"netmask\" | awk -F ' ' '&#123;print $4&#125;'`export mybc=`ifconfig enp0s17 | grep \"broadcast\" | awk -F ' ' '&#123;print $6&#125;'`export myhw=`ifconfig enp0s17 | grep \"ether\" | awk -F ' ' '&#123;print $2&#125;'`export mygw=`route -n | grep 0.0.0.0 | grep enp0s17 | grep UG | awk -F ' ' '&#123;print $2&#125;'`sed \"s/addr=192.168.1.2/addr=$&#123;myaddr&#125;/\" -i /data/f-stack/config.inised \"s/netmask=255.255.255.0/netmask=$&#123;mymask&#125;/\" -i /data/f-stack/config.inised \"s/broadcast=192.168.1.255/broadcast=$&#123;mybc&#125;/\" -i /data/f-stack/config.inised \"s/gateway=192.168.1.1/gateway=$&#123;mygw&#125;/\" -i /data/f-stack/config.ini# Compile F-Stack libexport FF_PATH=/data/f-stackexport FF_DPDK=/data/f-stack/dpdk/buildcd /data/f-stack/libmake# Compile Nginxcd ../app/nginx-1.11.10./configure --prefix=/usr/local/nginx_fstack --with-ff_modulemakemake install# offload NIC（if there is only one NIC，the follow commands must run in a script）ifconfig enp0s17 downpython /data/f-stack/dpdk/tools/dpdk-devbind.py --bind=igb_uio enp0s17# start Nginxcd ../.../start.sh -b /usr/local/nginx_fstack/sbin/nginx -c config.ini 测试在vbox主机上访问映射地址和端口192.168.55.165:8080 -&gt; 10.0.2.15:80curl http://192.168.55.165:808012345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 附录f-stack ngx配置文件/data/f-stack/config.ini123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[dpdk]## Hexadecimal bitmask of cores to run on.lcore_mask=3## Port mask, enable and disable ports.## Default: all ports are enabled.#port_mask=1channel=4## Number of ports.nb_ports=1promiscuous=1numa_on=1## TCP segment offload, default: disabled.tso=0## Port config section## According to dpdk.nb_ports: port0, port1...[port0]addr=10.0.2.15netmask=255.255.255.0broadcast=10.0.2.255gateway=10.0.2.2## Packet capture path, this will hurt performance#pcap=./a.pcap## Kni config: if enabled and method=reject,## all packets that do not belong to the following tcp_port and udp_port## will transmit to kernel; if method=accept, all packets that belong to## the following tcp_port and udp_port will transmit to kernel.#[kni]#enable=1#method=reject#tcp_port=80,443#udp_port=53## FreeBSD network performance tuning configurations.## Most native FreeBSD configurations are supported.[freebsd.boot]hz=100kern.ipc.maxsockets=262144net.inet.tcp.syncache.hashsize=4096net.inet.tcp.syncache.bucketlimit=100net.inet.tcp.tcbhashsize=65536[freebsd.sysctl]kern.ipc.somaxconn=32768kern.ipc.maxsockbuf=16777216net.inet.tcp.fast_finwait2_recycle=1net.inet.tcp.sendspace=16384net.inet.tcp.recvspace=8192net.inet.tcp.nolocaltimewait=1net.inet.tcp.cc.algorithm=htcpnet.inet.tcp.sendbuf_max=16777216net.inet.tcp.recvbuf_max=16777216net.inet.tcp.sendbuf_auto=1net.inet.tcp.recvbuf_auto=1net.inet.tcp.sendbuf_inc=16384net.inet.tcp.recvbuf_inc=524288net.inet.tcp.inflight.enable=0net.inet.tcp.sack=1net.inet.tcp.blackhole=1net.inet.tcp.msl=2000net.inet.tcp.delayed_ack=0net.inet.udp.blackhole=1net.inet.ip.redirect=0 f-stack ngx正常启动信息[root@localhost f-stack]# ./start.sh -b /usr/local/nginx_fstack/sbin/nginx -c config.ini/usr/local/nginx_fstack/sbin/nginx config.ini -c 1 –proc-type=primary –num-procs=1 –proc-id=0 EAL: Detected 1 lcore(s)EAL: Probing VFIO support…EAL: PCI device 0000:00:08.0 on NUMA socket -1EAL: probe driver: 8086:100f rte_em_pmdEAL: PCI device 0000:00:11.0 on NUMA socket -1EAL: probe driver: 8086:100f rte_em_pmdcreate mbuf pool on socket 0create ring:arp_ring_0_0 success, 2047 ring entries are now free!Port 0 MAC: 08 00 27 28 39 6cTSO is disabledset port 0 to promiscuous mode ok Checking link status………………..donePort 0 Link Up - speed 1000 Mbps - full-duplexlink_elf_lookup_symbol: missing symbol hash tablelink_elf_lookup_symbol: missing symbol hash tablenetisr_init: forcing maxthreads from 1 to 0Timecounters tick every 10.000 msecTimecounter “ff_clock” frequency 100 Hz quality 1f-stack-0: Ethernet address: 08:00:27:28:39:6c f-stack 环境安装完成后重启后应该重新设置的参数下述命令可放入/etc/rd.c/rd.local启动文件中，在机器重启后执行一次下面环境设置12345678echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesmount -t hugetlbfs nodev /mnt/hugemodprobe uioinsmod /data/f-stack/dpdk/build/kmod/igb_uio.koinsmod /data/f-stack/dpdk/build/kmod/rte_kni.koifconfig enp0s17 downpython /data/f-stack/dpdk/tools/dpdk-devbind.py --bind=igb_uio enp0s17/data/f-stack/start.sh -b /usr/local/nginx_fstack/sbin/nginx -c /data/f-stack/config.ini 如果kernel-devel yum无法找到内核对应版本可去centos官网查找下载wget https://buildlogs.centos.org/c7.1511.00/kernel/20151119220809/3.10.0-327.el7.x86_64/kernel-devel-3.10.0-327.el7.x86_64.rpmrpm -ivh kernel-devel-3.10.0-327.el7.x86_64.rpm","categories":[{"name":"DPDK","slug":"DPDK","permalink":"http://vcpu.me/categories/DPDK/"}],"tags":[{"name":"DPDK","slug":"DPDK","permalink":"http://vcpu.me/tags/DPDK/"},{"name":"f-stack","slug":"f-stack","permalink":"http://vcpu.me/tags/f-stack/"},{"name":"nginx","slug":"nginx","permalink":"http://vcpu.me/tags/nginx/"}]},{"title":"bind()实现源码分析","slug":"bind","date":"2017-06-12T09:35:01.000Z","updated":"2017-06-12T09:35:01.000Z","comments":true,"path":"bind/","link":"","permalink":"http://vcpu.me/bind/","excerpt":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。","text":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。 sys_bind12345678910111213141516171819202122SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (sock) &#123; err = move_addr_to_kernel(umyaddr, addrlen, &amp;address); if (err &gt;= 0) &#123; err = security_socket_bind(sock, (struct sockaddr *)&amp;address, addrlen); if (!err) err = sock-&gt;ops-&gt;bind(sock, (struct sockaddr *) &amp;address, addrlen);//inet_bind &#125; fput_light(sock-&gt;file, fput_needed); &#125; return err;&#125; sockfd_lookup_light 和move_addr_to_kernel分别为根据fd从当前进程取出socket和把参数从用户空间考入地址空间 bind系统调用最重要函数为sock-&gt;ops-&gt;bind 在TCP协议情况下inet_stream_ops中bind成员函数为inet_bind 后续为对此函数的分析 inet_bind实现较为复杂，现在版本和原始版本相比，支持端口复用了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123; struct sockaddr_in *addr = (struct sockaddr_in *)uaddr; struct sock *sk = sock-&gt;sk; struct inet_sock *inet = inet_sk(sk); struct net *net = sock_net(sk); unsigned short snum; int chk_addr_ret; int err; /* If the socket has its own bind function then use it. (RAW) */ /*raw socket才会用到，tcp_proc无此函数*/ if (sk-&gt;sk_prot-&gt;bind) &#123; err = sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len); goto out; &#125; err = -EINVAL; /*地址长度检验*/ if (addr_len &lt; sizeof(struct sockaddr_in)) goto out; /*bind地址中协议检查，必须是下面两种情况 * 1.绑定的地址协议为AF_INET * 2.绑定协议为0（AF_UNSPEC）同时地址也为0 * 否则直接退出inet_bind ,返回地址不支持错误码 */ if (addr-&gt;sin_family != AF_INET) &#123; /* Compatibility games : accept AF_UNSPEC (mapped to AF_INET) * only if s_addr is INADDR_ANY. */ err = -EAFNOSUPPORT; if (addr-&gt;sin_family != AF_UNSPEC || addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY)) goto out; &#125; /*获取根据IP地址得出地址类型 RTN_LOCAL 本机地址 RTN_MULTICAST 多播 RTN_BROADCAST 广播 RTN_UNICAST */ chk_addr_ret = inet_addr_type(net, addr-&gt;sin_addr.s_addr); /* Not specified by any standard per-se, however it breaks too * many applications when removed. It is unfortunate since * allowing applications to make a non-local bind solves * several problems with systems using dynamic addressing. * (ie. your servers still start up even if your ISDN link * is temporarily down) */ err = -EADDRNOTAVAIL; /* 地址类型必须是本机，多播，组播中的一个，否则直接返回，报地址参数异常 * */ if (!net-&gt;ipv4_sysctl_ip_nonlocal_bind &amp;&amp; !(inet-&gt;freebind || inet-&gt;transparent) &amp;&amp; addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY) &amp;&amp; chk_addr_ret != RTN_LOCAL &amp;&amp; chk_addr_ret != RTN_MULTICAST &amp;&amp; chk_addr_ret != RTN_BROADCAST) goto out; snum = ntohs(addr-&gt;sin_port); err = -EACCES; /* * 要绑定的端口小于1024时候，要求运行该应用程序的为超级权限 * 否则返回并报权限不运行的错误 */ if (snum &amp;&amp; snum &lt; PROT_SOCK &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_BIND_SERVICE)) goto out; /* We keep a pair of addresses. rcv_saddr is the one * used by hash lookups, and saddr is used for transmit. * * In the BSD API these are the same except where it * would be illegal to use them (multicast/broadcast) in * which case the sending device address is used. */ lock_sock(sk); /* Check these errors (active socket, double bind). */ err = -EINVAL; /*bind动作发生在最初状态，其TCP状态是CLOSE且没有绑定过 * 否则直接判别为异常 */ if (sk-&gt;sk_state != TCP_CLOSE || inet-&gt;inet_num) goto out_release_sock; /*inet_rcv_saddr 用作hash表查找使用 *inet_saddr作为发包源地址 *当为广播和组播时候发送地址为0 */ inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr; if (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST) inet-&gt;inet_saddr = 0; /* Use device */ /* Make sure we are allowed to bind here. */ /* TCP时候该函数负责查询该端口是否被使用，没有被使用返回0，否则返回非0 *如果已经被使用，则退出bind函数，并返回地址和端口已经被使用错误-EADDRINUSE *sk-&gt;sk_prot-&gt;get_port= inet_csk_get_port */ if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) &#123; inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0; err = -EADDRINUSE; goto out_release_sock; &#125; /* * 更新sk-&gt;sk_userlocks标记，表明本地地址和端口已经绑定 */ if (inet-&gt;inet_rcv_saddr) sk-&gt;sk_userlocks |= SOCK_BINDADDR_LOCK; if (snum) sk-&gt;sk_userlocks |= SOCK_BINDPORT_LOCK; inet-&gt;inet_sport = htons(inet-&gt;inet_num); inet-&gt;inet_daddr = 0; inet-&gt;inet_dport = 0; sk_dst_reset(sk); err = 0;out_release_sock: release_sock(sk);out: return err;&#125;EXPORT_SYMBOL(inet_bind); 绑定地址长度和协议检查 长度异常返回-EINVAL 表示参数异常，协议不支持 -EAFNOSUPPORT 对绑定地址进行类型检查inet_addr_type，必须是本机地址，组播和广播地址类型 -EADDRNOTAVAIL 否则报地址参数异常 如果端口小于1024 ，必须为超级权限ns_capable 否则 err = -EACCES 权限不允许 sk-&gt;sk_prot-&gt;get_port = inet_csk_get_port 四层端口检查，看是否被使用 更新sk-&gt;skuserlocks标记，代表地址和端口已经被绑定 扩展函数： inet_csk_get_port TCP四层端口检查 inet_addr_type 地址类型判别 ns_capable 超级权限检查 inet_csk_get_port123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202int inet_csk_get_port(struct sock *sk, unsigned short snum)&#123; struct inet_hashinfo *hashinfo = sk-&gt;sk_prot-&gt;h.hashinfo; struct inet_bind_hashbucket *head; struct inet_bind_bucket *tb; int ret, attempts = 5; struct net *net = sock_net(sk); int smallest_size = -1, smallest_rover; kuid_t uid = sock_i_uid(sk); int attempt_half = (sk-&gt;sk_reuse == SK_CAN_REUSE) ? 1 : 0; /*禁止上下半部，防止进程冲突*/ local_bh_disable(); /* * 如果没有bind端口 */ if (!snum) &#123;/*没有指定端口会自动选择端口*/ int remaining, rover, low, high;again: /*获取端口的取值范围*/ inet_get_local_port_range(net, &amp;low, &amp;high);/*后文辉对其进行分析*/ if (attempt_half) &#123; int half = low + ((high - low) &gt;&gt; 1); if (attempt_half == 1) high = half; else low = half; &#125; /*取值范围内端口数*/ remaining = (high - low) + 1; /*随机选择端口*/ smallest_rover = rover = net_random() % remaining + low; smallest_size = -1; do &#123; /*保留端口检查,服务端可以设置 /proc/sys/net/ipv4/ip_local_reserved_ports */ if (inet_is_reserved_local_port(rover)) goto next_nolock;/*端口加1继续*/ /*根据端口号和HASHsize从确定hash桶，并锁住它，后续便利查找*/ head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, rover, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == rover) &#123; /*判断端口是否可以复用，如果可以复用即使在链表中也一样复用*/ if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; (tb-&gt;num_owners &lt; smallest_size || smallest_size == -1)) &#123; /*记录下端口的使用个数和端口*/ smallest_size = tb-&gt;num_owners; smallest_rover = rover; /*系统绑定端口已经超过最大端口数了，要去检查inet_csk_bind_conflict绑定是否存在冲突*/ if (atomic_read(&amp;hashinfo-&gt;bsockets) &gt; (high - low) + 1 &amp;&amp; !inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; /*ok，绑定没有冲突*/ snum = smallest_rover; goto tb_found; &#125; &#125; /*端口冲突检查*/ if (!inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; snum = rover; goto tb_found; &#125; /*此端口在链表中但是不能复用，继续下一个*/ goto next; &#125; break;/*不在bind表中，端口可以使用，直接跳出循环*/ next: spin_unlock(&amp;head-&gt;lock); next_nolock: /*已经找到最大端口了，从最小开始找*/ if (++rover &gt; high) rover = low; &#125; while (--remaining &gt; 0);/*en,最大5次查找机会*/ /* Exhausted local port range during search? It is not * possible for us to be holding one of the bind hash * locks if this test triggers, because if 'remaining' * drops to zero, we broke out of the do/while loop at * the top level, not from the 'break;' statement. */ ret = 1; /*没有找到端口，那就最后一次机会*/ if (remaining &lt;= 0) &#123; if (smallest_size != -1) &#123; snum = smallest_rover; goto have_snum; &#125; if (attempt_half == 1) &#123; /* OK we now try the upper half of the range */ attempt_half = 2; goto again; &#125; goto fail; &#125; /* OK, here is the one we will use. HEAD is * non-NULL and we hold it's mutex. */ /*找到可用的端口了*/ snum = rover; &#125; else &#123; /*指定绑定了端口，在绑定的链表中查找，如果查找到，代表已经被绑定*/have_snum: head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, snum, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == snum) goto tb_found;/*端口已经被绑定*/ &#125; /*在绑定链表中没有发现，后续会创建*/ tb = NULL; goto tb_not_found; tb_found: if (!hlist_empty(&amp;tb-&gt;owners)) &#123; /*要bind的sk标记SK_FORCE_REUSE可以强制复用*/ if (sk-&gt;sk_reuse == SK_FORCE_REUSE) goto success; if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size == -1) &#123; /* 是否可以复用的判别 * fastreuseport Google添加选项&amp;&amp; 已经开启端口复用 &amp;&amp; 当前socket uid和查找到的uid相符合 * 当前socket也可以放到bind hash中，后续会将其加入 */ goto success; &#125; else &#123; ret = 1; /*端口绑定冲突，自动分配端口绑定冲突会走到此处，在自动分配端口时候进行了下列类似判别 *所以此判断基本不会执行知道跳到tb_not_found这个时候tb不为null的 */ if (inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, true)) &#123; if (((sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size != -1 &amp;&amp; --attempts &gt;= 0) &#123; spin_unlock(&amp;head-&gt;lock); goto again; &#125; goto fail_unlock; &#125; &#125; &#125;tb_not_found: ret = 1; /*绑定时没有发现过tb，直接创建一个*/ if (!tb &amp;&amp; (tb = inet_bind_bucket_create(hashinfo-&gt;bind_bucket_cachep, net, head, snum)) == NULL) goto fail_unlock; if (hlist_empty(&amp;tb-&gt;owners)) &#123;/*没有绑定过socket*/ if (sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) tb-&gt;fastreuse = 1; else tb-&gt;fastreuse = 0; /*设置了SO_REUSEPORT选项*/ if (sk-&gt;sk_reuseport) &#123; tb-&gt;fastreuseport = 1; tb-&gt;fastuid = uid; &#125; else tb-&gt;fastreuseport = 0; &#125; else &#123;/*如果绑定过socket*/ if (tb-&gt;fastreuse &amp;&amp; (!sk-&gt;sk_reuse || sk-&gt;sk_state == TCP_LISTEN)) tb-&gt;fastreuse = 0; if (tb-&gt;fastreuseport &amp;&amp; (!sk-&gt;sk_reuseport || !uid_eq(tb-&gt;fastuid, uid))) tb-&gt;fastreuseport = 0; &#125;success:/*找到可用端口，添加绑定表*/ if (!inet_csk(sk)-&gt;icsk_bind_hash) inet_bind_hash(sk, tb, snum);/*sk被放到tb-&gt;owners中*/ WARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb); ret = 0;fail_unlock: spin_unlock(&amp;head-&gt;lock);fail: local_bh_enable(); return ret;&#125; 如果端口为0；则自动选取端口选择过程如下： 先在[low,half] or [half,high]中随机选取一个端口，作为循环获取端口的起始端口，开始以下流程 步骤1: 保留端口检查，不满足，端口加1，重试次数减1，继续从步骤1开始 步骤2: 从当前端口映射的hash桶中取出列表头，遍历检查该端口是否被使用 步骤2-1:没有被使用，直接退出循环，tb为NULL，创建tb，跳转到tb_not_found将该端口连同创建的tb加入该hash桶的链表中，sk也被放到tb-&gt;owners中管理，结束退出 步骤2-2: 端口被使用了，检查端口使用是否冲突 步骤2-2-1:没有冲突，推出循环，跳转到tb_found,复用检查成功，sk被放到tb-&gt;owners中，结束退出 步骤2-2-2:存在冲突，直接端口+1，继续循环查找 步骤3:如果上半部分已经查找完毕，继续[half,high]中选择一个端口，进行步骤1 attempt_halfsk-&gt;sk_reuse == SK_CAN_REUSE 取端口范围 [low ,half]否则 取端口范围 [half,high] 该值会影响上述选择端口的流程从上半端还是从下半端选择端口 如果sk-&gt;sk_reuse被置SK_CAN_REUSE标记则先从下半端开始选择端口 否则直接从上半端选择端口 small_size和small_rover what’s the fuck!!! 疑惑了好久small_size和small_rover在3.10的版本中根本就没有使用基本用不到3.10版本的端口查找原则是确定端口查找区间，随机选择端口，只要该端口能复用就直接使用，已经完全去除了优先选择复用端口数较小的端口这一原则了（3.2kernel）So amazing！这两个变量可以去除了 inet_get_local_port_range1234567891011void inet_get_local_port_range(struct net *net, int *low, int *high)&#123; unsigned int seq; do &#123; seq = read_seqbegin(&amp;net-&gt;ipv4_sysctl_local_ports.lock); *low = net-&gt;ipv4_sysctl_local_ports.range[0]; *high = net-&gt;ipv4_sysctl_local_ports.range[1]; &#125; while (read_seqretry(&amp;net-&gt;ipv4_sysctl_local_ports.lock, seq));&#125; 12sysctl -a|grep ip_local_port_rangenet.ipv4.ip_local_port_range = 32768 60999 上述读取端口范围是用户态的ip_local_port_range，默认是3w多以后的，可以调整此参数扩大端口范围 上述read_seqbegin这种方式读取数据，是一种顺序锁，适用于读多写少的方式用方式，后续专门处博文研究 tcp端口冲突检查inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict1234567891011121314151617181920212223242526272829303132333435const struct inet_connection_sock_af_ops ipv4_specific = &#123; .queue_xmit = ip_queue_xmit, .send_check = tcp_v4_send_check, .rebuild_header = inet_sk_rebuild_header, .sk_rx_dst_set = inet_sk_rx_dst_set, .conn_request = tcp_v4_conn_request, .syn_recv_sock = tcp_v4_syn_recv_sock, .net_header_len = sizeof(struct iphdr), .setsockopt = ip_setsockopt, .getsockopt = ip_getsockopt, .addr2sockaddr = inet_csk_addr2sockaddr, .sockaddr_len = sizeof(struct sockaddr_in), .bind_conflict = inet_csk_bind_conflict,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_ip_setsockopt, .compat_getsockopt = compat_ip_getsockopt,#endif .mtu_reduced = tcp_v4_mtu_reduced,&#125;;static int tcp_v4_init_sock(struct sock *sk)&#123; struct inet_connection_sock *icsk = inet_csk(sk); tcp_init_sock(sk); icsk-&gt;icsk_af_ops = &amp;ipv4_specific;#ifdef CONFIG_TCP_MD5SIG tcp_sk(sk)-&gt;af_specific = &amp;tcp_sock_ipv4_specific;#endif return 0;&#125; 从上文得知inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict 函数是inet_csk_bind_conflict af_ops在tcp_v4_init_sock初始化 inet_csk_bind_conflict分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960int inet_csk_bind_conflict(const struct sock *sk, const struct inet_bind_bucket *tb, bool relax)&#123; struct sock *sk2; int reuse = sk-&gt;sk_reuse; int reuseport = sk-&gt;sk_reuseport; kuid_t uid = sock_i_uid((struct sock *)sk); /* * Unlike other sk lookup places we do not check * for sk_net here, since _all_ the socks listed * in tb-&gt;owners list belong to the same net - the * one this bucket belongs to. */ sk_for_each_bound(sk2, &amp;tb-&gt;owners) &#123; /*不会冲突情况1:socket绑定设备不同*/ if (sk != sk2 &amp;&amp; !inet_v6_ipv6only(sk2) &amp;&amp; (!sk-&gt;sk_bound_dev_if || !sk2-&gt;sk_bound_dev_if || sk-&gt;sk_bound_dev_if == sk2-&gt;sk_bound_dev_if)) &#123; /* *不会冲突情况2:地址不同 */ if ((!reuse || !sk2-&gt;sk_reuse || sk2-&gt;sk_state == TCP_LISTEN) &amp;&amp; (!reuseport || !sk2-&gt;sk_reuseport || (sk2-&gt;sk_state != TCP_TIME_WAIT &amp;&amp; !uid_eq(uid, sock_i_uid(sk2))))) &#123; /* * 不会冲突情况3: * 条件A: (reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state ！= TCP_LISTEN) * 条件B：(reuseport * &amp;&amp; sk2-&gt;sk_reuseport * &amp;&amp;(sk2-&gt;sk_state == TCP_TIME_WAIT || uid_eq(uid, sock_i_uid(sk2)))) * 条件A和条件B只要有一个成立，就不会冲突 * 条件A成立条件： * 链上sock和待检查sock开启地址复用且链上状态不是监听状态 * 条件B成立条件： * 链上sock和待检查sock开启端口复用且链表上状态为TW * 链上sock和待检查sock开启端口复用且两个sock的uid相同 */ if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; /*没有开启relax，要绑定方不能复用，已绑定方不能复用，以绑定方处理监听状态*/ if (!relax &amp;&amp; reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state != TCP_LISTEN) &#123; if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; &#125; &#125; return sk2 != NULL;&#125; 在端口自动选择时可以重用端口条件为： a设备不同b绑定ip地址不同c要绑定sock和已绑定sock地址允许重用，且已绑定socket不处于监听状态d 链上sock和待检查sock开启端口复用且链表上状态为TWe 链上sock和待检查sock开启端口复用且两个sock的uid相同 关于条件c的补充条件：即使c满足，也需要看relax的值确定，relax为TRUE时可复用，为fase时候不能复用 自动端口时候relax为false，所以条件c消失，仅仅剩下a、b、d、e四个条件","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"bind","slug":"bind","permalink":"http://vcpu.me/tags/bind/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"}]},{"title":"connect()实现源码分析","slug":"connect","date":"2017-06-09T09:30:34.000Z","updated":"2017-06-26T03:33:43.000Z","comments":true,"path":"connect/","link":"","permalink":"http://vcpu.me/connect/","excerpt":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 返回值: -1 失败 strerror(errno)可帮助获取失败原因 常见失败原因有： ETIMEOUT Connection timed out服务端一直未回复syn ack，尝试多次syn后返回 ECONNREFUSED Connection refused 服务端端口没有开启，回复rst EHOSTUNREACH No route to host 服务端在同局域网内arp请求获取办不到对方mac 0 成功 用法:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr)","text":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 返回值: -1 失败 strerror(errno)可帮助获取失败原因 常见失败原因有： ETIMEOUT Connection timed out服务端一直未回复syn ack，尝试多次syn后返回 ECONNREFUSED Connection refused 服务端端口没有开启，回复rst EHOSTUNREACH No route to host 服务端在同局域网内arp请求获取办不到对方mac 0 成功 用法:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr) 实例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#define SERVER_PORT 20000void usage(char *name)&#123; printf(\"usage: %s IP\\n\", name);&#125;int main(int argc, char **argv)&#123; int server_fd, client_fd, length = 0; struct sockaddr_in server_addr, client_addr; socklen_t socklen = sizeof(server_addr); if(argc &lt; 2) &#123; usage(argv[0]); exit(1); &#125; if((client_fd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0) &#123; printf(\"create socket error, exit!\\n\"); exit(1); &#125; srand(time(NULL)); bzero(&amp;client_addr, sizeof(client_addr)); client_addr.sin_family = AF_INET; client_addr.sin_addr.s_addr = htons(INADDR_ANY); bzero(&amp;server_addr, sizeof(server_addr)); server_addr.sin_family = AF_INET; inet_aton(argv[1], &amp;server_addr.sin_addr); server_addr.sin_port = htons(SERVER_PORT); if(connect(client_fd, (struct sockaddr*)&amp;server_addr, socklen) &lt; 0) &#123; printf(\"can not connect to %s, exit!\\n\", argv[1]); printf(\"%s\\n\", strerror(errno)); exit(1); &#125; return 0;&#125; 运行方法：[root@localhost socketdemo]# gcc connect.c -o connect[root@localhost socketdemo]# ./connect 192.168.55.181 系统调用12345678910111213141516171819202122232425262728293031323334353637383940414243SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)&#123; unsigned long a[AUDITSC_ARGS]; unsigned long a0, a1; int err; unsigned int len; if (call &lt; 1 || call &gt; SYS_SENDMMSG) return -EINVAL; len = nargs[call]; if (len &gt; sizeof(a)) return -EINVAL; /* copy_from_user should be SMP safe. */ if (copy_from_user(a, args, len)) return -EFAULT; err = audit_socketcall(nargs[call] / sizeof(unsigned long), a); if (err) return err; a0 = a[0]; a1 = a[1]; switch (call) &#123; case SYS_SOCKET: err = sys_socket(a0, a1, a[2]); break; case SYS_BIND: err = sys_bind(a0, (struct sockaddr __user *)a1, a[2]); break; case SYS_CONNECT: err = sys_connect(a0, (struct sockaddr __user *)a1, a[2]); break; ... default: err = -EINVAL; break; &#125; return err;&#125; 系统调用sys_socketcall会携带（fd,serveraddr,serveraddrlen）参数 系统中断处理函数sys_socketcall会将参数从用户态考入到内核态局部变量a中 调用sys_connect函数 sys_connect(a0, (struct sockaddr __user *)a1, a[2]); sys_connect执行入口分析123456789101112131415161718192021222324SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,int,addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; err = move_addr_to_kernel(uservaddr, addrlen, &amp;address); if (err &lt; 0) goto out_put; err = security_socket_connect(sock, (struct sockaddr *)&amp;address, addrlen); if (err) goto out_put; err = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)&amp;address, addrlen, sock-&gt;file-&gt;f_flags);out_put: fput_light(sock-&gt;file, fput_needed);out: return err; 根据fd描述符号从当前进程current的files指针中的struct fd_table中的fd成员取出file fdt-&gt;fd是一个数组用来管理当前进程的file指针 从file中privatedata中获取到socket变量 把connect连接的服务端地址存入内核空间中move_addr_to_kernel sock-&gt;ops-&gt;connect 以tco为例，此处会调用inet_stream_connect 函数集合中的inet_stream_connect inet_stream_connect分析12345678910int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; int err; lock_sock(sock-&gt;sk); err = __inet_stream_connect(sock, uaddr, addr_len, flags); release_sock(sock-&gt;sk); return err;&#125; inet_stream_connect() 为tcp socket时候connect动作调用的函数改函数会调用__inet_stream_connect函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; struct sock *sk = sock-&gt;sk; int err; long timeo; //socket地址长度检查，不合法返回 if (addr_len &lt; sizeof(uaddr-&gt;sa_family)) return -EINVAL; // 地址协议族检查，如果不合法则关闭连接 if (uaddr-&gt;sa_family == AF_UNSPEC) &#123; err = sk-&gt;sk_prot-&gt;disconnect(sk, flags); sock-&gt;state = err ? SS_DISCONNECTING : SS_UNCONNECTED; goto out; &#125; switch (sock-&gt;state) &#123; //非法参数 default: err = -EINVAL; goto out; //该socket和对端连接已经建立 case SS_CONNECTED: err = -EISCONN; goto out; //该socket和对端连接建立中 case SS_CONNECTING: err = -EALREADY; /* Fall out of switch with err, set for this state */ break; //该socket和对未连接 case SS_UNCONNECTED: err = -EISCONN; //如果未连接，但是socket还不是TCP_CLOSE状态错误返回 if (sk-&gt;sk_state != TCP_CLOSE) goto out; //tcp调用tcp_v4_connect，发送syn err = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len); if (err &lt; 0) goto out; //发送syn后sock状态从未连接更新为连接中 sock-&gt;state = SS_CONNECTING; /* Just entered SS_CONNECTING state; the only * difference is that return value in non-blocking * case is EINPROGRESS, rather than EALREADY. */ err = -EINPROGRESS; break; &#125; //默认情况下未设置非阻塞socket标志，timeo不为0，设置非阻塞，该值为0 timeo = sock_sndtimeo(sk, flags &amp; O_NONBLOCK); //发送syn后等待后续握手完成 /* * 阻塞socket * inet_wait_for_connect 会等待协议栈层的处理 * 1.等待超过timeo，connect返回EINPROGRESS 表明正在处理 * 2.收到信号 * 3.正常完成握手，返回0 * 非阻塞socket * 直接退出connect函数并返回EINPROGRESS，表明协议栈正在处理 */ if ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; int writebias = (sk-&gt;sk_protocol == IPPROTO_TCP) &amp;&amp; tcp_sk(sk)-&gt;fastopen_req &amp;&amp; tcp_sk(sk)-&gt;fastopen_req-&gt;data ? 1 : 0; /* Error code is set above */ if (!timeo || !inet_wait_for_connect(sk, timeo, writebias)) goto out; err = sock_intr_errno(timeo); if (signal_pending(current)) goto out; &#125; /* Connection was closed by RST, timeout, ICMP error * or another process disconnected us. */ if (sk-&gt;sk_state == TCP_CLOSE) goto sock_error; /* sk-&gt;sk_err may be not zero now, if RECVERR was ordered by user * and error was received after socket entered established state. * Hence, it is handled normally after connect() return successfully. */ //TCP握手完成，连接已经建立 sock-&gt;state = SS_CONNECTED; err = 0;out: return err;//异常处理，关闭连接sock_error: err = sock_error(sk) ? : -ECONNABORTED; sock-&gt;state = SS_UNCONNECTED; if (sk-&gt;sk_prot-&gt;disconnect(sk, flags)) sock-&gt;state = SS_DISCONNECTING; goto out;&#125; __inet_stream_connect检查地址长度和协议族 检查sock状态，正常情况下状态为SS_UNCONNECTED sk-&gt;sk_prot-&gt;connect tcp_v4_connect来发送syn 在syn包发完以后会有两种处理情况 情况1:立即返回，针对于非阻塞socket，此时协议栈正在处理握手connect会返回-EINPROGRESS情况2:阻塞运行 阻塞时间超时后，connect返回-EINPROGRESS收到信号，connect返回-ERESTARTSYS,-EINTR inet_wait_for_connect函数分析1234567891011121314151617181920212223242526272829303132333435363738static long inet_wait_for_connect(struct sock *sk, long timeo, int writebias)&#123; //初始化等待队列链表,设置队列唤醒后回调函数autoremove_wake_function DEFINE_WAIT(wait); prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); sk-&gt;sk_write_pending += writebias; /* Basic assumption: if someone sets sk-&gt;sk_err, he _must_ * change state of the socket from TCP_SYN_*. * Connect() does not allow to get error notifications * without closing the socket. */ while ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; release_sock(sk);/*等下要睡眠了释放sk锁*/ timeo = schedule_timeout(timeo); /* * 调用schedule_timeout sleep until timeout * 收到信号后，timeout值返回剩余等待时间 * 超时timeout后，返回0 */ /*进程被唤醒后新上sk锁*/ lock_sock(sk); /*进程有带处理信号，或者睡眠超时，推出循环*/ if (signal_pending(current) || !timeo) break; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); &#125; /*等待结束后，将进程从等待队列删除，标记为TASK_RUNNING*/ finish_wait(sk_sleep(sk), &amp;wait); sk-&gt;sk_write_pending -= writebias; return timeo;&#125; DEFINE_WAIT函数很重要其设置了唤醒时候删除队列成员调用的回调函数autoremove_wake_funtion 睡眠前进程被设置成TASK_INTERRUPTIBLE状态 SO_SNDTIMEO选项对上述的睡眠非常重要 SO_SNDTIMEO被设置，则睡眠时间会安装设置值 SO_SNDTIMEO没有被设置，则在没有收到信号前一只阻塞 睡眠结束，进程从睡眠队列中删除，并标记为TASK_RUNNING prepare_to_wait实现分析1234567891011void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)&#123; unsigned long flags; wait-&gt;flags &amp;= ~WQ_FLAG_EXCLUSIVE; spin_lock_irqsave(&amp;q-&gt;lock, flags); if (list_empty(&amp;wait-&gt;task_list)) __add_wait_queue(q, wait); set_current_state(state); spin_unlock_irqrestore(&amp;q-&gt;lock, flags);&#125; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); 把wait放入q队列中，设置当前进程状态为TASK_INTERRUPTIBLE TASK_INTERRUPTIBLE 是一种睡眠信号 标记TASK_INTERRUPTIBLE的信号会被唤醒并处理信号 阻塞socket唤醒机制[root@localhost stp]# stap bt.stp sock_def_wakeup WARNING: Missing unwind data for a module, rerun with ‘stap -d e1000’—————-START————————-In process [swapper/2]RIP: ffffffff81558150RSP: ffff88003fd03970 EFLAGS: 00000246RAX: 0000000000004308 RBX: ffff88003a82a6c0 RCX: 0000000000000000RDX: 0000000050000000 RSI: 0000000000ca00c8 RDI: ffff88003a82a6c0RBP: ffff88003fd03988 R08: ffff88003db89708 R09: ffff88003e001800R10: ffffffff815dabca R11: 0000000000000000 R12: ffff88001bfa3700R13: ffff880002db6762 R14: 0000000000000218 R15: ffff880002db675aFS: 0000000000000000(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 000000008005003bCR2: 00007ffaf3049072 CR3: 000000003b0b7000 CR4: 00000000000406e0 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815cbc09 : tcp_finish_connect+0xc9/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000]—————-END————————- 12345678910111213141516171819202122232425262728293031323334353637383940void tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123; struct tcp_sock *tp = tcp_sk(sk); struct inet_connection_sock *icsk = inet_csk(sk); tcp_set_state(sk, TCP_ESTABLISHED); if (skb != NULL) &#123; icsk-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb); security_inet_conn_established(sk, skb); &#125; /* Make sure socket is routed, for correct metrics. */ icsk-&gt;icsk_af_ops-&gt;rebuild_header(sk); tcp_init_metrics(sk); tcp_init_congestion_control(sk); /* Prevent spurious tcp_cwnd_restart() on first data * packet. */ tp-&gt;lsndtime = tcp_time_stamp; tcp_init_buffer_space(sk); if (sock_flag(sk, SOCK_KEEPOPEN)) inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp)); if (!tp-&gt;rx_opt.snd_wscale) __tcp_fast_path_on(tp, tp-&gt;snd_wnd); else tp-&gt;pred_flags = 0; if (!sock_flag(sk, SOCK_DEAD)) &#123; /*握手完成唤醒所有进程*/ sk-&gt;sk_state_change(sk); sk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT); &#125;&#125; sock_def_wakeup -&gt;wake_up_interruptible_all 上述过程发声在三次握手完成后，TCP从syn send或者syn rcv切换到establish状态时候发生 tcp_finish_connect-&gt;sk-&gt;sk_state_change[sock_def_wakeup] 此次唤醒是全部唤醒sk上等待队列的进程","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"http://vcpu.me/tags/socket/"}]},{"title":"socket()实现源码分析","slug":"socket","date":"2017-06-09T09:22:34.000Z","updated":"2017-06-22T10:13:08.000Z","comments":true,"path":"socket/","link":"","permalink":"http://vcpu.me/socket/","excerpt":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0","text":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0 (1).接口说明：按照顺序可传入如下参数： PF_INEAT SOCK_STREAM,SOCK_DGRAM,SOCK_RAW IPPROTO_TCP,IPPROTO_UDP,IPPROTO_IP 返回值说明 EAFNOSUPPORT 不支持地址类型 EMFILE 进程文件表溢出 ENFILE 核心内存不足无法建立新的socket EINVAL 参数domain/type/protocol不合法 EACCES 权限不允许 ENOBUFS/ENOMEM 内存不足 EPROTONOSUPPORT domain指定的类型不支持参数type或者protocol (2).内核调用栈 (3).结构体说明 struct socket 面向用户态的结构体基于虚拟文件系统创建创建socket时最先创建的结构体 struct sock 网络层socket struct inet_sock INET域socket表示提供INET域的一些属性，TTL、 组播、 地址 、端口 struct raw_socket、struct udp—sock、 struct inet_connection_sock 是对struct inet_sock的扩展struct raw_socket要处理ICMPstruct udp_sock udp协议socketstruct inet_connection_sock面向连接socketstruct tcp_sock TCP协议socket ，对inet_connection_sock扩展，增加了滑动窗口等拥塞控制属性struct inet_timewait_sock网络层超时控制使用struct tcp_timewait_sock TCP协议超时控制使用 (4).struct socket创建源码分析(4.1).sock_alloc函数123456789101112131415161718192021static struct socket *sock_alloc(void)&#123; struct inode *inode; struct socket *sock; inode = new_inode_pseudo(sock_mnt-&gt;mnt_sb); if (!inode) return NULL; sock = SOCKET_I(inode); kmemcheck_annotate_bitfield(sock, type); inode-&gt;i_ino = get_next_ino(); inode-&gt;i_mode = S_IFSOCK | S_IRWXUGO; inode-&gt;i_uid = current_fsuid(); inode-&gt;i_gid = current_fsgid(); inode-&gt;i_op = &amp;sockfs_inode_ops; this_cpu_add(sockets_in_use, 1); return sock;&#125; 一起申请两块内存struct socket和struct inode 两块内存用struct socket_alloc联系起来 inode是linux用来刻画一个存放在内存中的文件的 socket是一种网络文件类型，可以通过文件描述符使用read和write等文件操作函数操作socket 有了inode就支持了虚拟文件系统的操作 (4.2).sock_alloc-&gt;new_inode_pseudo-&gt;alloc_inode12345678910111213141516171819202122232425262728293031323334struct inode *new_inode_pseudo(struct super_block *sb)&#123; struct inode *inode = alloc_inode(sb); if (inode) &#123; spin_lock(&amp;inode-&gt;i_lock); inode-&gt;i_state = 0; spin_unlock(&amp;inode-&gt;i_lock); INIT_LIST_HEAD(&amp;inode-&gt;i_sb_list); &#125; return inode;&#125;static struct inode *alloc_inode(struct super_block *sb)&#123; struct inode *inode; if (sb-&gt;s_op-&gt;alloc_inode) inode = sb-&gt;s_op-&gt;alloc_inode(sb); else inode = kmem_cache_alloc(inode_cachep, GFP_KERNEL); if (!inode) return NULL; if (unlikely(inode_init_always(sb, inode))) &#123; if (inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode) inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode(inode); else kmem_cache_free(inode_cachep, inode); return NULL; &#125; return inode;&#125; alloc_inode获取内存有两种方式 1.通过自己alloc_inode分配 2.从高速缓存中分配 (4.3).alloc_inode -&gt; sock_alloc_inode12345678910111213141516171819202122232425static struct inode *sock_alloc_inode(struct super_block *sb)&#123; struct socket_alloc *ei; struct socket_wq *wq; ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL); if (!ei) return NULL; wq = kmalloc(sizeof(*wq), GFP_KERNEL); if (!wq) &#123; kmem_cache_free(sock_inode_cachep, ei); return NULL; &#125; init_waitqueue_head(&amp;wq-&gt;wait); wq-&gt;fasync_list = NULL; RCU_INIT_POINTER(ei-&gt;socket.wq, wq); ei-&gt;socket.state = SS_UNCONNECTED; ei-&gt;socket.flags = 0; ei-&gt;socket.ops = NULL; ei-&gt;socket.sk = NULL; ei-&gt;socket.file = NULL; return &amp;ei-&gt;vfs_inode;&#125; socket结构体最终会调用上述函数申请内存 该函数会在sock_init中被注册和挂载到系统上 (4.4).sock_init 中sock_allok_inode挂载过程123456789101112131415161718192021222324err = register_filesystem(&amp;sock_fs_type); if (err) goto out_fs; sock_mnt = kern_mount(&amp;sock_fs_type); if (IS_ERR(sock_mnt)) &#123; err = PTR_ERR(sock_mnt); goto out_mount; ... static struct file_system_type sock_fs_type = &#123; .name = \"sockfs\", .mount = sockfs_mount, .kill_sb = kill_anon_super,&#125;;static struct dentry *sockfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)&#123; return mount_pseudo(fs_type, \"socket:\", &amp;sockfs_ops, &amp;sockfs_dentry_operations, SOCKFS_MAGIC);&#125;static const struct super_operations sockfs_ops = &#123; .alloc_inode = sock_alloc_inode, .destroy_inode = sock_destroy_inode, .statfs = simple_statfs,&#125;; sock_init -&gt; register mount -&gt; sock_fs_type-&gt;sockfs_mount-&gt;sockfs_ops-&gt;sock_alloc_node (4.5).pf-&gt;create 即TCP／IP协议族的创建函数inet_create初始化步骤(4.5.1).PF_INET协议族的create函数inet_create会被组册1234567(void)sock_register(&amp;inet_family_ops);static const struct net_proto_family inet_family_ops = &#123; .family = PF_INET, .create = inet_create, .owner = THIS_MODULE,&#125;; (4.5.2).注册过程123456789101112131415161718192021int sock_register(const struct net_proto_family *ops)&#123; int err; if (ops-&gt;family &gt;= NPROTO) &#123; printk(KERN_CRIT \"protocol %d &gt;= NPROTO(%d)\\n\", ops-&gt;family, NPROTO); return -ENOBUFS; &#125; spin_lock(&amp;net_family_lock); if (rcu_dereference_protected(net_families[ops-&gt;family], lockdep_is_held(&amp;net_family_lock))) err = -EEXIST; else &#123; rcu_assign_pointer(net_families[ops-&gt;family], ops); err = 0; &#125; spin_unlock(&amp;net_family_lock); printk(KERN_INFO \"NET: Registered protocol family %d\\n\", ops-&gt;family); return err;&#125; 协议族选项ops会根基协议族类型PF_INET被放置到net_families系统全局变量中 (4.5.3).__sock_create使用过程1234567891011121314151617181920socket.c/__sock_create...rcu_read_lock(); pf = rcu_dereference(net_families[family]); err = -EAFNOSUPPORT; if (!pf) goto out_release; /* * We will call the -&gt;create function, that possibly is in a loadable * module, so we have to bump that loadable module refcnt first. */ if (!try_module_get(pf-&gt;owner)) goto out_release; /* Now protected by module ref count */ rcu_read_unlock(); err = pf-&gt;create(net, sock, protocol, kern); if (err &lt; 0) goto out_module_put; 根据socket传输过来的协议族PF_INET查找全局变量net_families获取ops 通过ops-&gt;create调用inet_create根据具体协议创建网络层socket struct sock (4.6).inet_create都干了什么？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140static int inet_create(struct net *net, struct socket *sock, int protocol, int kern)&#123; struct sock *sk; struct inet_protosw *answer; struct inet_sock *inet; struct proto *answer_prot; unsigned char answer_flags; int try_loading_module = 0; int err; if (protocol &lt; 0 || protocol &gt;= IPPROTO_MAX) return -EINVAL; sock-&gt;state = SS_UNCONNECTED;//步骤1:设置socket状态SS_UNCONNECTED /* Look for the requested type/protocol pair. */lookup_protocol: err = -ESOCKTNOSUPPORT; rcu_read_lock();／／步骤2:根据socket协议找到inet处理函数 connect、bind、accept、listen、等 list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123; err = 0; /* Check the non-wild match. */ if (protocol == answer-&gt;protocol) &#123; if (protocol != IPPROTO_IP) break; &#125; else &#123; /* Check for the two wild cases. */ if (IPPROTO_IP == protocol) &#123; protocol = answer-&gt;protocol; break; &#125; if (IPPROTO_IP == answer-&gt;protocol) break; &#125; err = -EPROTONOSUPPORT; &#125; if (unlikely(err)) &#123; if (try_loading_module &lt; 2) &#123; rcu_read_unlock(); /* * Be more specific, e.g. net-pf-2-proto-132-type-1 * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM) */ if (++try_loading_module == 1) request_module(\"net-pf-%d-proto-%d-type-%d\", PF_INET, protocol, sock-&gt;type); /* * Fall back to generic, e.g. net-pf-2-proto-132 * (net-pf-PF_INET-proto-IPPROTO_SCTP) */ else request_module(\"net-pf-%d-proto-%d\", PF_INET, protocol); goto lookup_protocol; &#125; else goto out_rcu_unlock; &#125; err = -EPERM; if (sock-&gt;type == SOCK_RAW &amp;&amp; !kern &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_RAW)) goto out_rcu_unlock;／／步骤3: 把协协议的inet操作集合赋值给socket结构的ops sock-&gt;ops = answer-&gt;ops; answer_prot = answer-&gt;prot; answer_flags = answer-&gt;flags; rcu_read_unlock(); WARN_ON(answer_prot-&gt;slab == NULL); err = -ENOBUFS; ／／步骤4:申请struct sock结构体，并切把协议操作集合赋值给sock结构体 ／／sk-&gt;sk_prot = sk-&gt;sk_prot_creator =协议操作集合; sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot); if (sk == NULL) goto out; err = 0; if (INET_PROTOSW_REUSE &amp; answer_flags) sk-&gt;sk_reuse = SK_CAN_REUSE;／／步骤5：inet_sock进行相关初始化 inet = inet_sk(sk); inet-&gt;is_icsk = (INET_PROTOSW_ICSK &amp; answer_flags) != 0; inet-&gt;nodefrag = 0; if (SOCK_RAW == sock-&gt;type) &#123; inet-&gt;inet_num = protocol; if (IPPROTO_RAW == protocol) inet-&gt;hdrincl = 1; &#125; if (net-&gt;sysctl_ip_no_pmtu_disc) inet-&gt;pmtudisc = IP_PMTUDISC_DONT; else inet-&gt;pmtudisc = IP_PMTUDISC_WANT; inet-&gt;inet_id = 0; sock_init_data(sock, sk); sk-&gt;sk_destruct = inet_sock_destruct; sk-&gt;sk_protocol = protocol; sk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv; inet-&gt;uc_ttl = -1; inet-&gt;mc_loop = 1; inet-&gt;mc_ttl = 1; inet-&gt;mc_all = 1; inet-&gt;mc_index = 0; inet-&gt;mc_list = NULL; inet-&gt;rcv_tos = 0; sk_refcnt_debug_inc(sk); if (inet-&gt;inet_num) &#123; /* It assumes that any protocol which allows * the user to assign a number at socket * creation time automatically * shares. */ inet-&gt;inet_sport = htons(inet-&gt;inet_num); /* Add to protocol hash chains. */ sk-&gt;sk_prot-&gt;hash(sk); &#125;／／步骤6:调用协议层初始化函数tcp_v4_init_sock()进行始化 if (sk-&gt;sk_prot-&gt;init) &#123; err = sk-&gt;sk_prot-&gt;init(sk); if (err) sk_common_release(sk); &#125;out: return err;out_rcu_unlock: rcu_read_unlock(); goto out;&#125; 设置socket状态SS_UNCONNECTED 根据协议类型找到具体的协议类型操作集合，例如协议处理函数tcp_proc和inet层处理函数集合inet_stream_ops socket-&gt;ops 获得协议操作集合inet_stream_ops 申请sock，并把tcp_proc赋值给它 sk-&gt;sk_prot = sk-&gt;sk_prot_creator=tcp_proc 把申请的sock和inet_sock进行初始化 sk-&gt;sk_prot-&gt;init(sk) 调用tcp_proc深度初始化TCP相关信息 尽管流程主要干了上述的事情，仍需要深入探究的问题是：a. inet_protosw inet_protosw初始化过程如何？b. inet_sock和sock是什么关系？c. 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ (4.6.1).inet_protosw初始化过程如何？12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485static struct inet_protosw inetsw_array[] =&#123; &#123; .type = SOCK_STREAM, .protocol = IPPROTO_TCP, .prot = &amp;tcp_prot, .ops = &amp;inet_stream_ops, .flags = INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_UDP, .prot = &amp;udp_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_PERMANENT, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_ICMP, .prot = &amp;ping_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_REUSE, &#125;, &#123; .type = SOCK_RAW, .protocol = IPPROTO_IP, /* wild card */ .prot = &amp;raw_prot, .ops = &amp;inet_sockraw_ops, .flags = INET_PROTOSW_REUSE, &#125;&#125;;//inet_init for (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q) inet_register_protosw(q); //inet_protosw放入全局inetsw管理void inet_register_protosw(struct inet_protosw *p)&#123; struct list_head *lh; struct inet_protosw *answer; int protocol = p-&gt;protocol; struct list_head *last_perm; spin_lock_bh(&amp;inetsw_lock); if (p-&gt;type &gt;= SOCK_MAX) goto out_illegal; /* If we are trying to override a permanent protocol, bail. */ answer = NULL; last_perm = &amp;inetsw[p-&gt;type]; list_for_each(lh, &amp;inetsw[p-&gt;type]) &#123; answer = list_entry(lh, struct inet_protosw, list); /* Check only the non-wild match. */ if (INET_PROTOSW_PERMANENT &amp; answer-&gt;flags) &#123; if (protocol == answer-&gt;protocol) break; last_perm = lh; &#125; answer = NULL; &#125; if (answer) goto out_permanent; /* Add the new entry after the last permanent entry if any, so that * the new entry does not override a permanent entry when matched with * a wild-card protocol. But it is allowed to override any existing * non-permanent entry. This means that when we remove this entry, the * system automatically returns to the old behavior. */ list_add_rcu(&amp;p-&gt;list, last_perm);out: spin_unlock_bh(&amp;inetsw_lock); return;out_permanent: pr_err(\"Attempt to override permanent protocol %d\\n\", protocol); goto out;out_illegal: pr_err(\"Ignoring attempt to register invalid socket type %d\\n\", p-&gt;type); goto out;&#125; inet_init 会把inet_protosw方式inet_sw中 inet_protosw很重要，其含有协议的具体操作函数tcp_close,tcp_v4_connect,tcp_recvmsg等 inet_protosw，内还包含inet层操作函数 inet_bind,inet_accept,inet_bind,inet_listen等 (4.6.2). inet_sock和sock是什么关系？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566struct sock *sk_alloc(struct net *net, int family, gfp_t priority, struct proto *prot)&#123; struct sock *sk; sk = sk_prot_alloc(prot, priority | __GFP_ZERO, family); if (sk) &#123; sk-&gt;sk_family = family; /* * See comment in struct sock definition to understand * why we need sk_prot_creator -acme */ sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; sock_lock_init(sk); sock_net_set(sk, get_net(net)); atomic_set(&amp;sk-&gt;sk_wmem_alloc, 1); sock_update_classid(sk); sock_update_netprioidx(sk); &#125; return sk;&#125;static struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority, int family)&#123; struct sock *sk; struct kmem_cache *slab; slab = prot-&gt;slab; if (slab != NULL) &#123; sk = kmem_cache_alloc(slab, priority &amp; ~__GFP_ZERO); if (!sk) return sk; if (priority &amp; __GFP_ZERO) &#123; if (prot-&gt;clear_sk) prot-&gt;clear_sk(sk, prot-&gt;obj_size); else sk_prot_clear_nulls(sk, prot-&gt;obj_size); &#125; &#125; else sk = kmalloc(prot-&gt;obj_size, priority);//申请内存大小为prot的objsize if (sk != NULL) &#123; kmemcheck_annotate_bitfield(sk, flags); if (security_sk_alloc(sk, family, priority)) goto out_free; if (!try_module_get(prot-&gt;owner)) goto out_free_sec; sk_tx_queue_clear(sk); &#125; return sk;out_free_sec: security_sk_free(sk);out_free: if (slab != NULL) kmem_cache_free(slab, sk); else kfree(sk); return NULL;&#125; 从上述sk_alloc -&gt; sk_prot_alloc -&gt; obj_size 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct proto tcp_prot = &#123; .name = \"TCP\", .owner = THIS_MODULE, .close = tcp_close, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, .sendpage = tcp_sendpage, .backlog_rcv = tcp_v4_do_rcv, .release_cb = tcp_release_cb, .hash = inet_hash, .unhash = inet_unhash, .get_port = inet_csk_get_port, .enter_memory_pressure = tcp_enter_memory_pressure, .stream_memory_free = tcp_stream_memory_free, .sockets_allocated = &amp;tcp_sockets_allocated, .orphan_count = &amp;tcp_orphan_count, .memory_allocated = &amp;tcp_memory_allocated, .memory_pressure = &amp;tcp_memory_pressure, .sysctl_wmem = sysctl_tcp_wmem, .sysctl_rmem = sysctl_tcp_rmem, .max_header = MAX_TCP_HEADER, .obj_size = sizeof(struct tcp_sock), .slab_flags = SLAB_DESTROY_BY_RCU, .twsk_prot = &amp;tcp_timewait_sock_ops, .rsk_prot = &amp;tcp_request_sock_ops, .h.hashinfo = &amp;tcp_hashinfo, .no_autobind = true,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_tcp_setsockopt, .compat_getsockopt = compat_tcp_getsockopt,#endif#ifdef CONFIG_MEMCG_KMEM .init_cgroup = tcp_init_cgroup, .destroy_cgroup = tcp_destroy_cgroup, .proto_cgroup = tcp_proto_cgroup,#endif&#125;; struct tcp_sock 包含strcut inet_sock 包含 struct sock 上述结构体为互相包含的关系 实际上在申请sock时候，申请内存大小为tcp_sock大小，也就是说三个结构体共同诞生了 (4.6.3). 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ struct socket会在inet_create函数中获取到ops sock-&gt;ops = answer-&gt;ops;struct sock在sk_allloc函数中获取pro sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; (5).socket与文件系统socket与文件系统关联通过sock_map_fd完成 其步骤如下： 1:获取fd get_unused_fd_flags 该函数从当前进程管理的files获取可用的fd 2:申请file sock_alloc_file 将struct socket放到file的private_data管理 file-&gt;private_data = sock 3:将file根据当前fd安装到current-&gt;files中 files有一个指针fdtfdt-&gt;fd是一个类型为file指针的数组，数组下标为fdrcu_assign_pointer(fdt-&gt;fd[fd], file); 将file安装fd为数组下标放到current-&gt;files管理","categories":[{"name":"socket","slug":"socket","permalink":"http://vcpu.me/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://vcpu.me/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"http://vcpu.me/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"http://vcpu.me/tags/socket/"}]},{"title":"systemtap使用调试记录（一）","slug":"systemtap使用调试记录（一）","date":"2017-06-05T10:15:52.000Z","updated":"2017-06-05T10:15:52.000Z","comments":true,"path":"systemtap使用调试记录（一）/","link":"","permalink":"http://vcpu.me/systemtap使用调试记录（一）/","excerpt":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法","text":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法 二、centos7安装方法yum install *.rpm 安装上述3个（debugifo,devel,debuginfo-common）rpm包 yum install systemtap stap -ve &apos;probe begin { log(&quot;hello world&quot;) exit() }&apos; 测试正常结果如下： [root@localhost qinlong]# stap -ve ‘probe begin { log(“hello world”) exit() }’Pass 1: parsed user script and 120 library scripts using 227352virt/40488res/3260shr/37400data kb, in 260usr/30sys/338real ms.Pass 2: analyzed script: 1 probe, 2 functions, 0 embeds, 0 globals using 228540virt/41804res/3420shr/38588data kb, in 10usr/0sys/6real ms.Pass 3: translated to C into “/tmp/stap5CqHmN/stap_f7a5084b8a638f5ce64a31271684ef1f_1133_src.c” using 228672virt/42408res/3996shr/38720data kb, in 0usr/0sys/0real ms.Pass 4: compiled C into “stap_f7a5084b8a638f5ce64a31271684ef1f_1133.ko” in 1000usr/330sys/1247real ms.Pass 5: starting run.hello worldPass 5: run completed in 10usr/40sys/362real ms. 三、通用案例1.函数调用栈打印123456789[root@localhost stp]# cat bt.stp probe kernel.function(@1)&#123; print(&quot;----------------START-------------------------\\n&quot;) printf(&quot;In process [%s]\\n&quot;, execname()) print_regs() print_backtrace() print(&quot;----------------END-------------------------\\n&quot;) exit() &#125; 打印内核函数的调用栈 [root@localhost stp]# stap bt.stp tcp_sendmsg—————-START————————-In process [sshd]RIP: ffffffff815c1ee0RSP: ffff88003d217d28 EFLAGS: 00000202RAX: ffffffff81aa20a0 RBX: ffff88003d217e38 RCX: 0000000000000024RDX: ffff88003d217da8 RSI: ffff88003b3b87c0 RDI: ffff88003d217e38RBP: ffff88003d217d50 R08: 0000000000000000 R09: 0000000000000000R10: ffff88003d217da8 R11: 0000000000000000 R12: ffff88003d217e38R13: 0000000000000001 R14: ffff88003d217e28 R15: ffff8800274d3480FS: 00007f03e5514840(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033CR2: 00007f19c6dc8000 CR3: 0000000035a5c000 CR4: 00000000000406e0 0xffffffff815c1ee0 : tcp_sendmsg+0x0/0xc40 [kernel] 0xffffffff815ed254 : inet_sendmsg+0x64/0xb0 [kernel] 0xffffffff81554e07 : sock_aio_write+0x157/0x180 [kernel] 0xffffffff811fdf3d : do_sync_write+0x8d/0xd0 [kernel] 0xffffffff811fe8a5 : vfs_write+0x1b5/0x1e0 [kernel] 0xffffffff811ff2cf : sys_write+0x7f/0xe0 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel]—————-END————————- 2.函数的调用过程1234567[root@localhost stp]# cat socket-trace.stpprobe kernel.function(&quot;*@net/socket.c&quot;).call&#123; printf(&quot;%s -&gt; %s\\n&quot;,thread_indent(1),ppfunc())&#125;probe kernel.function(&quot;*@net/socket.c&quot;).return&#123; printf(&quot;%s&lt;-%s\\n&quot;,thread_indent(-1),ppfunc())&#125; thread_indent(1) 打印程序名称（线程id）ppfunc() 打印出执行函数符号 kernel.function(“@net/socket.c”).call调用net/socket.c 文件中函数时候会触发函数体执行打印动作kernel.function(“@net/socket.c”).return调用net/socket.c文件中函数执行完成返回后会触发函数体打印动作 [root@localhost stp]# stap socket-trace.stp 0 dndX11(3295): -&gt; SyS_recvmsg 0 dndX11(3295): -&gt; sys_recvmsg 0 dndX11(3295): -&gt; sockfd_lookup_light 0 dndX11(3295):&lt;-sockfd_lookuplight 1 dndX11(3295): -&gt; sys_recvmsg 3 dndX11(3295): -&gt; sock_recvmsg 7 dndX11(3295):&lt;-sock_recvmsg 8 dndX11(3295):&lt;-_sys_recvmsg 9 dndX11(3295):&lt;-sys_recvmsg 10 dndX11(3295):&lt;-SyS_recvmsg25274 dndX11(3295): -&gt; SyS_recvmsg25279 dndX11(3295): -&gt; sys_recvmsg25281 dndX11(3295): -&gt; sockfd_lookup_light25284 dndX11(3295):&lt;-sockfd_lookuplight25285 dndX11(3295): -&gt; sys_recvmsg25288 dndX11(3295): -&gt; sock_recvmsg25291 dndX11(3295):&lt;-sock_recvmsgx 3.打印协议栈函数中某一行数据/home/qinlong/rpmbuild/SOURCES/linux-3.10.0-514.16.1.el7/net/ipv4/tcp.c局部源码如下：12345678910111213141065 int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,1066 size_t size)1067 &#123;1068 struct iovec *iov;1069 struct tcp_sock *tp = tcp_sk(sk);1070 struct sk_buff *skb;1071 int iovlen, flags, err, copied = 0;1072 int mss_now = 0, size_goal, copied_syn = 0, offset = 0;1073 bool sg;1074 long timeo;10751076 lock_sock(sk);10771078 flags = msg-&gt;msg_flags; 12[root@localhost ~]# stap -L &apos;kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;)&apos;kernel.statement(&quot;tcp_sendmsg@net/ipv4/tcp.c:1078&quot;) $iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int $mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 执行上述函数，可确代码具体的函数局部变量12345678910$iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int$mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 根据以上变量打印出size值123[root@localhost ~]# stap -e &apos;probe kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;) &#123;printf(&quot;size %d \\n&quot;,$size)&#125;&apos;size 36size 44","categories":[{"name":"linux kernel","slug":"linux-kernel","permalink":"http://vcpu.me/categories/linux-kernel/"}],"tags":[{"name":"systemtap","slug":"systemtap","permalink":"http://vcpu.me/tags/systemtap/"}]}]}