{"meta":{"title":"博客","subtitle":null,"description":null,"author":"vcpu","url":"https://vcpu.github.io"},"pages":[{"title":"categories","date":"2017-06-05T04:59:19.000Z","updated":"2017-06-05T05:00:31.000Z","comments":false,"path":"categories/index.html","permalink":"https://vcpu.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-06-05T04:50:47.000Z","updated":"2017-06-05T04:51:50.000Z","comments":false,"path":"tags/index.html","permalink":"https://vcpu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"TIME_WAIT状态分析","slug":"TIME_WAIT状态分析","date":"2017-06-12T10:19:10.000Z","updated":"2017-06-12T10:19:10.000Z","comments":true,"path":"2017/06/12/TIME_WAIT状态分析/","link":"","permalink":"https://vcpu.github.io/2017/06/12/TIME_WAIT状态分析/","excerpt":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat","text":"TIME_WAIT状态分析之所以起这样一个题目是因为很久以前我曾经写过一篇介绍TIME_WAIT的文章，不过当时基本属于浅尝辄止，并没深入说明问题的来龙去脉，碰巧这段时间反复被别人问到相关的问题，让我觉得有必要全面总结一下，以备不时之需。 讨论前大家可以拿手头的服务器摸摸底，记住「ss」比「netstat」快：1ss -ant | awk 'NR&gt;1 &#123;++s[$1]&#125; END &#123;for(k in s) print k,s[k]&#125; 更简单方法： 1cat /proc/net/sockstat 我猜你一定被巨大无比的TIME_WAIT网络连接总数吓到了！以我个人的经验，对于一台繁忙的Web服务器来说，如果主要以短连接为主，那么其TIME_WAIT网络连接总数很可能会达到几万，甚至十几万。虽然一个TIME_WAIT网络连接耗费的资源无非就是一个端口、一点内存，但是架不住基数大，所以这始终是一个需要面对的问题。 TIMEWAIT是什么因为TCP连接是双向的，所以在关闭连接的时候，两个方向各自都需要关闭。先发FIN包的一方执行的是主动关闭；后发FIN包的一方执行的是被动关闭。主动关闭的一方会进入TIME_WAIT状态，并且在此状态停留两倍的MSL时长。穿插一点MSL的知识：MSL指的是报文段的最大生存时间，如果报文段在网络活动了MSL时间，还没有被接收，那么会被丢弃。关于MSL的大小，RFC 793协议中给出的建议是两分钟，不过实际上不同的操作系统可能有不同的设置，以Linux为例，通常是半分钟，两倍的MSL就是一分钟，也就是60秒，并且这个数值是硬编码在内核中的，也就是说除非你重新编译内核，否则没法修改它： #define TCP_TIMEWAIT_LEN (60*HZ) 如果每秒的连接数是一千的话，那么一分钟就可能会产生六万个TIME_WAIT。为什么主动关闭的一方不直接进入CLOSED状态，而是进入TIME_WAIT状态，并且停留两倍的MSL时长呢？这是因为TCP是建立在不可靠网络上的可靠的协议。例子：主动关闭的一方收到被动关闭的一方发出的FIN包后，回应ACK包，同时进入TIME_WAIT状态，但是因为网络原因，主动关闭的一方发送的这个ACK包很可能延迟，从而触发被动连接一方重传FIN包。极端情况下，这一去一回，就是两倍的MSL时长。如果主动关闭的一方跳过TIME_WAIT直接进入CLOSED，或者在TIME_WAIT停留的时长不足两倍的MSL，那么当被动关闭的一方早先发出的延迟包到达后，就可能出现类似下面的问题： ▪ 旧的TCP连接已经不存在了，系统此时只能返回RST包 ▪ 新的TCP连接被建立起来了，延迟包可能干扰新的连接不管是哪种情况都会让TCP不再可靠，所以TIME_WAIT状态有存在的必要性。 如何控制TIME_WAIT的数量？从前面的描述我们可以得出这样的结论：TIME_WAIT这东西没有的话不行，不过太多可能也是个麻烦事。下面让我们看看有哪些方法可以控制TIME_WAIT数量，这里只说一些常规方法，另外一些诸如SO_LINGER之类的方法太过偏门，略过不谈。ip_conntrack：顾名思义就是跟踪连接。一旦激活了此模块，就能在系统参数里发现很多用来控制网络连接状态超时的设置，其中自然也包括TIME_WAIT：shell&gt; modprobe ip_conntrackshell&gt; sysctl net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait我们可以尝试缩小它的设置，比如十秒，甚至一秒，具体设置成多少合适取决于网络情况而定，当然也可以参考相关的案例。不过就我的个人意见来说，ip_conntrack引入的问题比解决的还多，比如性能会大幅下降，所以不建议使用。 tcp_tw_recycle：顾名思义就是回收TIME_WAIT连接。可以说这个内核参数已经变成了大众处理TIME_WAIT的万金油，如果你在网络上搜索TIME_WAIT的解决方案，十有八九会推荐设置它，不过这里隐藏着一个不易察觉的陷阱：当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。（tcp_tw_recycle和tcp_timestamps导致connect失败问题。同时开启情况下，60s内同一源ip主机socket 请求中timestamp必须是递增的） tcp_tw_reuse：顾名思义就是复用TIME_WAIT连接。当创建新连接的时候，如果可能的话会考虑复用相应的TIME_WAIT连接。通常认为「tcp_tw_reuse」比「tcp_tw_recycle」安全一些，这是因为一来TIME_WAIT创建时间必须超过一秒才可能会被复用；二来只有连接的时间戳是递增的时候才会被复用。官方文档里是这样说的：如果从协议视角看它是安全的，那么就可以使用。这简直就是外交辞令啊！按我的看法，如果网络比较稳定，比如都是内网连接，那么就可以尝试使用。不过需要注意的是在哪里使用，既然我们要复用连接，那么当然应该在连接的发起方使用，而不能在被连接方使用。举例来说：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端，此类情况使用「tcp_tw_reuse」是无效的，因为服务端是被连接方，所以不存在复用连接一说。让我们延伸一点来看，比如说服务端是PHP，它查询另一个MySQL服务端，然后主动断开连接，于是TIME_WAIT就落在了PHP一侧，此类情况下使用「tcp_tw_reuse」是有效的，因为此时PHP相对于MySQL而言是客户端，它是连接的发起方，所以可以复用连接。说明：如果使用tcp_tw_reuse，请激活tcp_timestamps，否则无效。 tcp_max_tw_buckets：顾名思义就是控制TIME_WAIT总数。官网文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为的降低它。如果缩小了它，那么系统会将多余的TIME_WAIT删除掉，日志里会显示：「TCP: time wait bucket table overflow」。需要提醒大家的是物极必反，曾经看到有人把「tcp_max_tw_buckets」设置成0，也就是说完全抛弃TIME_WAIT，这就有些冒险了，用一句围棋谚语来说：入界宜缓。…有时候，如果我们换个角度去看问题，往往能得到四两拨千斤的效果。前面提到的例子：客户端向服务端发起HTTP请求，服务端响应后主动关闭连接，于是TIME_WAIT便留在了服务端。这里的关键在于主动关闭连接的是服务端！在关闭TCP连接的时候，先出手的一方注定逃不开TIME_WAIT的宿命，套用一句歌词：把我的悲伤留给自己，你的美丽让你带走。如果客户端可控的话，那么在服务端打开KeepAlive，尽可能不让服务端主动关闭连接，而让客户端主动关闭连接，如此一来问题便迎刃而解了。 原文连接于https://huoding.com/2013/12/31/316","categories":[{"name":"TCP","slug":"TCP","permalink":"https://vcpu.github.io/categories/TCP/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"}]},{"title":"bind()实现源码分析","slug":"bind","date":"2017-06-12T09:35:01.000Z","updated":"2017-06-12T09:35:01.000Z","comments":true,"path":"2017/06/12/bind/","link":"","permalink":"https://vcpu.github.io/2017/06/12/bind/","excerpt":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。","text":"bind()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *my_addr, socklen_t addrlen); socket文件描述符 要绑定的承载地址和端口的结构体 struct sockaddr 第二个参数struct sockaddr的长度 该函数负责绑定套接字的地址和端口，按照绑定者身份来分，会存在两种情况 情况1:绑定者为客户端，主动发起请求方，绑定地址和端口成功后，会使用该地址和端口进行发包一般情况下，客户端的地址和端口都是其自动选择的，不需要绑定动作。情况2:绑定者为服务端，被动连接接收方，绑定地址和端口成功后，客户端只能向该地址和端口发送连接请求。服务端往往需要绑定地址和端口。如果服务端存在多网卡情况，其只需要绑定服务端口即可，其目的地址就是客户端访问的目的地址。 sys_bind12345678910111213141516171819202122SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (sock) &#123; err = move_addr_to_kernel(umyaddr, addrlen, &amp;address); if (err &gt;= 0) &#123; err = security_socket_bind(sock, (struct sockaddr *)&amp;address, addrlen); if (!err) err = sock-&gt;ops-&gt;bind(sock, (struct sockaddr *) &amp;address, addrlen);//inet_bind &#125; fput_light(sock-&gt;file, fput_needed); &#125; return err;&#125; sockfd_lookup_light 和move_addr_to_kernel分别为根据fd从当前进程取出socket和把参数从用户空间考入地址空间 bind系统调用最重要函数为sock-&gt;ops-&gt;bind 在TCP协议情况下inet_stream_ops中bind成员函数为inet_bind 后续为对此函数的分析 inet_bind实现较为复杂，现在版本和原始版本相比，支持端口复用了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123; struct sockaddr_in *addr = (struct sockaddr_in *)uaddr; struct sock *sk = sock-&gt;sk; struct inet_sock *inet = inet_sk(sk); struct net *net = sock_net(sk); unsigned short snum; int chk_addr_ret; int err; /* If the socket has its own bind function then use it. (RAW) */ /*raw socket才会用到，tcp_proc无此函数*/ if (sk-&gt;sk_prot-&gt;bind) &#123; err = sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len); goto out; &#125; err = -EINVAL; /*地址长度检验*/ if (addr_len &lt; sizeof(struct sockaddr_in)) goto out; /*bind地址中协议检查，必须是下面两种情况 * 1.绑定的地址协议为AF_INET * 2.绑定协议为0（AF_UNSPEC）同时地址也为0 * 否则直接退出inet_bind ,返回地址不支持错误码 */ if (addr-&gt;sin_family != AF_INET) &#123; /* Compatibility games : accept AF_UNSPEC (mapped to AF_INET) * only if s_addr is INADDR_ANY. */ err = -EAFNOSUPPORT; if (addr-&gt;sin_family != AF_UNSPEC || addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY)) goto out; &#125; /*获取根据IP地址得出地址类型 RTN_LOCAL 本机地址 RTN_MULTICAST 多播 RTN_BROADCAST 广播 RTN_UNICAST */ chk_addr_ret = inet_addr_type(net, addr-&gt;sin_addr.s_addr); /* Not specified by any standard per-se, however it breaks too * many applications when removed. It is unfortunate since * allowing applications to make a non-local bind solves * several problems with systems using dynamic addressing. * (ie. your servers still start up even if your ISDN link * is temporarily down) */ err = -EADDRNOTAVAIL; /* 地址类型必须是本机，多播，组播中的一个，否则直接返回，报地址参数异常 * */ if (!net-&gt;ipv4_sysctl_ip_nonlocal_bind &amp;&amp; !(inet-&gt;freebind || inet-&gt;transparent) &amp;&amp; addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY) &amp;&amp; chk_addr_ret != RTN_LOCAL &amp;&amp; chk_addr_ret != RTN_MULTICAST &amp;&amp; chk_addr_ret != RTN_BROADCAST) goto out; snum = ntohs(addr-&gt;sin_port); err = -EACCES; /* * 要绑定的端口小于1024时候，要求运行该应用程序的为超级权限 * 否则返回并报权限不运行的错误 */ if (snum &amp;&amp; snum &lt; PROT_SOCK &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_BIND_SERVICE)) goto out; /* We keep a pair of addresses. rcv_saddr is the one * used by hash lookups, and saddr is used for transmit. * * In the BSD API these are the same except where it * would be illegal to use them (multicast/broadcast) in * which case the sending device address is used. */ lock_sock(sk); /* Check these errors (active socket, double bind). */ err = -EINVAL; /*bind动作发生在最初状态，其TCP状态是CLOSE且没有绑定过 * 否则直接判别为异常 */ if (sk-&gt;sk_state != TCP_CLOSE || inet-&gt;inet_num) goto out_release_sock; /*inet_rcv_saddr 用作hash表查找使用 *inet_saddr作为发包源地址 *当为广播和组播时候发送地址为0 */ inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr; if (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST) inet-&gt;inet_saddr = 0; /* Use device */ /* Make sure we are allowed to bind here. */ /* TCP时候该函数负责查询该端口是否被使用，没有被使用返回0，否则返回非0 *如果已经被使用，则退出bind函数，并返回地址和端口已经被使用错误-EADDRINUSE *sk-&gt;sk_prot-&gt;get_port= inet_csk_get_port */ if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) &#123; inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0; err = -EADDRINUSE; goto out_release_sock; &#125; /* * 更新sk-&gt;sk_userlocks标记，表明本地地址和端口已经绑定 */ if (inet-&gt;inet_rcv_saddr) sk-&gt;sk_userlocks |= SOCK_BINDADDR_LOCK; if (snum) sk-&gt;sk_userlocks |= SOCK_BINDPORT_LOCK; inet-&gt;inet_sport = htons(inet-&gt;inet_num); inet-&gt;inet_daddr = 0; inet-&gt;inet_dport = 0; sk_dst_reset(sk); err = 0;out_release_sock: release_sock(sk);out: return err;&#125;EXPORT_SYMBOL(inet_bind); 绑定地址长度和协议检查 长度异常返回-EINVAL 表示参数异常，协议不支持 -EAFNOSUPPORT 对绑定地址进行类型检查inet_addr_type，必须是本机地址，组播和广播地址类型 -EADDRNOTAVAIL 否则报地址参数异常 如果端口小于1024 ，必须为超级权限ns_capable 否则 err = -EACCES 权限不允许 sk-&gt;sk_prot-&gt;get_port = inet_csk_get_port 四层端口检查，看是否被使用 更新sk-&gt;skuserlocks标记，代表地址和端口已经被绑定 扩展函数： inet_csk_get_port TCP四层端口检查 inet_addr_type 地址类型判别 ns_capable 超级权限检查 inet_csk_get_port123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202int inet_csk_get_port(struct sock *sk, unsigned short snum)&#123; struct inet_hashinfo *hashinfo = sk-&gt;sk_prot-&gt;h.hashinfo; struct inet_bind_hashbucket *head; struct inet_bind_bucket *tb; int ret, attempts = 5; struct net *net = sock_net(sk); int smallest_size = -1, smallest_rover; kuid_t uid = sock_i_uid(sk); int attempt_half = (sk-&gt;sk_reuse == SK_CAN_REUSE) ? 1 : 0; /*禁止上下半部，防止进程冲突*/ local_bh_disable(); /* * 如果没有bind端口 */ if (!snum) &#123;/*没有指定端口会自动选择端口*/ int remaining, rover, low, high;again: /*获取端口的取值范围*/ inet_get_local_port_range(net, &amp;low, &amp;high);/*后文辉对其进行分析*/ if (attempt_half) &#123; int half = low + ((high - low) &gt;&gt; 1); if (attempt_half == 1) high = half; else low = half; &#125; /*取值范围内端口数*/ remaining = (high - low) + 1; /*随机选择端口*/ smallest_rover = rover = net_random() % remaining + low; smallest_size = -1; do &#123; /*保留端口检查,服务端可以设置 /proc/sys/net/ipv4/ip_local_reserved_ports */ if (inet_is_reserved_local_port(rover)) goto next_nolock;/*端口加1继续*/ /*根据端口号和HASHsize从确定hash桶，并锁住它，后续便利查找*/ head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, rover, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == rover) &#123; /*判断端口是否可以复用，如果可以复用即使在链表中也一样复用*/ if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; (tb-&gt;num_owners &lt; smallest_size || smallest_size == -1)) &#123; /*记录下端口的使用个数和端口*/ smallest_size = tb-&gt;num_owners; smallest_rover = rover; /*系统绑定端口已经超过最大端口数了，要去检查inet_csk_bind_conflict绑定是否存在冲突*/ if (atomic_read(&amp;hashinfo-&gt;bsockets) &gt; (high - low) + 1 &amp;&amp; !inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; /*ok，绑定没有冲突*/ snum = smallest_rover; goto tb_found; &#125; &#125; /*端口冲突检查*/ if (!inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) &#123; snum = rover; goto tb_found; &#125; /*此端口在链表中但是不能复用，继续下一个*/ goto next; &#125; break;/*不在bind表中，端口可以使用，直接跳出循环*/ next: spin_unlock(&amp;head-&gt;lock); next_nolock: /*已经找到最大端口了，从最小开始找*/ if (++rover &gt; high) rover = low; &#125; while (--remaining &gt; 0);/*en,最大5次查找机会*/ /* Exhausted local port range during search? It is not * possible for us to be holding one of the bind hash * locks if this test triggers, because if 'remaining' * drops to zero, we broke out of the do/while loop at * the top level, not from the 'break;' statement. */ ret = 1; /*没有找到端口，那就最后一次机会*/ if (remaining &lt;= 0) &#123; if (smallest_size != -1) &#123; snum = smallest_rover; goto have_snum; &#125; if (attempt_half == 1) &#123; /* OK we now try the upper half of the range */ attempt_half = 2; goto again; &#125; goto fail; &#125; /* OK, here is the one we will use. HEAD is * non-NULL and we hold it's mutex. */ /*找到可用的端口了*/ snum = rover; &#125; else &#123; /*指定绑定了端口，在绑定的链表中查找，如果查找到，代表已经被绑定*/have_snum: head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, snum, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == snum) goto tb_found;/*端口已经被绑定*/ &#125; /*在绑定链表中没有发现，后续会创建*/ tb = NULL; goto tb_not_found; tb_found: if (!hlist_empty(&amp;tb-&gt;owners)) &#123; /*要bind的sk标记SK_FORCE_REUSE可以强制复用*/ if (sk-&gt;sk_reuse == SK_FORCE_REUSE) goto success; if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size == -1) &#123; /* 是否可以复用的判别 * fastreuseport Google添加选项&amp;&amp; 已经开启端口复用 &amp;&amp; 当前socket uid和查找到的uid相符合 * 当前socket也可以放到bind hash中，后续会将其加入 */ goto success; &#125; else &#123; ret = 1; /*端口绑定冲突，自动分配端口绑定冲突会走到此处，在自动分配端口时候进行了下列类似判别 *所以此判断基本不会执行知道跳到tb_not_found这个时候tb不为null的 */ if (inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, true)) &#123; if (((sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) || (tb-&gt;fastreuseport &gt; 0 &amp;&amp; sk-&gt;sk_reuseport &amp;&amp; uid_eq(tb-&gt;fastuid, uid))) &amp;&amp; smallest_size != -1 &amp;&amp; --attempts &gt;= 0) &#123; spin_unlock(&amp;head-&gt;lock); goto again; &#125; goto fail_unlock; &#125; &#125; &#125;tb_not_found: ret = 1; /*绑定时没有发现过tb，直接创建一个*/ if (!tb &amp;&amp; (tb = inet_bind_bucket_create(hashinfo-&gt;bind_bucket_cachep, net, head, snum)) == NULL) goto fail_unlock; if (hlist_empty(&amp;tb-&gt;owners)) &#123;/*没有绑定过socket*/ if (sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN) tb-&gt;fastreuse = 1; else tb-&gt;fastreuse = 0; /*设置了SO_REUSEPORT选项*/ if (sk-&gt;sk_reuseport) &#123; tb-&gt;fastreuseport = 1; tb-&gt;fastuid = uid; &#125; else tb-&gt;fastreuseport = 0; &#125; else &#123;/*如果绑定过socket*/ if (tb-&gt;fastreuse &amp;&amp; (!sk-&gt;sk_reuse || sk-&gt;sk_state == TCP_LISTEN)) tb-&gt;fastreuse = 0; if (tb-&gt;fastreuseport &amp;&amp; (!sk-&gt;sk_reuseport || !uid_eq(tb-&gt;fastuid, uid))) tb-&gt;fastreuseport = 0; &#125;success:/*找到可用端口，添加绑定表*/ if (!inet_csk(sk)-&gt;icsk_bind_hash) inet_bind_hash(sk, tb, snum);/*sk被放到tb-&gt;owners中*/ WARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb); ret = 0;fail_unlock: spin_unlock(&amp;head-&gt;lock);fail: local_bh_enable(); return ret;&#125; 如果端口为0；则自动选取端口选择过程如下： 先在[low,half] or [half,high]中随机选取一个端口，作为循环获取端口的起始端口，开始以下流程 步骤1: 保留端口检查，不满足，端口加1，重试次数减1，继续从步骤1开始 步骤2: 从当前端口映射的hash桶中取出列表头，遍历检查该端口是否被使用 步骤2-1:没有被使用，直接退出循环，tb为NULL，创建tb，跳转到tb_not_found将该端口连同创建的tb加入该hash桶的链表中，sk也被放到tb-&gt;owners中管理，结束退出 步骤2-2: 端口被使用了，检查端口使用是否冲突 步骤2-2-1:没有冲突，推出循环，跳转到tb_found,复用检查成功，sk被放到tb-&gt;owners中，结束退出 步骤2-2-2:存在冲突，直接端口+1，继续循环查找 步骤3:如果上半部分已经查找完毕，继续[half,high]中选择一个端口，进行步骤1 attempt_halfsk-&gt;sk_reuse == SK_CAN_REUSE 取端口范围 [low ,half]否则 取端口范围 [half,high] 该值会影响上述选择端口的流程从上半端还是从下半端选择端口 如果sk-&gt;sk_reuse被置SK_CAN_REUSE标记则先从下半端开始选择端口 否则直接从上半端选择端口 small_size和small_rover what’s the fuck!!! 疑惑了好久small_size和small_rover在3.10的版本中根本就没有使用基本用不到3.10版本的端口查找原则是确定端口查找区间，随机选择端口，只要该端口能复用就直接使用，已经完全去除了优先选择复用端口数较小的端口这一原则了（3.2kernel）So amazing！这两个变量可以去除了 inet_get_local_port_range1234567891011void inet_get_local_port_range(struct net *net, int *low, int *high)&#123; unsigned int seq; do &#123; seq = read_seqbegin(&amp;net-&gt;ipv4_sysctl_local_ports.lock); *low = net-&gt;ipv4_sysctl_local_ports.range[0]; *high = net-&gt;ipv4_sysctl_local_ports.range[1]; &#125; while (read_seqretry(&amp;net-&gt;ipv4_sysctl_local_ports.lock, seq));&#125; 12sysctl -a|grep ip_local_port_rangenet.ipv4.ip_local_port_range = 32768 60999 上述读取端口范围是用户态的ip_local_port_range，默认是3w多以后的，可以调整此参数扩大端口范围 上述read_seqbegin这种方式读取数据，是一种顺序锁，适用于读多写少的方式用方式，后续专门处博文研究 tcp端口冲突检查inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict1234567891011121314151617181920212223242526272829303132333435const struct inet_connection_sock_af_ops ipv4_specific = &#123; .queue_xmit = ip_queue_xmit, .send_check = tcp_v4_send_check, .rebuild_header = inet_sk_rebuild_header, .sk_rx_dst_set = inet_sk_rx_dst_set, .conn_request = tcp_v4_conn_request, .syn_recv_sock = tcp_v4_syn_recv_sock, .net_header_len = sizeof(struct iphdr), .setsockopt = ip_setsockopt, .getsockopt = ip_getsockopt, .addr2sockaddr = inet_csk_addr2sockaddr, .sockaddr_len = sizeof(struct sockaddr_in), .bind_conflict = inet_csk_bind_conflict,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_ip_setsockopt, .compat_getsockopt = compat_ip_getsockopt,#endif .mtu_reduced = tcp_v4_mtu_reduced,&#125;;static int tcp_v4_init_sock(struct sock *sk)&#123; struct inet_connection_sock *icsk = inet_csk(sk); tcp_init_sock(sk); icsk-&gt;icsk_af_ops = &amp;ipv4_specific;#ifdef CONFIG_TCP_MD5SIG tcp_sk(sk)-&gt;af_specific = &amp;tcp_sock_ipv4_specific;#endif return 0;&#125; 从上文得知inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict 函数是inet_csk_bind_conflict af_ops在tcp_v4_init_sock初始化 inet_csk_bind_conflict分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960int inet_csk_bind_conflict(const struct sock *sk, const struct inet_bind_bucket *tb, bool relax)&#123; struct sock *sk2; int reuse = sk-&gt;sk_reuse; int reuseport = sk-&gt;sk_reuseport; kuid_t uid = sock_i_uid((struct sock *)sk); /* * Unlike other sk lookup places we do not check * for sk_net here, since _all_ the socks listed * in tb-&gt;owners list belong to the same net - the * one this bucket belongs to. */ sk_for_each_bound(sk2, &amp;tb-&gt;owners) &#123; /*不会冲突情况1:socket绑定设备不同*/ if (sk != sk2 &amp;&amp; !inet_v6_ipv6only(sk2) &amp;&amp; (!sk-&gt;sk_bound_dev_if || !sk2-&gt;sk_bound_dev_if || sk-&gt;sk_bound_dev_if == sk2-&gt;sk_bound_dev_if)) &#123; /* *不会冲突情况2:地址不同 */ if ((!reuse || !sk2-&gt;sk_reuse || sk2-&gt;sk_state == TCP_LISTEN) &amp;&amp; (!reuseport || !sk2-&gt;sk_reuseport || (sk2-&gt;sk_state != TCP_TIME_WAIT &amp;&amp; !uid_eq(uid, sock_i_uid(sk2))))) &#123; /* * 不会冲突情况3: * 条件A: (reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state ！= TCP_LISTEN) * 条件B：(reuseport * &amp;&amp; sk2-&gt;sk_reuseport * &amp;&amp;(sk2-&gt;sk_state == TCP_TIME_WAIT || uid_eq(uid, sock_i_uid(sk2)))) * 条件A和条件B只要有一个成立，就不会冲突 * 条件A成立条件： * 链上sock和待检查sock开启地址复用且链上状态不是监听状态 * 条件B成立条件： * 链上sock和待检查sock开启端口复用且链表上状态为TW * 链上sock和待检查sock开启端口复用且两个sock的uid相同 */ if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; /*没有开启relax，要绑定方不能复用，已绑定方不能复用，以绑定方处理监听状态*/ if (!relax &amp;&amp; reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_state != TCP_LISTEN) &#123; if (!sk2-&gt;sk_rcv_saddr || !sk-&gt;sk_rcv_saddr || sk2-&gt;sk_rcv_saddr == sk-&gt;sk_rcv_saddr) break; &#125; &#125; &#125; return sk2 != NULL;&#125; 在端口自动选择时可以重用端口条件为： a设备不同b绑定ip地址不同c要绑定sock和已绑定sock地址允许重用，且已绑定socket不处于监听状态d 链上sock和待检查sock开启端口复用且链表上状态为TWe 链上sock和待检查sock开启端口复用且两个sock的uid相同 关于条件c的补充条件：即使c满足，也需要看relax的值确定，relax为TRUE时可复用，为fase时候不能复用 自动端口时候relax为false，所以条件c消失，仅仅剩下a、b、d、e四个条件","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"bind","slug":"bind","permalink":"https://vcpu.github.io/tags/bind/"},{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"}]},{"title":"connect()实现源码分析","slug":"connect","date":"2017-06-09T10:59:10.000Z","updated":"2017-06-09T10:59:10.000Z","comments":true,"path":"2017/06/09/connect/","link":"","permalink":"https://vcpu.github.io/2017/06/09/connect/","excerpt":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 实例:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr)","text":"connect()内核版本：3.10.0-514.16.1.el7.x86_64下述源码分析均以tcp socket为背景 用户态函数int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);参数： socketfd socket文件描述索引下标addr 要连接的服务端的地址addrlen addr的长度 实例:123456struct sockaddr_in remote_addr;memset(&amp;remote_addr,0,sizeof(remote_addr));remote_addr.sin_family=AF_INET;remote_addr.sin_addr.s_addr=inet_addr(\"180.97.33.108\");remote_addr.sin_port = htons(80);connect(fd,(struct sockaddr*)&amp;remote_addr,sizeof(struct sockaddr) 系统调用12345678910111213141516171819202122232425262728293031323334353637383940414243SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)&#123; unsigned long a[AUDITSC_ARGS]; unsigned long a0, a1; int err; unsigned int len; if (call &lt; 1 || call &gt; SYS_SENDMMSG) return -EINVAL; len = nargs[call]; if (len &gt; sizeof(a)) return -EINVAL; /* copy_from_user should be SMP safe. */ if (copy_from_user(a, args, len)) return -EFAULT; err = audit_socketcall(nargs[call] / sizeof(unsigned long), a); if (err) return err; a0 = a[0]; a1 = a[1]; switch (call) &#123; case SYS_SOCKET: err = sys_socket(a0, a1, a[2]); break; case SYS_BIND: err = sys_bind(a0, (struct sockaddr __user *)a1, a[2]); break; case SYS_CONNECT: err = sys_connect(a0, (struct sockaddr __user *)a1, a[2]); break; ... default: err = -EINVAL; break; &#125; return err;&#125; 系统调用sys_socketcall会携带（fd,serveraddr,serveraddrlen）参数 系统中断处理函数sys_socketcall会将参数从用户态考入到内核态局部变量a中 调用sys_connect函数 sys_connect(a0, (struct sockaddr __user *)a1, a[2]); sys_connect执行入口分析123456789101112131415161718192021222324SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,int,addrlen)&#123; struct socket *sock; struct sockaddr_storage address; int err, fput_needed; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; err = move_addr_to_kernel(uservaddr, addrlen, &amp;address); if (err &lt; 0) goto out_put; err = security_socket_connect(sock, (struct sockaddr *)&amp;address, addrlen); if (err) goto out_put; err = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)&amp;address, addrlen, sock-&gt;file-&gt;f_flags);out_put: fput_light(sock-&gt;file, fput_needed);out: return err; 根据fd描述符号从当前进程current的files指针中的struct fd_table中的fd成员取出file fdt-&gt;fd是一个数组用来管理当前进程的file指针 从file中privatedata中获取到socket变量 把connect连接的服务端地址存入内核空间中move_addr_to_kernel sock-&gt;ops-&gt;connect 以tco为例，此处会调用inet_stream_connect 函数集合中的inet_stream_connect inet_stream_connect分析12345678910int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; int err; lock_sock(sock-&gt;sk); err = __inet_stream_connect(sock, uaddr, addr_len, flags); release_sock(sock-&gt;sk); return err;&#125; inet_stream_connect() 为tcp socket时候connect动作调用的函数改函数会调用__inet_stream_connect函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags)&#123; struct sock *sk = sock-&gt;sk; int err; long timeo; //socket地址长度检查，不合法返回 if (addr_len &lt; sizeof(uaddr-&gt;sa_family)) return -EINVAL; // 地址协议族检查，如果不合法则关闭连接 if (uaddr-&gt;sa_family == AF_UNSPEC) &#123; err = sk-&gt;sk_prot-&gt;disconnect(sk, flags); sock-&gt;state = err ? SS_DISCONNECTING : SS_UNCONNECTED; goto out; &#125; switch (sock-&gt;state) &#123; //非法参数 default: err = -EINVAL; goto out; //该socket和对端连接已经建立 case SS_CONNECTED: err = -EISCONN; goto out; //该socket和对端连接建立中 case SS_CONNECTING: err = -EALREADY; /* Fall out of switch with err, set for this state */ break; //该socket和对未连接 case SS_UNCONNECTED: err = -EISCONN; //如果未连接，但是socket还不是TCP_CLOSE状态错误返回 if (sk-&gt;sk_state != TCP_CLOSE) goto out; //tcp调用tcp_v4_connect，发送syn err = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len); if (err &lt; 0) goto out; //发送syn后sock状态从未连接更新为连接中 sock-&gt;state = SS_CONNECTING; /* Just entered SS_CONNECTING state; the only * difference is that return value in non-blocking * case is EINPROGRESS, rather than EALREADY. */ err = -EINPROGRESS; break; &#125; //默认情况下未设置非阻塞socket标志，timeo不为0，设置非阻塞，该值为0 timeo = sock_sndtimeo(sk, flags &amp; O_NONBLOCK); //发送syn后等待后续握手完成 /* * 阻塞socket * inet_wait_for_connect 会等待协议栈层的处理 * 1.等待超过timeo，connect返回EINPROGRESS 表明正在处理 * 2.收到信号 * 3.正常完成握手，返回0 * 非阻塞socket * 直接退出connect函数并返回EINPROGRESS，表明协议栈正在处理 */ if ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; int writebias = (sk-&gt;sk_protocol == IPPROTO_TCP) &amp;&amp; tcp_sk(sk)-&gt;fastopen_req &amp;&amp; tcp_sk(sk)-&gt;fastopen_req-&gt;data ? 1 : 0; /* Error code is set above */ if (!timeo || !inet_wait_for_connect(sk, timeo, writebias)) goto out; err = sock_intr_errno(timeo); if (signal_pending(current)) goto out; &#125; /* Connection was closed by RST, timeout, ICMP error * or another process disconnected us. */ if (sk-&gt;sk_state == TCP_CLOSE) goto sock_error; /* sk-&gt;sk_err may be not zero now, if RECVERR was ordered by user * and error was received after socket entered established state. * Hence, it is handled normally after connect() return successfully. */ //TCP握手完成，连接已经建立 sock-&gt;state = SS_CONNECTED; err = 0;out: return err;//异常处理，关闭连接sock_error: err = sock_error(sk) ? : -ECONNABORTED; sock-&gt;state = SS_UNCONNECTED; if (sk-&gt;sk_prot-&gt;disconnect(sk, flags)) sock-&gt;state = SS_DISCONNECTING; goto out;&#125; __inet_stream_connect检查地址长度和协议族 检查sock状态，正常情况下状态为SS_UNCONNECTED sk-&gt;sk_prot-&gt;connect tcp_v4_connect来发送syn 在syn包发完以后会有两种处理情况 情况1:立即返回，针对于非阻塞socket，此时协议栈正在处理握手connect会返回-EINPROGRESS情况2:阻塞运行 阻塞时间超时后，connect返回-EINPROGRESS收到信号，connect返回-ERESTARTSYS,-EINTR inet_wait_for_connect函数分析12345678910111213141516171819202122232425262728293031323334353637static long inet_wait_for_connect(struct sock *sk, long timeo, int writebias)&#123; DEFINE_WAIT(wait); prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); sk-&gt;sk_write_pending += writebias; /* Basic assumption: if someone sets sk-&gt;sk_err, he _must_ * change state of the socket from TCP_SYN_*. * Connect() does not allow to get error notifications * without closing the socket. */ while ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123; release_sock(sk);/*等下要睡眠了释放sk锁*/ timeo = schedule_timeout(timeo); /* * 调用schedule_timeout sleep until timeout * 收到信号后，timeout值返回剩余等待时间 * 超时timeout后，返回0 */ /*进程被唤醒后新上sk锁*/ lock_sock(sk); /*进程有带处理信号，或者睡眠超时，推出循环*/ if (signal_pending(current) || !timeo) break; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); &#125; /*等待结束后，将进程从等待队列删除，标记为TASK_RUNNING*/ finish_wait(sk_sleep(sk), &amp;wait); sk-&gt;sk_write_pending -= writebias; return timeo;&#125; 睡眠前进程被设置成TASK_INTERRUPTIBLE状态 SO_SNDTIMEO选项对上述的睡眠非常重要 SO_SNDTIMEO被设置，则睡眠时间会安装设置值 SO_SNDTIMEO没有被设置，则在没有收到信号前一只阻塞 睡眠结束，进程从睡眠队列中删除，并标记为TASK_RUNNING prepare_to_wait实现分析1234567891011void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)&#123; unsigned long flags; wait-&gt;flags &amp;= ~WQ_FLAG_EXCLUSIVE; spin_lock_irqsave(&amp;q-&gt;lock, flags); if (list_empty(&amp;wait-&gt;task_list)) __add_wait_queue(q, wait); set_current_state(state); spin_unlock_irqrestore(&amp;q-&gt;lock, flags);&#125; prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); 把wait放入q队列中，设置当前进程状态为TASK_INTERRUPTIBLE TASK_INTERRUPTIBLE 是一种睡眠信号 标记TASK_INTERRUPTIBLE的信号会被唤醒并处理信号 阻塞socket唤醒机制[root@localhost stp]# stap bt.stp sock_def_wakeup WARNING: Missing unwind data for a module, rerun with ‘stap -d e1000’—————-START————————-In process [swapper/2]RIP: ffffffff81558150RSP: ffff88003fd03970 EFLAGS: 00000246RAX: 0000000000004308 RBX: ffff88003a82a6c0 RCX: 0000000000000000RDX: 0000000050000000 RSI: 0000000000ca00c8 RDI: ffff88003a82a6c0RBP: ffff88003fd03988 R08: ffff88003db89708 R09: ffff88003e001800R10: ffffffff815dabca R11: 0000000000000000 R12: ffff88001bfa3700R13: ffff880002db6762 R14: 0000000000000218 R15: ffff880002db675aFS: 0000000000000000(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 000000008005003bCR2: 00007ffaf3049072 CR3: 000000003b0b7000 CR4: 00000000000406e0 0xffffffff81558150 : sock_def_wakeup+0x0/0x40 [kernel] 0xffffffff815cbc09 : tcp_finish_connect+0xc9/0x120 [kernel] 0xffffffff815cc297 : tcp_rcv_state_process+0x637/0xf20 [kernel] 0xffffffff815d5ffb : tcp_v4_do_rcv+0x17b/0x340 [kernel] 0xffffffff815d76d9 : tcp_v4_rcv+0x799/0x9a0 [kernel] 0xffffffff815b1094 : ip_local_deliver_finish+0xb4/0x1f0 [kernel] 0xffffffff815b1379 : ip_local_deliver+0x59/0xd0 [kernel] 0xffffffff815b0d1a : ip_rcv_finish+0x8a/0x350 [kernel] 0xffffffff815b16a6 : ip_rcv+0x2b6/0x410 [kernel] 0xffffffff815700d2 : netif_receive_skb_core+0x582/0x800 [kernel] 0xffffffff81570368 : netif_receive_skb+0x18/0x60 [kernel] 0xffffffff815703f0 : netif_receive_skb_internal+0x40/0xc0 [kernel] 0xffffffff81571578 : napi_gro_receive+0xd8/0x130 [kernel] 0xffffffffa00472fc [e1000]—————-END————————- 12345678910111213141516171819202122232425262728293031323334353637383940void tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123; struct tcp_sock *tp = tcp_sk(sk); struct inet_connection_sock *icsk = inet_csk(sk); tcp_set_state(sk, TCP_ESTABLISHED); if (skb != NULL) &#123; icsk-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb); security_inet_conn_established(sk, skb); &#125; /* Make sure socket is routed, for correct metrics. */ icsk-&gt;icsk_af_ops-&gt;rebuild_header(sk); tcp_init_metrics(sk); tcp_init_congestion_control(sk); /* Prevent spurious tcp_cwnd_restart() on first data * packet. */ tp-&gt;lsndtime = tcp_time_stamp; tcp_init_buffer_space(sk); if (sock_flag(sk, SOCK_KEEPOPEN)) inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp)); if (!tp-&gt;rx_opt.snd_wscale) __tcp_fast_path_on(tp, tp-&gt;snd_wnd); else tp-&gt;pred_flags = 0; if (!sock_flag(sk, SOCK_DEAD)) &#123; /*握手完成唤醒所有进程*/ sk-&gt;sk_state_change(sk); sk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT); &#125;&#125; sock_def_wakeup -&gt;wake_up_interruptible_all 上述过程发声在三次握手完成后，TCP从syn send或者syn rcv切换到establish状态时候发生 tcp_finish_connect-&gt;sk-&gt;sk_state_change[sock_def_wakeup] 此次唤醒是全部唤醒sk上等待队列的进程","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"}]},{"title":"socket()实现源码分析","slug":"socket","date":"2017-06-09T10:03:12.000Z","updated":"2017-06-09T10:03:12.000Z","comments":true,"path":"2017/06/09/socket/","link":"","permalink":"https://vcpu.github.io/2017/06/09/socket/","excerpt":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0","text":"socket()内核版本：3.10.0-514.16.1.el7.x86_64 1234#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);fd=socket(PF_INET,SOCK_STREAM,0 (1).接口说明：按照顺序可传入如下参数： PF_INEAT SOCK_STREAM,SOCK_DGRAM,SOCK_RAW IPPROTO_TCP,IPPROTO_UDP,IPPROTO_IP 返回值说明 EAFNOSUPPORT 不支持地址类型 EMFILE 进程文件表溢出 ENFILE 核心内存不足无法建立新的socket EINVAL 参数domain/type/protocol不合法 EACCES 权限不允许 ENOBUFS/ENOMEM 内存不足 EPROTONOSUPPORT domain指定的类型不支持参数type或者protocol (2).内核调用栈 (3).结构体说明 struct socket 面向用户态的结构体基于虚拟文件系统创建创建socket时最先创建的结构体 struct sock 网络层socket struct inet_sock INET域socket表示提供INET域的一些属性，TTL、 组播、 地址 、端口 struct raw_socket、struct udp—sock、 struct inet_connection_sock 是对struct inet_sock的扩展struct raw_socket要处理ICMPstruct udp_sock udp协议socketstruct inet_connection_sock面向连接socketstruct tcp_sock TCP协议socket ，对inet_connection_sock扩展，增加了滑动窗口等拥塞控制属性struct inet_timewait_sock网络层超时控制使用struct tcp_timewait_sock TCP协议超时控制使用 (4).struct socket创建源码分析(4.1).sock_alloc函数123456789101112131415161718192021static struct socket *sock_alloc(void)&#123; struct inode *inode; struct socket *sock; inode = new_inode_pseudo(sock_mnt-&gt;mnt_sb); if (!inode) return NULL; sock = SOCKET_I(inode); kmemcheck_annotate_bitfield(sock, type); inode-&gt;i_ino = get_next_ino(); inode-&gt;i_mode = S_IFSOCK | S_IRWXUGO; inode-&gt;i_uid = current_fsuid(); inode-&gt;i_gid = current_fsgid(); inode-&gt;i_op = &amp;sockfs_inode_ops; this_cpu_add(sockets_in_use, 1); return sock;&#125; 一起申请两块内存struct socket和struct inode 两块内存用struct socket_alloc联系起来 inode是linux用来刻画一个存放在内存中的文件的 socket是一种网络文件类型，可以通过文件描述符使用read和write等文件操作函数操作socket 有了inode就支持了虚拟文件系统的操作 (4.2).sock_alloc-&gt;new_inode_pseudo-&gt;alloc_inode12345678910111213141516171819202122232425262728293031323334struct inode *new_inode_pseudo(struct super_block *sb)&#123; struct inode *inode = alloc_inode(sb); if (inode) &#123; spin_lock(&amp;inode-&gt;i_lock); inode-&gt;i_state = 0; spin_unlock(&amp;inode-&gt;i_lock); INIT_LIST_HEAD(&amp;inode-&gt;i_sb_list); &#125; return inode;&#125;static struct inode *alloc_inode(struct super_block *sb)&#123; struct inode *inode; if (sb-&gt;s_op-&gt;alloc_inode) inode = sb-&gt;s_op-&gt;alloc_inode(sb); else inode = kmem_cache_alloc(inode_cachep, GFP_KERNEL); if (!inode) return NULL; if (unlikely(inode_init_always(sb, inode))) &#123; if (inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode) inode-&gt;i_sb-&gt;s_op-&gt;destroy_inode(inode); else kmem_cache_free(inode_cachep, inode); return NULL; &#125; return inode;&#125; alloc_inode获取内存有两种方式 1.通过自己alloc_inode分配 2.从高速缓存中分配 (4.3).alloc_inode -&gt; sock_alloc_inode12345678910111213141516171819202122232425static struct inode *sock_alloc_inode(struct super_block *sb)&#123; struct socket_alloc *ei; struct socket_wq *wq; ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL); if (!ei) return NULL; wq = kmalloc(sizeof(*wq), GFP_KERNEL); if (!wq) &#123; kmem_cache_free(sock_inode_cachep, ei); return NULL; &#125; init_waitqueue_head(&amp;wq-&gt;wait); wq-&gt;fasync_list = NULL; RCU_INIT_POINTER(ei-&gt;socket.wq, wq); ei-&gt;socket.state = SS_UNCONNECTED; ei-&gt;socket.flags = 0; ei-&gt;socket.ops = NULL; ei-&gt;socket.sk = NULL; ei-&gt;socket.file = NULL; return &amp;ei-&gt;vfs_inode;&#125; socket结构体最终会调用上述函数申请内存 该函数会在sock_init中被注册和挂载到系统上 (4.4).sock_init 中sock_allok_inode挂载过程123456789101112131415161718192021222324err = register_filesystem(&amp;sock_fs_type); if (err) goto out_fs; sock_mnt = kern_mount(&amp;sock_fs_type); if (IS_ERR(sock_mnt)) &#123; err = PTR_ERR(sock_mnt); goto out_mount; ... static struct file_system_type sock_fs_type = &#123; .name = \"sockfs\", .mount = sockfs_mount, .kill_sb = kill_anon_super,&#125;;static struct dentry *sockfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)&#123; return mount_pseudo(fs_type, \"socket:\", &amp;sockfs_ops, &amp;sockfs_dentry_operations, SOCKFS_MAGIC);&#125;static const struct super_operations sockfs_ops = &#123; .alloc_inode = sock_alloc_inode, .destroy_inode = sock_destroy_inode, .statfs = simple_statfs,&#125;; sock_init -&gt; register mount -&gt; sock_fs_type-&gt;sockfs_mount-&gt;sockfs_ops-&gt;sock_alloc_node (4.5).pf-&gt;create 即TCP／IP协议族的创建函数inet_create初始化步骤(4.5.1).PF_INET协议族的create函数inet_create会被组册1234567(void)sock_register(&amp;inet_family_ops);static const struct net_proto_family inet_family_ops = &#123; .family = PF_INET, .create = inet_create, .owner = THIS_MODULE,&#125;; (4.5.2).注册过程123456789101112131415161718192021int sock_register(const struct net_proto_family *ops)&#123; int err; if (ops-&gt;family &gt;= NPROTO) &#123; printk(KERN_CRIT \"protocol %d &gt;= NPROTO(%d)\\n\", ops-&gt;family, NPROTO); return -ENOBUFS; &#125; spin_lock(&amp;net_family_lock); if (rcu_dereference_protected(net_families[ops-&gt;family], lockdep_is_held(&amp;net_family_lock))) err = -EEXIST; else &#123; rcu_assign_pointer(net_families[ops-&gt;family], ops); err = 0; &#125; spin_unlock(&amp;net_family_lock); printk(KERN_INFO \"NET: Registered protocol family %d\\n\", ops-&gt;family); return err;&#125; 协议族选项ops会根基协议族类型PF_INET被放置到net_families系统全局变量中 (4.5.3).__sock_create使用过程1234567891011121314151617181920socket.c/__sock_create...rcu_read_lock(); pf = rcu_dereference(net_families[family]); err = -EAFNOSUPPORT; if (!pf) goto out_release; /* * We will call the -&gt;create function, that possibly is in a loadable * module, so we have to bump that loadable module refcnt first. */ if (!try_module_get(pf-&gt;owner)) goto out_release; /* Now protected by module ref count */ rcu_read_unlock(); err = pf-&gt;create(net, sock, protocol, kern); if (err &lt; 0) goto out_module_put; 根据socket传输过来的协议族PF_INET查找全局变量net_families获取ops 通过ops-&gt;create调用inet_create根据具体协议创建网络层socket struct sock (4.6).inet_create都干了什么？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140static int inet_create(struct net *net, struct socket *sock, int protocol, int kern)&#123; struct sock *sk; struct inet_protosw *answer; struct inet_sock *inet; struct proto *answer_prot; unsigned char answer_flags; int try_loading_module = 0; int err; if (protocol &lt; 0 || protocol &gt;= IPPROTO_MAX) return -EINVAL; sock-&gt;state = SS_UNCONNECTED;//步骤1:设置socket状态SS_UNCONNECTED /* Look for the requested type/protocol pair. */lookup_protocol: err = -ESOCKTNOSUPPORT; rcu_read_lock();／／步骤2:根据socket协议找到inet处理函数 connect、bind、accept、listen、等 list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123; err = 0; /* Check the non-wild match. */ if (protocol == answer-&gt;protocol) &#123; if (protocol != IPPROTO_IP) break; &#125; else &#123; /* Check for the two wild cases. */ if (IPPROTO_IP == protocol) &#123; protocol = answer-&gt;protocol; break; &#125; if (IPPROTO_IP == answer-&gt;protocol) break; &#125; err = -EPROTONOSUPPORT; &#125; if (unlikely(err)) &#123; if (try_loading_module &lt; 2) &#123; rcu_read_unlock(); /* * Be more specific, e.g. net-pf-2-proto-132-type-1 * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM) */ if (++try_loading_module == 1) request_module(\"net-pf-%d-proto-%d-type-%d\", PF_INET, protocol, sock-&gt;type); /* * Fall back to generic, e.g. net-pf-2-proto-132 * (net-pf-PF_INET-proto-IPPROTO_SCTP) */ else request_module(\"net-pf-%d-proto-%d\", PF_INET, protocol); goto lookup_protocol; &#125; else goto out_rcu_unlock; &#125; err = -EPERM; if (sock-&gt;type == SOCK_RAW &amp;&amp; !kern &amp;&amp; !ns_capable(net-&gt;user_ns, CAP_NET_RAW)) goto out_rcu_unlock;／／步骤3: 把协协议的inet操作集合赋值给socket结构的ops sock-&gt;ops = answer-&gt;ops; answer_prot = answer-&gt;prot; answer_flags = answer-&gt;flags; rcu_read_unlock(); WARN_ON(answer_prot-&gt;slab == NULL); err = -ENOBUFS; ／／步骤4:申请struct sock结构体，并切把协议操作集合赋值给sock结构体 ／／sk-&gt;sk_prot = sk-&gt;sk_prot_creator =协议操作集合; sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot); if (sk == NULL) goto out; err = 0; if (INET_PROTOSW_REUSE &amp; answer_flags) sk-&gt;sk_reuse = SK_CAN_REUSE;／／步骤5：inet_sock进行相关初始化 inet = inet_sk(sk); inet-&gt;is_icsk = (INET_PROTOSW_ICSK &amp; answer_flags) != 0; inet-&gt;nodefrag = 0; if (SOCK_RAW == sock-&gt;type) &#123; inet-&gt;inet_num = protocol; if (IPPROTO_RAW == protocol) inet-&gt;hdrincl = 1; &#125; if (net-&gt;sysctl_ip_no_pmtu_disc) inet-&gt;pmtudisc = IP_PMTUDISC_DONT; else inet-&gt;pmtudisc = IP_PMTUDISC_WANT; inet-&gt;inet_id = 0; sock_init_data(sock, sk); sk-&gt;sk_destruct = inet_sock_destruct; sk-&gt;sk_protocol = protocol; sk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv; inet-&gt;uc_ttl = -1; inet-&gt;mc_loop = 1; inet-&gt;mc_ttl = 1; inet-&gt;mc_all = 1; inet-&gt;mc_index = 0; inet-&gt;mc_list = NULL; inet-&gt;rcv_tos = 0; sk_refcnt_debug_inc(sk); if (inet-&gt;inet_num) &#123; /* It assumes that any protocol which allows * the user to assign a number at socket * creation time automatically * shares. */ inet-&gt;inet_sport = htons(inet-&gt;inet_num); /* Add to protocol hash chains. */ sk-&gt;sk_prot-&gt;hash(sk); &#125;／／步骤6:调用协议层初始化函数tcp_v4_init_sock()进行始化 if (sk-&gt;sk_prot-&gt;init) &#123; err = sk-&gt;sk_prot-&gt;init(sk); if (err) sk_common_release(sk); &#125;out: return err;out_rcu_unlock: rcu_read_unlock(); goto out;&#125; 设置socket状态SS_UNCONNECTED 根据协议类型找到具体的协议类型操作集合，例如协议处理函数tcp_proc和inet层处理函数集合inet_stream_ops socket-&gt;ops 获得协议操作集合inet_stream_ops 申请sock，并把tcp_proc赋值给它 sk-&gt;sk_prot = sk-&gt;sk_prot_creator=tcp_proc 把申请的sock和inet_sock进行初始化 sk-&gt;sk_prot-&gt;init(sk) 调用tcp_proc深度初始化TCP相关信息 尽管流程主要干了上述的事情，仍需要深入探究的问题是：a. inet_protosw inet_protosw初始化过程如何？b. inet_sock和sock是什么关系？c. 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ (4.6.1).inet_protosw初始化过程如何？12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485static struct inet_protosw inetsw_array[] =&#123; &#123; .type = SOCK_STREAM, .protocol = IPPROTO_TCP, .prot = &amp;tcp_prot, .ops = &amp;inet_stream_ops, .flags = INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_UDP, .prot = &amp;udp_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_PERMANENT, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_ICMP, .prot = &amp;ping_prot, .ops = &amp;inet_dgram_ops, .flags = INET_PROTOSW_REUSE, &#125;, &#123; .type = SOCK_RAW, .protocol = IPPROTO_IP, /* wild card */ .prot = &amp;raw_prot, .ops = &amp;inet_sockraw_ops, .flags = INET_PROTOSW_REUSE, &#125;&#125;;//inet_init for (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q) inet_register_protosw(q); //inet_protosw放入全局inetsw管理void inet_register_protosw(struct inet_protosw *p)&#123; struct list_head *lh; struct inet_protosw *answer; int protocol = p-&gt;protocol; struct list_head *last_perm; spin_lock_bh(&amp;inetsw_lock); if (p-&gt;type &gt;= SOCK_MAX) goto out_illegal; /* If we are trying to override a permanent protocol, bail. */ answer = NULL; last_perm = &amp;inetsw[p-&gt;type]; list_for_each(lh, &amp;inetsw[p-&gt;type]) &#123; answer = list_entry(lh, struct inet_protosw, list); /* Check only the non-wild match. */ if (INET_PROTOSW_PERMANENT &amp; answer-&gt;flags) &#123; if (protocol == answer-&gt;protocol) break; last_perm = lh; &#125; answer = NULL; &#125; if (answer) goto out_permanent; /* Add the new entry after the last permanent entry if any, so that * the new entry does not override a permanent entry when matched with * a wild-card protocol. But it is allowed to override any existing * non-permanent entry. This means that when we remove this entry, the * system automatically returns to the old behavior. */ list_add_rcu(&amp;p-&gt;list, last_perm);out: spin_unlock_bh(&amp;inetsw_lock); return;out_permanent: pr_err(\"Attempt to override permanent protocol %d\\n\", protocol); goto out;out_illegal: pr_err(\"Ignoring attempt to register invalid socket type %d\\n\", p-&gt;type); goto out;&#125; inet_init 会把inet_protosw方式inet_sw中 inet_protosw很重要，其含有协议的具体操作函数tcp_close,tcp_v4_connect,tcp_recvmsg等 inet_protosw，内还包含inet层操作函数 inet_bind,inet_accept,inet_bind,inet_listen等 (4.6.2). inet_sock和sock是什么关系？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566struct sock *sk_alloc(struct net *net, int family, gfp_t priority, struct proto *prot)&#123; struct sock *sk; sk = sk_prot_alloc(prot, priority | __GFP_ZERO, family); if (sk) &#123; sk-&gt;sk_family = family; /* * See comment in struct sock definition to understand * why we need sk_prot_creator -acme */ sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; sock_lock_init(sk); sock_net_set(sk, get_net(net)); atomic_set(&amp;sk-&gt;sk_wmem_alloc, 1); sock_update_classid(sk); sock_update_netprioidx(sk); &#125; return sk;&#125;static struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority, int family)&#123; struct sock *sk; struct kmem_cache *slab; slab = prot-&gt;slab; if (slab != NULL) &#123; sk = kmem_cache_alloc(slab, priority &amp; ~__GFP_ZERO); if (!sk) return sk; if (priority &amp; __GFP_ZERO) &#123; if (prot-&gt;clear_sk) prot-&gt;clear_sk(sk, prot-&gt;obj_size); else sk_prot_clear_nulls(sk, prot-&gt;obj_size); &#125; &#125; else sk = kmalloc(prot-&gt;obj_size, priority);//申请内存大小为prot的objsize if (sk != NULL) &#123; kmemcheck_annotate_bitfield(sk, flags); if (security_sk_alloc(sk, family, priority)) goto out_free; if (!try_module_get(prot-&gt;owner)) goto out_free_sec; sk_tx_queue_clear(sk); &#125; return sk;out_free_sec: security_sk_free(sk);out_free: if (slab != NULL) kmem_cache_free(slab, sk); else kfree(sk); return NULL;&#125; 从上述sk_alloc -&gt; sk_prot_alloc -&gt; obj_size 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct proto tcp_prot = &#123; .name = \"TCP\", .owner = THIS_MODULE, .close = tcp_close, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, .sendpage = tcp_sendpage, .backlog_rcv = tcp_v4_do_rcv, .release_cb = tcp_release_cb, .hash = inet_hash, .unhash = inet_unhash, .get_port = inet_csk_get_port, .enter_memory_pressure = tcp_enter_memory_pressure, .stream_memory_free = tcp_stream_memory_free, .sockets_allocated = &amp;tcp_sockets_allocated, .orphan_count = &amp;tcp_orphan_count, .memory_allocated = &amp;tcp_memory_allocated, .memory_pressure = &amp;tcp_memory_pressure, .sysctl_wmem = sysctl_tcp_wmem, .sysctl_rmem = sysctl_tcp_rmem, .max_header = MAX_TCP_HEADER, .obj_size = sizeof(struct tcp_sock), .slab_flags = SLAB_DESTROY_BY_RCU, .twsk_prot = &amp;tcp_timewait_sock_ops, .rsk_prot = &amp;tcp_request_sock_ops, .h.hashinfo = &amp;tcp_hashinfo, .no_autobind = true,#ifdef CONFIG_COMPAT .compat_setsockopt = compat_tcp_setsockopt, .compat_getsockopt = compat_tcp_getsockopt,#endif#ifdef CONFIG_MEMCG_KMEM .init_cgroup = tcp_init_cgroup, .destroy_cgroup = tcp_destroy_cgroup, .proto_cgroup = tcp_proto_cgroup,#endif&#125;; struct tcp_sock 包含strcut inet_sock 包含 struct sock 上述结构体为互相包含的关系 实际上在申请sock时候，申请内存大小为tcp_sock大小，也就是说三个结构体共同诞生了 (4.6.3). 从inet_protosw获取的prot和ops哪些结构体上会记录使用？ struct socket会在inet_create函数中获取到ops sock-&gt;ops = answer-&gt;ops;struct sock在sk_allloc函数中获取pro sk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot; (5).socket与文件系统socket与文件系统关联通过sock_map_fd完成 其步骤如下： 1:获取fd get_unused_fd_flags 该函数从当前进程管理的files获取可用的fd 2:申请file sock_alloc_file 将struct socket放到file的private_data管理 file-&gt;private_data = sock 3:将file根据当前fd安装到current-&gt;files中 files有一个指针fdtfdt-&gt;fd是一个类型为file指针的数组，数组下标为fdrcu_assign_pointer(fdt-&gt;fd[fd], file); 将file安装fd为数组下标放到current-&gt;files管理","categories":[{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/categories/socket/"}],"tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"https://vcpu.github.io/tags/tcp-ip/"},{"name":"kernel3.10.0-514.16.1","slug":"kernel3-10-0-514-16-1","permalink":"https://vcpu.github.io/tags/kernel3-10-0-514-16-1/"},{"name":"socket","slug":"socket","permalink":"https://vcpu.github.io/tags/socket/"}]},{"title":"systemtap使用调试记录（一）","slug":"systemtap使用调试记录（一）","date":"2017-06-05T10:15:52.000Z","updated":"2017-06-05T10:15:52.000Z","comments":true,"path":"2017/06/05/systemtap使用调试记录（一）/","link":"","permalink":"https://vcpu.github.io/2017/06/05/systemtap使用调试记录（一）/","excerpt":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法","text":"systemtap使用调试记录（一）一、调试环境介绍Linux 3.10.0-514.16.1.el7.x86_64 kernel-devel-3.10.0-514.16.1.el7.x86_64.rpm 同版本的开发头文件 kernel-debuginfo-common-x86_64-3.10.0-514.16.1.el7.x86_64.rpm kernel-debuginfo-3.10.0-514.16.1.el7.x86_64.rpm 同版本调试数据包 linux-3.10.0-514.16.1.el7.tar.xz 同版本的源码 kernel开发头文件下载地址kernel调试包下载地址kernel调试common包下载地址根据当前虚拟机获取内核代码的方法 二、centos7安装方法yum install *.rpm 安装上述3个（debugifo,devel,debuginfo-common）rpm包 yum install systemtap stap -ve &apos;probe begin { log(&quot;hello world&quot;) exit() }&apos; 测试正常结果如下： [root@localhost qinlong]# stap -ve ‘probe begin { log(“hello world”) exit() }’Pass 1: parsed user script and 120 library scripts using 227352virt/40488res/3260shr/37400data kb, in 260usr/30sys/338real ms.Pass 2: analyzed script: 1 probe, 2 functions, 0 embeds, 0 globals using 228540virt/41804res/3420shr/38588data kb, in 10usr/0sys/6real ms.Pass 3: translated to C into “/tmp/stap5CqHmN/stap_f7a5084b8a638f5ce64a31271684ef1f_1133_src.c” using 228672virt/42408res/3996shr/38720data kb, in 0usr/0sys/0real ms.Pass 4: compiled C into “stap_f7a5084b8a638f5ce64a31271684ef1f_1133.ko” in 1000usr/330sys/1247real ms.Pass 5: starting run.hello worldPass 5: run completed in 10usr/40sys/362real ms. 三、通用案例1.函数调用栈打印123456789[root@localhost stp]# cat bt.stp probe kernel.function(@1)&#123; print(&quot;----------------START-------------------------\\n&quot;) printf(&quot;In process [%s]\\n&quot;, execname()) print_regs() print_backtrace() print(&quot;----------------END-------------------------\\n&quot;) exit() &#125; 打印内核函数的调用栈 [root@localhost stp]# stap bt.stp tcp_sendmsg—————-START————————-In process [sshd]RIP: ffffffff815c1ee0RSP: ffff88003d217d28 EFLAGS: 00000202RAX: ffffffff81aa20a0 RBX: ffff88003d217e38 RCX: 0000000000000024RDX: ffff88003d217da8 RSI: ffff88003b3b87c0 RDI: ffff88003d217e38RBP: ffff88003d217d50 R08: 0000000000000000 R09: 0000000000000000R10: ffff88003d217da8 R11: 0000000000000000 R12: ffff88003d217e38R13: 0000000000000001 R14: ffff88003d217e28 R15: ffff8800274d3480FS: 00007f03e5514840(0000) GS:ffff88003fd00000(0000) knlGS:0000000000000000CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033CR2: 00007f19c6dc8000 CR3: 0000000035a5c000 CR4: 00000000000406e0 0xffffffff815c1ee0 : tcp_sendmsg+0x0/0xc40 [kernel] 0xffffffff815ed254 : inet_sendmsg+0x64/0xb0 [kernel] 0xffffffff81554e07 : sock_aio_write+0x157/0x180 [kernel] 0xffffffff811fdf3d : do_sync_write+0x8d/0xd0 [kernel] 0xffffffff811fe8a5 : vfs_write+0x1b5/0x1e0 [kernel] 0xffffffff811ff2cf : sys_write+0x7f/0xe0 [kernel] 0xffffffff81697189 : system_call_fastpath+0x16/0x1b [kernel]—————-END————————- 2.函数的调用过程1234567[root@localhost stp]# cat socket-trace.stpprobe kernel.function(&quot;*@net/socket.c&quot;).call&#123; printf(&quot;%s -&gt; %s\\n&quot;,thread_indent(1),ppfunc())&#125;probe kernel.function(&quot;*@net/socket.c&quot;).return&#123; printf(&quot;%s&lt;-%s\\n&quot;,thread_indent(-1),ppfunc())&#125; thread_indent(1) 打印程序名称（线程id）ppfunc() 打印出执行函数符号 kernel.function(“@net/socket.c”).call调用net/socket.c 文件中函数时候会触发函数体执行打印动作kernel.function(“@net/socket.c”).return调用net/socket.c文件中函数执行完成返回后会触发函数体打印动作 [root@localhost stp]# stap socket-trace.stp 0 dndX11(3295): -&gt; SyS_recvmsg 0 dndX11(3295): -&gt; sys_recvmsg 0 dndX11(3295): -&gt; sockfd_lookup_light 0 dndX11(3295):&lt;-sockfd_lookuplight 1 dndX11(3295): -&gt; sys_recvmsg 3 dndX11(3295): -&gt; sock_recvmsg 7 dndX11(3295):&lt;-sock_recvmsg 8 dndX11(3295):&lt;-_sys_recvmsg 9 dndX11(3295):&lt;-sys_recvmsg 10 dndX11(3295):&lt;-SyS_recvmsg25274 dndX11(3295): -&gt; SyS_recvmsg25279 dndX11(3295): -&gt; sys_recvmsg25281 dndX11(3295): -&gt; sockfd_lookup_light25284 dndX11(3295):&lt;-sockfd_lookuplight25285 dndX11(3295): -&gt; sys_recvmsg25288 dndX11(3295): -&gt; sock_recvmsg25291 dndX11(3295):&lt;-sock_recvmsgx 3.打印协议栈函数中某一行数据/home/qinlong/rpmbuild/SOURCES/linux-3.10.0-514.16.1.el7/net/ipv4/tcp.c局部源码如下：12345678910111213141065 int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,1066 size_t size)1067 &#123;1068 struct iovec *iov;1069 struct tcp_sock *tp = tcp_sk(sk);1070 struct sk_buff *skb;1071 int iovlen, flags, err, copied = 0;1072 int mss_now = 0, size_goal, copied_syn = 0, offset = 0;1073 bool sg;1074 long timeo;10751076 lock_sock(sk);10771078 flags = msg-&gt;msg_flags; 12[root@localhost ~]# stap -L &apos;kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;)&apos;kernel.statement(&quot;tcp_sendmsg@net/ipv4/tcp.c:1078&quot;) $iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int $mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 执行上述函数，可确代码具体的函数局部变量12345678910$iocb:struct kiocb* $sk:struct sock* $msg:struct msghdr* $size:size_t $copied:int$mss_now:int $size_goal:int $copied_syn:int $offset:int $timeo:long int 根据以上变量打印出size值123[root@localhost ~]# stap -e &apos;probe kernel.statement(&quot;*@net/ipv4/tcp.c:1078&quot;) &#123;printf(&quot;size %d \\n&quot;,$size)&#125;&apos;size 36size 44","categories":[{"name":"linux kernel","slug":"linux-kernel","permalink":"https://vcpu.github.io/categories/linux-kernel/"}],"tags":[{"name":"systemtap","slug":"systemtap","permalink":"https://vcpu.github.io/tags/systemtap/"}]}]}